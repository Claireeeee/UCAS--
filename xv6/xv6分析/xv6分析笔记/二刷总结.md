管理并抽象底层硬件

多路复用

程序交互，共享数据、共同工作





让接口的设计依赖于少量的*机制*（*mechanism*)，通过这些机制的组合提供强大、通用的功能

xv6 操作系统提供 Unix 操作系统中的基本接口（由 Ken Thompson 和 Dennis Ritchie 引入），同时模仿 Unix 的内部设计



Unix的接口设计非常成功：比如一个文件描述符接口，就可以处理外围设备和程序之间的所用通信



xv6 使用了传统的**内核**概念

数据保护和共享是内存管理的一个主要任务，比如用户进程只能访问自己的内存空间，比如所有cpu共享一个ptable表

特权级，相应的硬件实现的检查电路





xv6 内核提供了 Unix 传统系统调用的一部分

系统调用，程序的通用操作包

| 系统调用                  | 描述                               |
| ------------------------- | ---------------------------------- |
| fork()                    | 创建进程                           |
| exit()                    | 结束当前进程                       |
| wait()                    | 等待子进程结束                     |
| kill(pid)                 | 结束 pid 所指进程                  |
| getpid()                  | 获得当前进程 pid                   |
| sleep(n)                  | 睡眠 n 秒                          |
| exec(filename, *argv)     | 加载并执行一个文件                 |
| sbrk(n)                   | 为进程内存空间增加 n 字节          |
| open(filename, flags)     | 打开文件，flags 指定读/写模式      |
| read(fd, buf, n)          | 从文件中读 n 个字节到 buf          |
| write(fd, buf, n)         | 从 buf 中写 n 个字节到文件         |
| close(fd)                 | 关闭打开的 fd                      |
| dup(fd)                   | 复制 fd                            |
| pipe( p)                  | 创建管道， 并把读和写的 fd 返回到p |
| chdir(dirname)            | 改变当前目录                       |
| mkdir(dirname)            | 创建新的目录                       |
| mknod(name, major, minor) | 创建设备文件                       |
| fstat(fd)                 | 返回文件信息                       |
| link(f1, f2)              | 给 f1 创建一个新名字(f2)           |
| unlink(filename)          | 删除文件                           |



### 进程和内存

一个 xv6 进程由两部分组成，一部分是用户内存空间（指令，数据，栈），另一部分是仅对内核可见的进程状态

内核将每个进程和一个 **pid** (process identifier) 关联起来

`fork` 函数在父进程返回子进程的 pid，子进程返回 0

父子进程拥有不同的内存空间和寄存器，改变一个进程中的变量不会影响另一个进程

`exec`： 某个可执行文件（xv6 使用 ELF 文件格式）内存镜像，替换内存空间

`exec`执行成功后，直接从ELF头中声明的入口开始，执行从文件中加载的指令

`exec` 接受两个参数：可执行文件路径和一个字符串参数数组。

shell 

* 主循环通过 `getcmd` 读取命令行的输入

* 调用 `fork` 生成一个 shell 进程的副本

* 父 shell 调用 `wait`，而子进程执行用户命令


xv6 通常隐式地分配用户的内存空间

- `fork` 在子进程需要装入父进程的内存拷贝时分配空间，`exec` 在需要装入可执行文件时分配空间。
- 一个进程在需要额外内存时可以通过调用 `sbrk(n)` 来增加 n 字节的数据内存。 `sbrk` 返回新的内存的地址

xv6 没有用户这个概念当然更没有不同用户间的保护隔离措施。按照 Unix 的术语来说，所有的 xv6 进程都以 root 用户执行

### I/O 和文件描述符

处理外围设备和程序之间的所用通信

每个进程都有一张ofile表，从0开始的文件描述符空间

按照惯例，进程从文件描述符0读入（标准输入），从文件描述符1输出（标准输出），从文件描述符2输出错误（标准错误输出）

可以重定向，用fork和exec实现的， `open` 执行时会打开最小可用文件描述符

shell 保证在任何时候都有3个打开的文件描述符（8007），他们是控制台（console）的默认文件描述符

文件描述符是一个强大的抽象

### 管道

管道是一个小的内核缓冲区，它以文件描述符对的形式提供给进程，一个用于写操作，一个用于读操作

管道提供了一种进程间交互的方式，通信，共享

具有以下特点：

半双工的，数据只能向一个方向流动

只能用于父子进程或者兄弟进程之间（具有亲缘关系的进程）；

单独构成一种独立的文件系统：管道对于管道两端的进程而言，就是一个文件，但管道不是普通的文件，管道就是管道；只存在与内存中

管道的不足

不能用来对多个接受者广播数据。

②如果一个管道有多个读进程,那么写进程不能发送数据到指定的读进程。同样,如果有多个写进程,那么没有方法来判别是它们中的哪一个发送的数据。

但管道和临时文件起码有三个关键的不同点。首先，管道会进行自我清扫，如果是 shell 重定向的话，我们必须要在任务完成后删除 `/tmp/xyz`。第二，管道可以传输任意长度的数据。第三，管道允许同步：每一个读操作都阻塞调用进程，直到另一个进程用 `write` 完成数据的发送。

### 文件系统

xv6 文件系统提供文件和目录

xv6 把目录实现为一种特殊的文件

每一个 inode 都由一个唯一的 `inode 号` 直接确定，一个文件的 inode 和磁盘空间当链接数变为 0 的时候被清空





xv6 为每个进程维护了不同的页表。

一片地址空间包含了从虚拟地址0开始的*用户内存*。地址最低处的指令，接下来则是全局变量，栈区，以及一个用户可按需拓展的“堆”区（malloc 用）

每个进程都有一个运行线程（或简称为*线程*）来执行进程的指令

每个进程都有用户栈和内核栈（`p->kstack`）

当进程运行用户指令时，内核栈则是空的。

#### 代码：第一个地址空间

载入 *boot loader* 到内存并运行。开保护模式，开16位以上

*boot loader* 把 xv6 内核从磁盘中载入（到物理地址 0x100000 处）并从 `entry`（1040）开始运行。

`entry` 的代码设置页表，将 0x80000000（称为 `KERNBASE`（0207））开始的虚拟地址映射到物理地址 0x0 处，打开分页，开一个简单的栈，跳到main

`main` 初始化了一些设备和子系统后，它通过调用 `userinit`（1239）建立了第一个进程：

`userinit` 首先调用 `allocproc`，然后自己完成剩下的填充

调用 `scheduler` 开始运行进程，切换后进入initcode.S进程的内核栈上下文，到forkret，然后trapret，进入initcode.S，触发 `exec` ，手动载入参数init，调用init

init创建控制台设备文件，开启shell



### 分页

#### 硬件处理过程

一个 x86 页表就是一个包含 2^20（1,048,576）条*页表条目*（PTE）的数组。二级数组，页目录，页，PTE

* 虚拟地址高20位来找到该虚拟地址在页表中的索引

* （高 10 位来决定对应页目录条目。如果想要的条目已经放在了页目录中，分页硬件就会继续使用接下来的 10 位来从页表页中选择出对应的 PTE。否则，分页硬件就会抛出错误。）

* 高 20 位用 PTE 的 PPN，低 12 位原样复制


每个 PTE 都包含一些标志位，说明分页硬件对应的虚拟地址的使用权限。

#### 进程地址空间

`entry` 中建立的页表已经产生了足够多的映射来让内核的 C 代码正常运行。但是 `main` 还是调用了 `kvmalloc`（1757） 立即转换到新的页表中，这是因为内核建立的页表更加精巧地映射了内存空间

当进程向 xv6 要求更多的内存时，xv6 首先要找到空闲的物理页，然后把这些页对应的 PTE 加入该进程的页表中

xv6 在每个进程的页表中都包含了内核运行所需要的所有映射

有一些使用内存映射的 I/O 设备的物理内存在 0xFE000000 之上，对于这些设备 xv6 页表采用了直接映射。

* `KERNBASE` 之上的页对应的 PTE 中，`PTE_U` 位均被置 0，因而只有内核能够使用这些页。



现在来回顾一下，

* xv6 保证了每个进程只能使用其自己的内存，
* 每个进程所看到的内存都是从虚拟地址 0 开始的一段连续内存。
* 对于一个进程，xv6 只把该进程所使用的内存对应的 PTE 的 `PTE_U` 设为 1，其他 PTE 则不然



#### 物理内存的分配

xv6 使用从内核结尾到 `PHYSTOP` 之间的物理内存为运行时分配提供内存

kalloc，kfree，freelist

资源每次分配，它会将整块 4096 字节大小的页分配出去

先是entrypedir建立了一个简单的kernbase映射，启动main，main用kinit1，用简单的kernbase虚拟地址建了一个以内核内存终点为起始，4M为结束，的freelist（freelist记录的都是虚拟地址）

然后用这一点空间，建立了内核页表，其实pde就4K，kalloc了之后用kmap初始化

然后一堆初始化，在最后userinit之前调kinit2，建立了4M到phstop的freelist，用的还是kernbase的虚拟地址

在用kfree加到freelist的时候，清空那一页的时候，会自然的用内核页表找到物理地址（就是减去kernbase）

用户空间用自己的虚拟地址，构建页表的exec运行时用kalloc得到一个kernbase编译的虚拟地址，mappage往pde中放的时候，会减去kernbase，直接放入物理地址



堆在栈之上，它可以增长（通过 `sbrk`）

栈占用了单独的一页内存（只有一页），每次调用函数的时候会修改esp，ebp

栈下方有一个没有映射的保护页

exec  直接看注释吧



#### 现实情况

xv6 不能向磁盘中请求页，

没有实现 copy-on-write （写时复制）的 fork 操作、共享内存和惰性分配页（lazily-allocated page），也不能自动扩展栈

x86 支持段式内存转换（见附录 B），但xv6仅用它来实现 `proc` 这种有固定地址，但在不同 CPU 上有不同值的 per-CPU 变量（见 `seginit`）

对于不支持段式内存的体系结构而言，想要实现这种 per-CPU（或 per-thread）的变量，就必须要额外用一个寄存器来保存指向 per-CPU 数据区的指针。由于 x86 的寄存器实在太少，所以花费额外代价用段式内存来实现 per-CPU 变量是值得的。

？？？段式内存？



xv6 只在初始页表（1311）中使用了“超级页”。

* 数组的初始化设置了 1024 条 PDE 中的 2 条，即 0 号和 512 号（`KERBASE >> PDXSHIFT`），而其他的 PDE 均为 0。
* xv6 设置了这两条 PDE 中的 `PTE_PS` 位，标记它们为“超级页”。内核还通过设置 `%cr4` 中的 `CP_PSE（Page Size Extension）` 位来通知分页硬件允许使用超级页。

xv6 本来应该确定实际 RAM 的配置，而不是假设有 240MB 内存。

* 在 x86 上，至少有三个通用算法：第一种是探测物理地址空间，寻找像内存一样能够维持被写入数据的区域；第二种是从 PC 非易失性 RAM 中某个已知的 16-bit 位置处读取内存大小；第三种是在 BIOS 中查看作为多处理器表一部分的内存布局表。读取内存布局表是一项比较复杂的工作。

内存分配：内存高效使用，为可能出现的各种内存请求做好准备；相关资料请搜索 Knuth。

精巧的内核往往会分配不同大小的内存块，而不是像 xv6 一样固定分配 4096 字节；实际使用的内存分配器必须做到，对小块内存和大块内存的分配请求都能很好地处理。

#### 练习

1. 查看一下真实的操作系统的内存大小。
2. 如果 xv6 没有使用超级页，我们应该如何声明 `entrypgdir`？
3. Unix 在 `exec` 的实现中考虑了对 shell 脚本的特殊处理。执行一个以文本 `#!` 开头的文件时，第一行内容会被系统理解为执行该文件的翻译器。例如，如果用 `exec` 执行 `myprog arg1`，而 `myprog` 的第一行又是 `#!interp`，那么 `exec` 会运行命令 `/interp myprog arg1`。如何 xv6 中实现该功能？

### 陷入，中断和驱动程序

有些情况下用户程序需要进入内核，而不是执行下一条用户指令。

进入内核不也是命令嘛……13



进入内核：int中断，硬件中断时（从TSS里加载）（都是查表，用门转换）——保存上下文（不用用户栈，用户可能还没建立，硬件会使用任务段中指定的栈（就是加载TSS啦）），跳转（用内核栈，特权级）

磁盘可能在接受了一个进程的数据块之后发出一个中断，但是在中断的时候可能运行的是其他进程。

* 中断的这一特性使得思考中断的相关问题比陷入要难，因为中断和其它活动是并行的

当前执行指令的特权级存在于 %cs 寄存器中的 CPL 域中。（就是段描述符上的标志位）



![屏幕快照 2018-09-28 上午10.44.46](/Users/caowanlu/Desktop/md笔记中用到的图/屏幕快照 2018-09-28 上午10.44.46.png)一个 int 指令之后的栈的情况，保存用户态的上下文（如果没有特权级转换，不会保存 %ss 和 %esp）

iret 指令来从一个 int 指令中返回：弹出 int 指令保存的值



### 代码：汇编陷入处理程序

x86 允许 256 个不同的中断。中断 0-31 被定义为软件异常，比如除 0 错误和访问非法的内存页。xv6 将中断号 32-63 映射给硬件中断，并且用 64 作为系统调用的中断号。

`Tvinit` (3067) 在 `main` 中被调用，它设置了 `idt` 表（门描述符表）中的 256 个表项。

*  `vectors[i]` 放着中断处理代码。初始化门的时候会把位置vectors[i]的值放进去
  - 因为 x86 并未把中断号传递给中断处理程序，使用 `256` 个不同的处理程序是区分这 256 种情况的唯一办法

* 特别地：它通过传递第二个参数值为 1 来指定这是一个陷阱门。陷阱门不会清除 FL 位，这使得在处理系统调用的时候也接受其他中断。

- 同时也设置系统调用门的权限为 DPL_USER，这使得用户程序可以通过 int 指令产生一个内陷（访问TSS，转换到内核栈）

* xv6 不允许进程用 int 来产生其他中断（比如设备中断）；会抛出通用保护异常，就是13 号中断

用户进程的内核栈顶地址存入任务段描述符中，是在函数 `switchuvm`中进行的

xv6 使用一个 perl 脚本来产生“ IDT 表项指向的”中断处理函数入口点

每一个入口都会压入一个错误码（如果 CPU 没有压入的话），压入中断号，然后跳转到 `alltraps`

`Alltraps`继续压栈，形成tf

![屏幕快照 2018-09-28 上午11.16.23](/Users/caowanlu/Desktop/md笔记中用到的图/屏幕快照 2018-09-28 上午11.16.23.png)

 %eax存系统调用号，调用后保存处理结果

改段基，esp压栈（代表tf的地址），调用trap

trap 返回，弹出栈上的esp（用esp+4实现），执行 `trapret` ，trapret最后执行 了iret，会跳回到用户空间

### 代码：中断

添加一些简单设备的中断并不会增加太多额外的复杂性。中断可以使用与系统调用和异常处理相同的代码。

分时硬件大约以每秒 100 次的速度产生一个中断，内核可以据此对进程进行时钟分片

100 次每秒的速度足以提供良好的交互性能并且同时不会使处理器进入不断的中断处理中。

像 x86 处理器一样，PC 主板也在进步，并且提供中断的方式也在进步。

* 早期的主板有一个简单的可编程中断控制器（被称作 PIC），你可以在 picirq.c 中找到管理它的代码。
* 随着多核处理器主板的出现，需要一种新的处理中断的方式，因为每一颗 CPU 都需要一个中断控制器来处理发送给它的中断，而且也得有一个方法来分发中断。
* 这一方式包括两个部分：第一个部分是在 I/O 系统中的（IO APIC，ioapic.c），另一部分是关联在每一个处理器上的（局部 APIC，lapic.c）。
* xv6 是为搭载多核处理器的主板设计的，每一个处理器都需要编程接受中断。

为了在单核处理器上也能够正常运行

* xv6 也为 PIC 编程（6932）。每一个 PIC 可以处理最多 8 个中断（设备）并且将他们接到处理器的中断引脚上。
* 为了支持多于八个硬件，PIC 可以进行级联，典型的主板至少有两集级联。使用 `inb` 和 `outb` 指令，xv6 配置主 PIC 产生 IRQ 0 到 7，从 PIC 产生 IRQ 8 到 16。
* 最初 xv6 配置 PIC 屏蔽所有中断。timer.c 中的代码设置时钟 1 并且使能 PIC 上相应的中断（7574）。
* 这样的说法忽略了编写 PIC 的一些细节。这些 PIC（也包括 IOAPIC 和 LAPIC）的细节对本书来说并不重要，但是感兴趣的读者可以参考 xv6 源码引用的各设备的手册。

在多核处理器上

* xv6 必须编写 IOAPIC 和每一个处理器的 LAPIC。
* IO APIC 维护了一张表，处理器可以通过内存映射 I/O 写这个表的表项，而非使用 `inb` 和 `outb` 指令。
* 在初始化的过程中，xv6 将第 0 号中断映射到 IRQ 0，以此类推，然后把它们都屏蔽掉。
* 不同的设备自己开启自己的中断，并且同时指定哪一个处理器接受这个中断。举例来说，xv6 将键盘中断分发到处理器 0（7516）。将磁盘中断分发到编号最大的处理器，你们将在下面看到。

时钟芯片在 LAPIC 中，每一个处理器可以独立地接收时钟中断。

* xv6 在 `lapicinit`（6651）中设置时钟中断
* 关键的一行代码`timer`（6664）告诉 LAPIC 周期性地在 IRQ_TIMER（也就是 IRQ 0) 产生中断。第 6693 行打开 CPU 的 LAPIC 的中断，这使得 LAPIC 能够将中断传递给本地处理器

处理器可以通过设置 `eflags` 寄存器中的 `IF` 位来控制自己是否想要收到中断。

* 指令 `cli` 通过清除 `IF` 位来屏蔽中断，而 `sti` 又打开一个中断。
* xv6 在启动主 cpu（8412）和其他 cpu（1126）时屏蔽中断。每个处理器的调度器打开中断（2464）。
* 为了控制一些特殊的代码片段不被中断，xv6 在进入这些代码片段之前关中断（例如 `switchuvm`（1773））。

xv6 在 `idtinit`（1265）中设置时钟中断触发中断向量 32（xv6 使用它来处理 IRQ 0）

* 中断向量 32 和中断向量 64（用于实现系统调用）的唯一区别就是 32 是一个中断门，而 64 是一个陷阱门。
* 中断门会清除 IF，所以被中断的处理器在处理当前中断的时候不会接受其他中断。从这儿开始直到 `trap` 为止，中断执行和系统调用或异常处理有相同的代码——建立中断帧。

当因时钟中断而调用 `trap` 时，`trap` 只完成两个任务：递增时钟变量的值（3064），并且调用 `wakeup`。后者可能会使得中断返回到一个不同的进程

### 驱动程序

驱动程序是操作系统中用于管理某个设备的代码

它提供设备相关的中断处理程序，操纵设备完成操作，操纵设备产生中断，等等

驱动程序可能会非常难写，因为它和它管理的设备同时在并发地运行着；必须要理解设备的接口（例如，哪一个 I/O 端口是做什么的）。设备的接口有可能非常复杂并且文档稀缺。

磁盘驱动程序从磁盘上拷出和拷入数据。

* 磁盘硬件一般将磁盘上的数据表示为一系列的 512 字节的块（亦称扇区）
* 扇区 0 是最初的 512 字节，扇区 1 是下一个，以此类推。
* 为了表示磁盘扇区，操作系统也有一个数据结构与之对应。
* 这个结构中存储的数据往往和磁盘上的不同步：可能还没有从磁盘中读出（磁盘正在读数据但是还没有完全读出），或者它可能已经被更新但还没有写出到磁盘。
* 磁盘驱动程序必须保证 xv6 的其他部分不会因为不同步的问题而产生错误。

### 代码：磁盘驱动程序

通过 IDE 设备可以访问连接到 PC 标准 IDE 控制器上的磁盘。IDE 现在不如 SCSI 和 SATA 流行，它的接口比较简单

磁盘驱动程序用结构体 buf（称为缓冲区）（3500）来表示一个磁盘扇区

* 每一个缓冲区表示磁盘设备上的一个扇区。
* 域 `dev` 和 `sector` 给出了设备号和扇区号，域 `data` 是该磁盘扇区数据的内存中的拷贝。域 `flags` 记录了内存和磁盘的联系：B_VALID 位代表数据已经被读入，B_DIRTY 位代表数据需要被写出。B_BUSY 位是一个锁；它代表某个进程正在使用这个缓冲区，其他进程必须等待。当一个缓冲区的 B_BUSY 位被设置，我们称这个缓冲区被锁住

内核在启动时通过调用 `main`（1234）中的 `ideinit`（3851）初始化磁盘驱动程序。

* `ideinit` 调用 `picenable` 和 `ioapicenable` 来打开 `IDE_IRQ` 中断（3856-3857）。
* 调用 `picenable` 打开单处理器的中断；`ioapicenable` 打开多处理器的中断，但只是打开最后一个 CPU 的中断（`ncpu-1`）：在一个双处理器系统上，CPU 1 专门处理磁盘中断。
* 接下来，`ideinit` 检查磁盘硬件。
* 它最初调用 `idewait`（3858）来等待磁盘接受命令。
* PC 主板通过 I/O 端口 0x1f7 来表示磁盘硬件的状态位。`idewait`（3833）获取状态位，直到 busy 位（IDE_BSY）被清除，以及 ready 位（IDE_DRDY)被设置。

现在磁盘控制器已经就绪，`ideinit` 可以检查有多少磁盘。

* 它假设磁盘 0 是存在的，因为启动加载器和内核都是从磁盘 0 加载的，但它必须检查磁盘 1。它通过写 I/O 端口 0x1f6 来选择磁盘 1 然后等待一段时间，获取状态位来查看磁盘是否就绪（3860-3867）。如果不就绪，`ideinit` 认为磁盘不存在，不再往后查，返回

`ideinit` 之后，通过块高速缓冲（buffer cache）调用 `iderw`

* `iderw` 根据标志位更新一个锁住的缓冲区。如果 B_DIRTY 被设置，iderw 将缓冲区的内容写到磁盘；如果 B_VALID 没有被设置，iderw 从磁盘中读出数据到缓冲区

磁盘访问耗时在毫秒级，对于处理器来说是很漫长的。

引导加载器发出磁盘读命令并反复读磁盘状态位直到数据就绪。这种轮询或者忙等待的方法对于引导加载器来说是可以接受的，因为没有更好的事儿可做。

但是在操作系统中，更有效的方法是让其他进程占有 CPU 并且在磁盘操作完成时接受一个中断。

`iderw` 采用的就是后一种方法，维护一个等待中的磁盘请求队列，然后用中断来指明哪一个请求已经完成

虽然 `iderw` 维护了一个请求的队列，简单的 IDE 磁盘控制器每次只能处理一个操作。

`iderw`（3954）将缓冲区 `b` 送到队列的末尾（3967-3971）。

* 如果这个缓冲区在队首，`iderw` 通过 `idestart` 将它送到磁盘上（3924-3926）；在其他情况下，一个缓冲区被开始处理当且仅当它前面的缓冲区被处理完毕。

`idestart` 发出关于缓冲区所在设备和扇区的读或者写操作（由标志位指明）

* 写操作：`idestart` 必须提供数据（3889）而在写出到磁盘完成后会发出一个中断。
* 读操作，则发出一个代表数据就绪的中断，然后中断处理程序会读出数据。
* 注意 `iderw` 有一些关于 IDE 设备的细节，并且在几个特殊的端口进行读写。如果任何一个 `outb` 语句错误了，IDE 就会做一些我们意料之外的事。保证这些细节正确也是写设备驱动程序的一大挑战。

`iderw` 已经将请求添加到了队列中，并且会在必要的时候开始处理， 还必须等待结果。

`iderw` 睡眠，等待中断处理程序在操作完成时更新缓冲区的标志位（3978-3979）。

当这个进程睡眠时，xv6 会调度其他进程来保持 CPU 处于工作状态。

最终，磁盘会完成自己的操作并且触发一个中断。

* `trap` 会调用 `ideintr` 来处理（3124）
* `ideintr`（3902）查询队列中的第一个缓冲区，看正在发生什么操作
* 缓冲区正在被读入并且磁盘控制器有数据在等待：`ideintr` 调用 `insl` 将数据读入缓冲区（3915-3917）。
* 缓冲区就绪：`ideintr` 设置 B_VALID，清除 B_DIRTY，唤醒任何一个睡眠在这个缓冲区上的进程（3919-3922），将下一个等待中的缓冲区传递给磁盘（3924-3926）

### 实际情况

很多操作系统当中，各种驱动合起来的代码数量要比系统内核的数量更多。

实际的设备驱动远比这一章的磁盘驱动要复杂的多，但是他们的基本思想是一样的：设备通常比 CPU 慢，所以硬件必须使用中断来提醒系统它的状态发生了改变。现代磁盘控制器一般在同一时间接受多个未完成的磁盘请求，甚至重排这些请求使得磁盘使用可以得到更高的效率。当磁盘没有这项功能时，操作系统经常负责重排请求队列。

其他硬件和磁盘非常的相似：网络设备缓冲区保存包，音频设备缓冲区保存音频采样，显存保存图像数据和指令序列。

高带宽的设备，如硬盘，显卡和网卡在驱动中同样都使用直接内存访问（Direct memory access, DMA）而不是直接用 I/O(`insl`, `outsl`)。DMA 允许磁盘控制器或者其他控制器直接访问物理内存。驱动给予设备缓存数据区域的物理地址，可以让设备直接地从主存中读取或者写入，一旦复制完成就发出中断。使用 DMA 意味着 CPU 不直接参与传输，这样做可以提高CPU工作效率，并且 CPU Cache 开销也更小。

在这一章中的绝大多数设备使用了“ I/O 指令”来进行编程，但这都是针对老设备的了。而所有现代设备都使用“内存映射 I/O” （Memory-mapped I/O）来进行编程。

有些设备动态地在轮询模式和中断模式之间切换，因为使用中断的成本很高，但是在驱动去处理一个事件之前，使用轮询会导致延迟。举个例子，对于一个收到大量包的网络设备来说，可能会从中断模式到轮询模式之间切换，因为它知道会到来更多的包被处理，使用轮询会降低处理它们的成本。一旦没有更多的包需要处理了，驱动可能就会切换回中断模式，使得当有新的包到来的时候能够被立刻通知。

IDE 硬盘的驱动静态的发送中断到一个特定的处理器上。有些驱动使用了复杂的算法来发送中断，使得处理负载均衡，并且达到良好的局部性。例如，一个网络驱动程序可能为一个“网络连接的包”向“处理这个连接的处理器”发送一个中断，而来自其他连接的包的中断发送给另外的处理器。这种分配方式很复杂;例如，如果有某些网络连接的活动时间很短，但是其他的网络连接却很长，这时候操作系统就要保持所有的处理器都工作来获得一个高的吞吐量。

用户在读一个文件的时候，这个文件的数据将会被拷贝两次。第一次是由驱动从硬盘拷贝到内核内存（buf），之后通过 read 系统调用，从内核内存拷贝到用户内存。

（buf块是固定的（buf结构数组编译之后就在kernel的全局变量区，这个空间之后就没再释放过），进程访问buf中的数据需要系统调用read复制到自己的内存）

同理当在网络上发送数据的时候，数据也是被拷贝了两次：先是从用户内存到内核空间，然后是从内核空间拷贝到网络设备（网络传输也有类似buf块的机制吗？）

对于很多程序来说低延迟是相当重要的（比如说 Web 服务器服务一个静态页面），操作系统使用了一些特别的代码来避免这种多次拷贝。一个在真实世界中的例子就是缓冲区大小通常是符合内存页大小的，这使得只读的数据拷贝可以直接通过分页映射到进程的地址空间，而不用任何的复制。

（如果buf和页对齐，就可以不用复制，通过设置页表直接引用）

### 练习

1. 在 `syscall()` 的第一条指令处设置一个断点来截获第一次系统调用（例如 `br syscall`）。此时栈上有一些什么值？解释这个断点下执行 `x/37x $esp` 的输出，对每一个值说明它的含义（如为 trap 保存的 %ebp，trapframe.eip，临时分配空间等等）。
2. 添加一个新的系统调用
3. 添加一个网络驱动







### 锁



cpu间，中断与非中断代码间，共享的数据，原子操作

（在没实现锁的时候，acquire里面的原子操作是xchg实现的。“spinlock 用于CPU同步, 它的实现是基于CPU锁定数据总线的指令”）

什么时候使用锁，这就需要程序员对哪里可能出现干扰有很深入完备的分析。记得锁的两个要素：实现cpu的原子操作，保护共享数据

锁的使用时机，当且仅当：修改共享数据的时候（防止覆盖），访问共享数据的时候（可能其他cpu进入修改），交互信息修改（比如状态）需要保证原子操作的时候

cpu共享数据：ptable，log，磁盘（通过每个buf锁保护），bcache，buf，icache

锁提供互斥功能，保证某个时间点只有一个 CPU 能持有锁

锁的保护机制就体现在每次acquire的时候，要保护某个数据，就在要修改那个数据的时候acquire它的锁，如果别人在修改，acquire就不会成功，因此修改不了，这就算保护

只是有时候仅仅是保护一个小域，却因为一个结构只有这一个锁而导致其他acquire也只能acquire这个锁，就算是其他域也修改不了



系统设计力求简单、模块化的抽象

锁的机制则和这种模块化理念有所冲突。现在还没有一种透明方案可以让调用者和被调者互相隐藏所使用的锁

#### 使用锁

锁会降低并发度，要避免过度使用锁

* 当效率不是很重要的时候，可以使用单处理器计算机，这样完全不用考虑锁

锁的粒度选择是并行编程中的一个重要问题

xv6 只使用了几个简单的锁；例如，xv6 中使用了一个单独的锁来保护进程表及其不变量，我们将在第5章讨论这个问题

更精细的做法是给进程表中的每一个条目都上一个锁，这样在不同条目上运行的线程也能并行了

然而，当一个操作需要维持关于整个进程表的不变量时，这样的做法会让情况变得很复杂，因为此时它可能需要持有多个锁

#### 锁的顺序

如果某些代码段要使用多个锁，注意每次运行都要以相同的顺序获得锁，否则两块并行的此类代码可能形成思索（各自占一个，都走不下去）

xv6最长的锁链也就只有两个锁

#### 中断处理程序

中断也可能导致并发

在允许中断时，内核代码可能在任何时候停下来，包括持有锁的时候，然后执行中断处理程序，中断处理程序可能也需要锁

为了避免这种情况，当中断处理程序会使用某个锁时，处理器就不能在允许中断发生时持有该锁。

xv6 做得更决绝：允许中断时不能持有任何锁。

* `acquire` 在尝试获得锁之前调用了 `pushcli`，`release` 则在释放锁后调用了 `popcli`
* `pushcli`和 `popcli`包装了 `cli` 和 `sti`，做了计数工作，每次的都count

#### 内存乱序

许多处理器会通过指令乱序来提高性能：如果一个指令需要多个周期完成，处理器会希望这条指令尽早开始执行，这样就能与其他指令交叠，避免延误太久

所以有的地方需要锁来强制原子操作，我还记得见到过关闭乱序的机器指令

#### 现实情况

使用了锁机制的程序编写仍然是个巨大的挑战，并发和并行至今还是研究的热点。我们最好以锁为基础来构建高级的同步队列，虽然 xv6 并没有这么做

最好用一些工具来确定竞争条件，否则很容易遗漏掉某些需要锁保护的不变量

用户级程序也需要锁，但 xv6 的程序只有一个运行线程，进程间也不会共享内存，所以就不需要锁了

当然我们也有可能用非原子性的操作来实现锁，只不过那样做代价很高，而且大多数的操作系统都是使用了原子操作的。

原子操作的代价也不小。如果一个处理器在它的本地缓存中有一个锁，而这时另一个处理器必须获得该锁，那么更新缓存中该行的原子操作就必须把这行从一个处理器的缓存中移到另一个处理器的缓存中（为什么？？？），同时还可能需要让这行缓存的其他备份失效。从其他处理器的缓存中取得一行数据要比从本地缓存中取代价大得多

（没看懂这段……）

为了减少使用锁所产生的代价，许多操作系统使用了锁无关的数据结构和算法，并在这些算法中尽量避免原子操作。例如，对于本章开篇提到的链表，我们在查询时不需要获得锁，然后用一个原子操作来添加元素

#### 练习

1. 若在 `acquire` 中不用 `xchg`，运行 xv6 会发生什么情况？
2. 把 `iderw` 中的 `acquire` 移到 `sleep` 之前会出现竞争吗？你可以通过运行 xv6 并运行 `stressfs` 来观察。用简单的循环扩大临界区看看会发生什么，并对此作出解释。
3. 完成公布的作业。
4. 在缓冲区的 `flags` 中置位并不是原子操作：处理器会在寄存器中拷贝一份 `flags`，修改寄存器然后写回去。所以两个处理器不能同时写 `flags`。xv6 只在持有 `buflock` 的时候修改 `B_BUSY`，但修改 `B_VALID` 和 `B_WRITE` 的时候并没有锁。为什么这么做仍然是安全的呢？



### 调度

#### 多路复用

当一个进程等待磁盘请求时，xv6 使之进入睡眠状态，然后调度执行另一个进程

当一个进程耗尽了它在处理器上运行的时间片（100毫秒）后，xv6 使用时钟中断强制它停止运行，调度运行其他进程。

进程切换：内核态，上下文切换，swtch，调度器线程与进程线程，scheduler和sleep（或yield）中的swach通过跳转衔接

第三，多个 CPU 同时切换进程的情况，必须使用一个带锁的方案来避免竞争（？？？为什么）

第四，进程退出时必须释放其占用内存与资源

* 由于它本身在使用自己的资源（譬如其内核栈），不能由该进程本身释放其占有的所有资源。

xv6 为进程间互相协作的时候，就会出现等待，等待的时候就是可以挂起的时候

父进程需要等待子进程结束，以及读取管道数据的进程需要等待其他进程向管道中写入数据

#### 代码：调度

进程要让出 CPU ：

* 获得进程表的锁 `ptable.lock`
* 释放其拥有的其他锁
* 修改自己的状态（`proc->state`）
* 然后调用 `sched`。

`sched` ：

* 检查两次状态（2507-2512）（进程此时持有锁，CPU 应该是在中断关闭的情况下运行的）
* 用swtch返回调度器

`scheduler`（2458）运行了一个普通的循环：找到一个进程来运行，运行直到其停止，然后继续循环。

`scheduler` 大部分时间里都持有 `ptable.lock`，但在每次外层循环中都要释放该锁（并显式地允许中断）

* 当 CPU 闲置（找不到 `RUNNABLE` 的进程）时这样做十分有必要。
* 如果一个闲置的调度器一直持有锁，其他 CPU 就不可能执行上下文切换或任何和进程相关的系统调用，也就不能将某个进程标记为 `RUNNABLE` 让闲置的调度器能够跳出循环
* 周期性地允许中断，是因为可能进程都在等待 I/O，从而找不到一个 `RUNNABLE` 的进程（例如 shell）；如果调度器一直不允许中断，I/O 就永远无法到达了



从另一个层面研究这段调度代码：

对于每个进程，调度维护了进程的一系列固定状态，并且保证当状态变化时必须持有锁 `ptable.lock`。

第一个固定状态：如果进程为 `RUNNING` 的，那么必须确保使用时钟中断的 `yield` 时，能够无误地切换到其他进程；

* CPU 寄存器必须保存着进程的寄存器值（这些寄存器值并非在 `context` 中）`%cr3` 必须指向进程的页表，`%esp` 则要指向进程的内核栈，这样 `swtch` 才能正确地向栈中压入寄存器值，另外 `proc` 必须指向进程的 `proc[]` 槽中

另一个固定状态：如果进程是 `RUNNABLE`，必须保证调度器能够无误地调度执行该进程；

* `p->context` 必须保存着进程的内核线程变量，并且没有任何 CPU 此时正在其内核栈上运行，没有任何 CPU 的 `%cr3` 寄存器指向进程的页表，也没有任何 CPU 的 `proc` 指向该进程。

由于要坚持以上两个原则，xv6 必须在一个线程中获得 `ptable.lock`（通常是在 `yield` 中），然后在另一个线程中释放这个锁（在调度器线程或者其他内核线程中）

如果一段代码想要将运行中进程的状态修改为 `RUNNABLE`，那么在恢复到固定状态中之前必须持有锁；

最早的可以释放锁的时机是在 `scheduler` 停止使用该进程页表并清空 `proc` 时

类似地，如果 `scheduler`想把一个可运行进程的状态修改为 `RUNNING`，在该进程的内核线程完全运行起来（`swtch` 之后，例如在 `yield` 中）之前必须持有锁

除此之外，`ptable.lock` 也保护了一些其他的状态：进程 ID 的分配，进程表槽的释放，`exit` 和 `wait` 之间的互动，保证对进程的唤醒不会被丢失等等

我们应该思考一下 `ptable.lock` 有哪些不同的功能可以分离，使之更为简洁高效。

#### 睡眠与唤醒

用锁保证状态修改为原子操作；sleep内部有一次锁的释放与获取

##### *从阻塞到睡眠唤醒*

睡眠和唤醒实际上就提供了进程间通信的机制：让一个进程暂时休眠，等待某个特定事件的发生，当发生时，另一个进程唤醒该进程

睡眠与唤醒通常被称为*顺序合作（sequence coordination）*或者*有条件同步（conditional synchronization）*机制，在操作系统的哲学中，还有很多类似的机制。

为了说明，假设有一个生产者/消费者队列

* 这个队列有些类似于 IDE 驱动用来同步处理器和设备驱动的队列（见第3章），不过下面所讲的更能概括 IDE 驱动中的代码

该队列允许一个进程将一个非零指针发送给另一个进程（信息交换）

假设只有一个发送者和一个接受者，并且它们运行在不同的 CPU 上，那么下面的实现显然是正确的：

```
struct q {
    void *ptr;
};

void*
send(struct q *q, void *p)
{
    while(q->ptr != 0)
        ;
    q->ptr = p;
}

void*
recv(struct q * q)
{
    void *p;
    while((p = q->ptr) == 0)
        ;
    q->ptr = 0;
    return p;
}
```

`send` 会不断循环，直到队列为空（`ptr == 0`），然后将指针 `p` 放到队列中。`recv` 会不断循环，直到队列非空然后取出指针

当不同的进程运行时，`send` 和 `recv` 会同时修改 `q->ptr`， `send` 只在队列空时写入指针，而 `recv` 只在队列非空时拿出指针

***到睡眠唤醒***

上面这种实现方法是代价是巨大的：如果发送者很少发送，那么接受者就会消耗大量的时间在 `while` 循环中苦苦等待一个指针的出现。

如果有一种方法使得 `send` 放入指针时，能够通知接受者（就是wake那个channel呀），接受者所在的 CPU 就不用一直循环，能在这段时间找到更有意义的事情做。

考虑一对调用 `sleep` 和 `wakeup`，其工作方式如下：

* `sleep(chan)` 让进程在任意的 `chan` 上休眠，称之为*等待队列（wait channel），*释放所占 CPU。
* `wakeup(chan)` 唤醒在 `chan` 上休眠的所有进程，让他们的 `sleep` 调用返回。
* 如果没有进程在 `chan` 上等待唤醒，`wakeup` 就什么也不做

我们用 `sleep` 和 `wakeup` 来重新实现上面的代码：

```
void*
send(struct q *q, void *p)
{
    while(q->ptr != 0)
        ;
    q->ptr = p;
    wakeup(q);    /*wake recv*/
}

void*
recv(struct q *q)
{
    void *p;
    while((p = q->ptr) == 0)
        sleep(q);
    q->ptr = 0;
    return p;
}
```

***到保证共享信息的原子操作***

[![figure5-2](https://gitee.com/senjienly/xv6-chinese/raw/master/pic/f5-2.png)](https://gitee.com/senjienly/xv6-chinese/raw/master/pic/f5-2.png)

但对于图表5-2中出现的“遗失的唤醒”问题，我们很难通过已有的接口，直观地设计出能够避免该问题的 `sleep` 和 `wakeup` 机制。

* 假设在第215行 `recv` 发现 `q->ptr == 0`，然后决定调用 `sleep`，
* 但是在 `recv` 调用 `sleep` 之前（譬如这时处理器突然收到一个中断然后开始执行中断处理，延迟了对 `sleep` 的调用），`send` 又在另一个 CPU 上运行了，它将 `q->ptr` 置为非零，然后调用 `wakeup`，发现没有进程在休眠，于是什么也没有做。
* 接着，`recv` 从第216行继续执行了：它调用 `sleep` 进入休眠
* 这就出现问题了，休眠的 `recv` 实际上在等待一个已经到达的指针。而下一个 `send` 又在睡眠中等着 `recv` 取出队列中的指针。这种情况就被称为*死锁（deadlock）*。

这个问题的根源在于没有维持好一个固定状态：由于 `send` 在错误的时机运行了，使得 `recv` 只能在 `q->ptr == 0` 时睡眠这个行为被妨碍了。

下面我们还将看到一段能保护该固定状态但仍有问题的代码：

```
struct q {
    struct spinlock lock;
    void *ptr;
};

void *
send(struct q *q, void *p)
{
    acquire(&q->lock);
    while(q->ptr != 0)
        ;
    q->ptr = p;
    wakeup(q);
    release(&q->lock);
}

void*
recv(struct q *q)
{
    void *p;
    acquire(&q->lock);
    while((p = q->ptr) == 0)
        sleep(q);
    q->ptr = 0;
    release(&q->lock;
    return p;
}
```

由于要调用 `sleep` 的进程是持有锁 `q->lock` 的， `send` 想要调用 `wakeup` 也必须获得锁，所以这种方案能够保护上面讲到的固定状态。

***到睡眠时释放锁***

但是这种方案也会出现死锁：当 `recv` 带着锁 `q->lock` 进入睡眠后，发送者就会在希望获得锁时一直阻塞。

要解决问题，我们必须要改变 `sleep` 的接口

`sleep` 必须将锁作为一个参数，然后在进入睡眠状态后释放之，一旦进程被唤醒了，`sleep` 在返回之前还需要重新获得锁。

于是我们应该使用下面的代码：

```
struct q {
    struct spinlock lock;
    void *ptr;
};

void *
send(struct q *q, void *p)
{
    acquire(&q->lock);
    while(q->ptr != 0)
        ;                  //这里也该加上sleep的
    q->ptr = p;
    wakeup(q);
    release(&q->lock);
}

void*
recv(struct q *q)
{
    void *p;
    acquire(&q->lock);
    while((p = q->ptr) == 0)
        sleep(q, &q->lock);
    q->ptr = 0;
    release(&q->lock;
    return p;
}
```

`sleep` 能用原子操作释放 `q->lock` 并让接收进程进入休眠状态。

完整的发送者/接收者的实现还应该让发送者在等待接收者拿出前一个 `send` 放入的值时处于休眠状态。

#### 代码：睡眠与唤醒

接下来让我们看看 xv6 中 `sleep` 和 `wakeup` 的实现。

总体思路是希望 `sleep` 将当前进程转化为 `SLEEPING` 状态并调用 `sched` 以释放 CPU，而 `wakeup` 则寻找一个睡眠状态的进程并把它标记为 `RUNNABLE`。



`wackup` 必须在有一个监视唤醒条件的锁的时候才能被调用：就是保护当前队列不变量的那个锁

有些情况下可能有多个进程在同一队列中睡眠



`sleep` 和 `wakeup` 的调用者可以使用任何方便使用的数字作为队列号码

* 实际上，xv6 通常使用内核中和等待相关的数据结构的地址，譬如磁盘缓冲区
* 即使两组 `sleep`/`wakeup` 使用了相同的队列号码，也无妨：对于那些无用的唤醒，它们会通过不断检查状态忽略之。

`sleep`/`wakeup` 的优点主要是其轻量级（不需另定义一个结构来作为睡眠队列），并且提供了一层抽象（调用者不需要了解与之交互的是哪一个进程）。

#### 代码：管道

`sleep`/`wakeup` 是管道的重要部分

一块封装的内存区，两个文件接口，几个操作函数（alloc，close，read，write），read和write会用sleep顺便wake对方的chan

#### 代码：`wait`, `exit`, `kill`

sleep和wake的其他应用

父进程可以调用 `wait` ，wait用sleep等待一个子进程退出

子进程在 `exit` 时已经做好了大部分的清理工作，但父进程一定要为其释放 `p->kstack` 和 `p->pgdir`；



`kill`（2625）则让一个应用程序可以终结其他进程。

* 它在进程表中设置被终结的进程的 `p->killed`，如果这个进程在睡眠中则唤醒之
* 如果被终结的进程正在另一个处理器上运行，它总会通过系统调用或者中断（例如时钟中断）进入内核

相应的关键操作之前检查killed值：

所以 xv6 谨慎地在调用 `sleep` 时使用了 `while` 循环，检查 `p->killed`是否被设置了

* 若是，则返回到调用者。调用者也必须再次检查 `p->killed` 是否被设置，若是，返回到再上一级调用者，依此下去
* 最后进程的栈展开（unwind）到了 `trap`，`trap` 若检查到 `p->killed` 被设置了，则调用 `exit` 终结自己
* 我们已经在管道的实现中（6087）看到了在 `sleep` 外的 `while` 循环中检查 `p->killed`。

有一处的 `while` 没有检查 `p->killed`：ide 驱动（3979）直接重新调用了 `sleep`

为什么？？？

之所以可以确保能被唤醒，是因为它在等待一个磁盘中断，如果有第二个进程在中断之前调用了 `iderw`，`ideintr` 会唤醒该进程（第二个），而非原来等待中断的那一个（第一个）进程。第二个进程会认为它收到了它正在等待的数据，但实际上它收到的是第一个进程想要读的数据

（？这一段什么意思？）

#### 现实情况

xv6 所实现的调度算法非常朴素，仅仅是让每个进程轮流执行。这种算法被称作*轮转法（round robin）*

真正的操作系统使用了更为复杂的算法，例如让每个进程都有优先级

由于要权衡多项指标，例如要保证公平性和高的吞吐量，调度算法往往很快变得复杂起来。

另外，复杂的调度算法还会无意中导致像*优先级倒转（priority inversion）*和*护航（convoy）*这样的现象

* 优先级倒转是指当高优先级进程和低优先级进程共享一个锁时，如果锁已经被低优先级进程获得了，高优先级进程就无法运行了
* 护航则是指当很多高优先级进程等待一个持有锁的低优先级进程的情况，护航一旦发生，则可能持续很久
* 如果要避免这些问题，就必须在复杂的调度器中设计更多的机制。

`sleep` 和 `wakeup` 是非常普通但有效的同步方法，当然还有很多其他的同步方法。

同步要解决的第一个问题是本章开始我们看到的“丢失的唤醒”问题。

* 原始的 Unix 内核的 `sleep` 只是简单地屏蔽中断，由于 Unix 只在单处理器上运行，所以这样已经足够了
* 由于 xv6 要在多处理器上运行，所以它给 `sleep` 增加了一个现实的锁
* FreeBSD 的 `msleep` 也使用了同样的方法，Plan 9 的 `sleep` 使用了一个回调函数，并在其返回到 `sleep` 中之前持有调度用的锁；这个函数对睡眠状态作了最后检查，以避免丢失的唤醒。
* Linux 内核的 `sleep` 用一个显式的进程队列代替 xv6 中的等待队列（wait channel）；而该队列本身内部还有锁。

在 `wakeup` 中遍历整个进程表来寻找对应 `chan` 的进程是非常低效的。更好的办法是用另一个结构体代替 `sleep` 和 `wakeup`中的 `chan`，该结构体中维护了一个睡眠进程的链表

* Plan 9 的 `sleep` 和 `wakeup` 把该结构体称为*集合点（rendezvous point）*或者 `Rendez`
* 许多线程库都把相同的一个结构体作为一个状态变量；如果是这样的话，`sleep` 和 `wakeup` 操作则被称为 `wait` 和 `signal`



所有此类机制都有同一个思想：使得睡眠状态可以被某种执行原子操作的锁保护。

惊群：

* 在 `wakeup` 的实现中，它唤醒了特定队列中的所有进程，而有可能很多进程都在同一个队列中等待被唤醒。
* 操作系统会调度这里的所有进程，它们会互相竞争以检查其睡眠状态。
* 这样的一堆进程被称作*惊群（thundering herd）*
* 我们最好是避免这种情况的发生。大多数的状态变量都有两种不同的 `wakeup`，一种唤醒一个进程，即 `signal`；另一种唤醒所有进程，即 `broadcast`。

信号量是另一种合作机制

* 一个信号量是一个提供两种操作，即加和减的整数
* 可以不断对一个信号量进行加操作，但是只能在信号量为正时进行减操作，对为零的信号量减时，进程会进入睡眠，直到另一个进程对其进行加操作，这对加减操作就会抵消掉
* 这个整数实际上反映了一个真正的计数值，例如管道缓冲区中可写的字节数，或者进程拥有的僵尸子进程数
* 在这个抽象中显式地计数可以避免“丢失的唤醒”问题：通过显式地记录唤醒次数。这样计数同时还能避免无谓的唤醒和惊群问题。

终结、清理进程是 xv6 中较为复杂的内容，大多数操作系统则更为复杂

* 例如，被终结的进程可能在内核深处睡眠，我们需要谨慎地编写代码以展开其栈
* 许多操作系统使用显式的处理异常的机制来展开栈，例如使用 `longjmp`。
* 另外，有些事件可能让睡眠的进程被唤醒，即使这些事件尚未发生。例如，当进程在睡眠时，另一个进程向它发送一个*信号*，进程就会从被打断的系统调用中返回值-1并在 `EINTR` 中存放错误代码。应用程序可以查看这些值以决定下一步怎么做。（？？？？）
* xv6 并不支持信号，所以没有这么复杂。

### 文件系统

数据组织：进程共享（组织，数据），稳定（崩溃恢复）

xv6 的文件系统中使用了类似 Unix 的文件，文件描述符，目录和路经名（参阅第零章），并且把数据存储到一块 IDE 磁盘上



### 概述

xv6 的文件系统分6层实现

* 块缓冲读写 IDE 硬盘
* 磁盘的更新按会话打包，通过会话保证原子操作（要么都被应用，要么都不被应用）
* 无名文件 i 节点
* 目录项 i 节点
* 层次路经名。通过递归的方式来查询路径对应的文件
* 资源（如管道，设备，文件等）抽象为文件描述符

[![figure6-2](https://gitee.com/senjienly/xv6-chinese/raw/master/pic/f6-2.png)](https://gitee.com/senjienly/xv6-chinese/raw/master/pic/f6-2.png)

### 块缓冲层

维护：buf结构（bread，bwrite），活跃buf链表bcache（binit，bget，brelse）

### 代码：块缓冲

睡眠只发生在了idrew更新buf的时候，进程对buf的访问是不会sleep的，只有在修改buf信息的时候加了几个锁

### 日志层

错误恢复

一个文件涉及多个block的写，如果在写操作的过程当中系统崩溃，磁盘上的文件系统就会处于不一致的状态中

* 举例来说，根据写的顺序的不同，上述错误可能会导致一个目录项指向一个空闲的 i 节点，或者产生一个已被分配但是未被引用的块
* 后一种情况相对来说好一些，但在前一种情况中，目录项指向了一个空闲的 i 节点，重启之后就会导致非常严重的问题。

xv6 用一个日志块做中介，日志写到一半没关系，不更新到磁盘就好了，写好了更新，更到一半断了也不怕，按日志重新来就好



会话:一个必须从头到尾原子完成的写操作序列（比数据库中的会话要简单得多）

任何时候只能有一个进程在一个会话之中，其他进程必须等待当前会话中的进程结束。同一时刻日志最多只记录一次会话。

xv6 不允许并发会话，是为了避免：

* 假设会话 X 把一个对 i 节点的修改写入了会话中，并发的会话 Y 从同一块中读出了另一个 i 节点，更新了它，把 i 节点块写入了日志并且提交。这就会导致可怕的后果：
* Y 的提交导致被 X 修改过的 i 节点块被写入磁盘，而 X 此时并没有提交它的修改。如果这时候发生崩溃会使得 X 的修改只应用了一部分而不是全部，从而打破会话是原子的这一性质
* 有一些复杂的办法可以解决这个问题，但 xv6 直接通过不允许并行的会话来回避这个问题

xv6 允许只读的系统调用在一次会话中并发执行。i 节点锁会使得会话对只读系统调用看上去是原子性的。

xv6 使用固定量的磁盘空间保存日志。

对于大多数系统调用来说这都不是个问题，

但`write` 和 `unlink：`

* 写一个大文件可能会写很多的数据块、位图块，以及 i 节点块
* 移除对一个大文件的链接可能会写很多的位图块以及一个 i 节点块
* xv6 的写系统调用将大的写操作拆分成几个小的写操作，使得被修改的块能放入日志中。`unlink` 不会导致问题因为实际上 xv6 只使用一个位图块

### 代码：块分配器

位图区，balloc，bfree，定位的计算过程有点绕

### i 节点

*i 节点*这个术语可以有两个意思

* 磁盘上：记录文件大小、数据块扇区号的数据结构。
* 内存中：包含一个磁盘上 i 节点的拷贝，以及一些内核需要的附加信息。

维护：inode结构（iupdate，lock，read，write，trunc，idup）；icache（init，iget，iput，ialloc）

### 代码：目录层

就是一个记录（名字和inode）结构的inode

### 代码：路径名

path解析：namei        路径名查询： dirlookup

### 文件描述符层

维护：file（filedup，fileclose，read，write），ftable（系统打开文件表，fileinit，filealloc，filealloc），ofile（fdalloc）

UNIX 接口大多数的资源都可以用文件来表示，包括设备、管道，真正的文件

文件描述符是一个进程的活跃文件（ofile数组）

系统中的打开文件都存在于一个全局的文件表 `ftable` 中

函数`filestat`，`fileread`，`filewrite` 实现了对文件的 `stat`，`read`，`write` 操作。

`filestat` 只允许作用在 i 节点上，copy通过调用 `stati` 实现

### 代码：系统调用

有了底层的这些函数，大多数的系统调用的实现都是很简单的

`sys_link` 和`sys_unlink` 修改目录，可能会创建或者移除对 i 节点的引用。

`create`开一个新inode，连到path里，是open，mkdir，mknod的工具函数



`sys_open` 是最复杂的

* 如果 `open` 以 `O_CREATE` 调用，它会调用 `create`（5712）
* 否则会调用 `namei`（5717）
* `create` 会返回一个带锁的 i 节点，但是 `namei` 并不会，`sys_open` 必须要自己锁上这个 i 节点，（用来提供只读模式）
* 获得一个 i 节点后`sys_open` 分配了一个文件和文件描述符（5726），接着填充这个文件（5734-5738）
* 没有其他进程能够访问初始化尚未完成的文件，它只存在于当前进程的文件表中

函数 `sys_pipe` 通过管道对的方式把管道的实现和文件系统连接来

* 参数是一个指向可装入两个整数的数组指针，用于记录两个新的文件描述符。
* 分配管道，将两个文件描述符存入这个数组中。

#### 现实情况

现实情况中操作系统的块缓冲比xv6中要复杂的多

但依旧为两个主要目的服务：缓冲和同步到磁盘。

xv6的块缓冲，与V6的类似，使用简单的“近期最少使用算法”（Least Recently Used, or LRU）的回收策略

* 现实中还有很多其他相对复杂的策略，各有自己的优劣
* 一种更有效的LRU缓冲策略可以减少链表的使用，用哈希表来实现查找、用堆来实现LRU的回收
* 现代块缓冲一般会结合虚拟内存系统从而可以支持映射到内存的文件。

xv6的日志系统相当不高效

* 不支持并发、可更新的系统调用，即使当系统调用命令在文件系统完全不同的部分间执行
* 要记录整个块，即使一个块只有很少一部分字节改变。
* 可以实现日志记录的同步，但每次只有一个块，每次都可能需要整个磁盘的运转时间
* 现实的日志系统可以处理这些问题

日志记录不是唯一的崩溃后的恢复机制

* 早期文件系统在重新启动的时候使用清扫机制（比如，UNIX系统中的 `fsck` 命令）检查每个文件，目录，各个块， i 节点可用的链表，查找并解决出现的不一致问题
* 大型文件系统中清扫机制可能耗费数小时，并且存在一些情况，无法猜测出正确解决方案
* 相对来说，通过日志系统来恢复更快捷准确

xv6使用基于磁盘分层结构的 i 节点和目录，和早期的UNIX系统相同

* 这种机制近年仍然存在，BSD的UFS/FFS和Linux的ext2/ext3 都使用了类似的数据结构
* 文件系统结构最不高效的部分是目录，每次查找都需要在磁盘块上线性查找，对于“文件存在于多个磁盘块”的目录来说效率很低
* （目录是个树结构）
* 微软Windows的NTFS，Mac OS X的HFS，以及Solaris的ZFS，仅命名了一些目录，把目录实现为磁盘上块的平衡树结构，保证了log级别时间复杂度的查找

xv6处理磁盘任务失败的方式：too simple, sometimes naive

* 如果磁盘操作失败，xv6报警
* 这种方式是否有效要取决于硬件：如果 操作系统在某些特殊的硬件上，该硬件使用冗余来掩饰磁盘错误（？？？），或许该操作系统不会经常出错，报警机制有效
* 如果是普通的硬件，可能会遇到更多操作失败，需要经常处理才能在一个文件中的块丢失情况下不会影响到文件系统其他的使用

xv6需要文件系统固定在一个磁盘设备上，各个块大小不改变

* 大型数据库和多媒体文件需要更大的储存容量，操作系统需要攻破“一个磁盘一个文件系统”的瓶颈
* （哪里限制了一个文件系统必须放在一个磁盘上？）
* 基础的方法是把许多磁盘联合到一个逻辑磁盘上，硬件解决方案如RAID依旧是最流行的
* 但目前的趋势是在软件上尽可能地实现这种逻辑
* 这种软件的实现可以允许丰富的功能，如通过快速添加和移除命令来实现逻辑设备的增长和缩减（？）
* 当然，存储层的快速增加和减少需要类似机制的文件系统，UNIX文件系统使用的 i 节点块结构是固定空间大小的数组结构，它们不能很好的实现上述功能
* 把文件系统和磁盘管理系统分开也许是最干净的设计，但是两个系统的复杂接口产生了新的系统，如Sun的ZFS，可以把它们联系起来。

xv6文件系统缺少很多现今其他文件系统的特征

* 比如缺乏对“备份的快照和增加”机制的支持（？）

xv6有两种不同的文件实现方式：管道和 i 节点

* 现代的UNIX系统：管道，网络连接，许多来自不同类型文件系统的 i 节点，包括网络文件系统
* 与 `fileread` 和 `filewrite` 中 `if` 声明不同，这些系统一般给每个打开的文件一个由函数指针构成的表，每次操作之后，调用函数指针来实现相关 i 节点的调用
* 网络文件系统和用户层面的文件系统提供了这些函数，调用到网络的RPC并在返回前等待答复

#### 练习

1.为什么在 `balloc` 中会 `panic` ? 我们能恢复吗？

2.为什么在 `ialloc` 中会 `panic` ? 我们能恢复吗？

3.i 节点产生编号

4.为什么当 `filealloc` 用完文件的时候不会 `panic`？为什么这是很普通但是值得处理的？

5.假设 `ip` 对应的文件在 `sys_link` 调用到 `iunlock(ip)` 和 `dirlink` 时被另一个进程去除对应的联系，这个联系还会被正确的创建吗？并说明原因。

6.`create` 有四个函数调用（其中一个是调用到 `ialloc`, 其他三个调用到 `dirlink` ）需要成功创建。如果不能，`create` c产生 `panic` 。为什么这是可取的？为什么这四个调用中的任意一个都不会失败？

7.`sys_chdir` 在调用 `iput(cp->cwd)` 之前调用 `iunlock(ip)`，这个过程可能会锁住 `cp->cwd`，然而推迟 `iunlock(ip)` 调用直到`iput`调用结束，将不会造成死锁，为什么不会？



## 附录 A

### PC 硬件

x86 运行的PC硬件平台。

X86是由Intel推出的一种复杂指令集，用于控制芯片的运行的程序，现在X86已经广泛运用到了家用PC（机箱+xx主板+xx电源+xx处理器+（光驱选装）的领域。

1978年6月8日，Intel发布了新款16位微处理器“8086”，也同时开创了一个新时代：x86架构诞生了
x86指的是特定微处理器执行的一些计算机语言指令集，定义了芯片的基本使用规则，一如今天的x64、IA64等。

PC 是指遵守一定工业标准的计算机

* 它的目标是使得不同厂家生产的机器都能够运行一定范围内的软件。这些标准随时时间迁移不断变化，因此90年代的 PC 与今日的 PC 看起来已是大不相同。

从外观来看，PC 是一个配置有键盘、屏幕和各种设备的“盒子”。盒子内部则是一块集成电路——*主板*，上面有 CPU 芯片，内存芯片，显卡芯片，I/O 控制器芯片，以及负责芯片间通信的总线。总线会遵守某种标准（如 PCI 或 USB），从而能够兼容不同厂家的设备。

PC 抽象为三部分：CPU、内存和 I/O 设备。CPU 负责计算，内存用于保存计算用的指令和数据，其他设备用于实现存储、通讯等其他功能。

主存以一组导线与 CPU 相连接，地址线，数据线，控制线。

CPU 要从主存读出一个值，需要向地址线上输出一系列表示0和1的电压，并在规定的时间内在 “读” 线上发出信号1，接下来再从数据线上的高低电压中获取数据。

CPU 若要向内存中写入一个值，则向数据线和地址线上写入合适的值，并在规定时间内在 "写" 位上发出信号1

真实的内存接口比这复杂的多，但除非你是在追求高性能，否则不必考虑这么多的细节。

### 处理器和内存

CPU（中央处理单元，或处理器）只是在执行一个非常简单的循环

从PC寄存器中获取一个内存地址，读出机器指令，增加PC的值，执行机器指令，不断反复。某些机器指令如分支和函数调用会改变程序计数器

存储器：速度最快的数据存储器是处理器的寄存器组

* 一个寄存器是处理器内的一个存储单元，能够保存一个字大小的值（按照机器不同，一个字通常会是16，32或者64位）
* 寄存器内的值能在一个 CPU 周期内被快速地读写。

PC 处理器实现了 x86 指令集，该指令集由 Intel 发布并成为了一种标准，一些产商生产实现了该指令集的处理器。

和其他的 PC 标准一样，这个标准也在不断更新，但是新的标准是向前兼容的。

PC 处理器启动时都是模拟1981年 IBM PC 上使用的芯片 Intel 8088，boot loader 需要作出改变以应对标准的更新。但是对于 xv6 的绝大部分内容，你只需要关心现代 x86 指令集。

现代 x86 提供了8个32位通用寄存器--`%eax, %ebx, %ecx, %edx, %edi, %esi, %ebp, %esp` 和一个程序计数器 `%eip`（*instruction pointer*）

* 前缀*e*是指*扩展的*（*extended*），表示它们是16位寄存器`%ax, %bx, %cx, %dx, %di, %si, %bp, %sp` 的32位扩展
* 这两套寄存器其实是相互的别名，例如 `%ax` 是 `%eax` 的低位：我们在写 `%ax` 的时候也会改变 `%eax`，反之亦然
* 前四个寄存器的两个低8位还有自己的名字：`%al, %ah` 分别表示 `%ax`的低8位和高8位，`%bl, %bh, %cl, %ch, %dl, %dh`同理
* x86 还有8个80位的浮点寄存器，以及一系列特殊用途的寄存器如*控制寄存器* `%cr0, %cr2, %cr3, %cr4`，*调试寄存器* `%dr0, %dr1, %dr2, %dr3`；段寄存器 `%cs, %ds, %es, %fs, %gs, %ss`；还有全局和局部描述符表的伪寄存器`%gdtr, %ldtr`
* 控制寄存器和段寄存器对于任何操作系统都是非常重要的。浮点寄存器和调试寄存器则没那么有意思，并且也没有在 xv6 中使用。

下一个层次的存储器是随机存储器（RAM）

* 主存的速度大概比寄存器慢10到100倍，但要便宜得多，容量可以更大，今天的 PC 通常有 GB 级的主存
* 主存较慢的一个原因是它不在处理器芯片上
* 缓存是主存和寄存器在速度和大小上的折衷，大多数处理器，包括 x86，都在芯片上的缓存中保存了最近使用的主存数据
* 现在的 x86 处理器通常有二级缓存，第一级较小，读写速率接近处理器的时钟周期，第二级较大，读写速率在第一级缓存和主存之间。下表显示了 Intel Core 2 Duo 系统的实际数据：

Intel Core 2 Duo E7200 at 2.53 GHz 

| 存储器 | 读写时间 | 大小     |
| ------ | -------- | -------- |
| 寄存器 | 0.6ns    | 64 字节  |
| L1缓存 | 0.5ns    | 64K 字节 |
| L2缓存 | 10ns     | 4M 字节  |
| 主存   | 100ns    | 4G 字节  |

通常 x86 对操作系统隐藏了缓存，所以我们只需要考虑寄存器和主存两种存储器，不用担心主存的层次结构引发的差异。

### I/O

处理器必须像和主存交互一样同设备交互

x86 处理提供了特殊的 `in` `out` 指令在设备地址（称为'I/O 端口'） 上读写，两个指令的硬件实现本质上和读写内存相同

* 早期的 x86 处理器有一条附加的地址线：0表示从 I/O 端口读写，1则表示从主存读写。

每个硬件设备会处理它所在 I/O 端口所接收到的读写操作。设备的端口使得软件可以配置设备，检查状态，使用设备；例如，软件可以通过对 I/O 端口的读写，使磁盘接口硬件对磁盘扇区进行读写。

很多计算机体系结构都没有单独的设备访问指令，取而代之的是让设备拥有固定的内存地址，然后通过内存读写实现设备读写。

* 现代 x86 体系结构就在大多数高速设备上（如网络、磁盘、显卡控制器）使用了该技术，叫做 *内存映射 I/O*
* 由于向前兼容，`in` `out` 指令仍能使用，比较老的设备如 xv6 中使用的 IDE 磁盘控制器仍使用两个指令



## 附录 B

### 引导加载器（boot loader）

当 x86 PC 启动时，它执行的是一个叫 BIOS 的程序。

* BIOS 存放在非易失存储器中，BIOS 的作用是在启动时进行硬件的准备工作，接着把控制权交给操作系统。

BIOS 会把引导扇区加载到内存 0x7c00 处，接着（通过设置寄存器 `%ip`）跳转至该地址。

* 把控制权交给从引导扇区（用于引导的磁盘的第一个512字节的数据区）加载的代码。引导扇区中包含引导加载器——负责内核加载到内存中。

xv6 引导加载器包括两个源文件，一个由16位和32位汇编混合编写而成（`bootasm.S`；（8400）），另一个由 C 写成（`bootmain.c`；（8500））。

* 引导加载器开始执行后，处理器处于模拟 Intel 8088 处理器的模式下。接下来要把处理器设置为现代的操作模式，并从磁盘中把 xv6 内核载入到内存中，然后将控制权交给内核。

bootloader启动相关的一些代码和流程。简单点说，就是把地址的寻址模式设置好，然后把内核加载进来。

### 代码：汇编引导程序

第一条指令 `cli`（8412）屏蔽硬件中断（这时候就算中断也不一定有出来程序，可能idt表都还没有init）

配置分段硬件，使之不对逻辑地址做任何改变，直接得到线性地址：

* 将 `%ax` 置零，然后把这个零值拷贝到三个段寄存器中（8415-8418）。（因为现在是实模式，段寄存器的值需要为0）
* 背景：现在处理器处在模拟 Intel 8088 的*实模式*下，有8个16位通用寄存器可用，但实际上处理器发送给内存的是20位的地址。这时，多出来的4位其实是由段寄存器`%cs, %ds, %es, %ss`提供的。当程序用到一个内存地址时，处理器（硬件实现）会自动在该地址上加上某个16位段寄存器值的16倍。因此，内存引用中其实隐含地使用了段寄存器的值：取指会用到 `%cs`，读写数据会用到 `%ds`，读写栈会用到 `%ss`。

sete20.1 20.2

* 引导加载器用 I/O 指令控制端口 0x64 和 0x60 上的键盘控制器，使其输出端口的第2位为高位，来使第21位地址正常工作
* 背景：虚拟地址 *segment:offset* 可能产生21位物理地址（有这个可能），但 Intel 8088 只能向内存传递20位地址，所以它截断了地址的最高位：0xffff0 + 0xffff = 0x10ffef，但在8088上虚拟地址 0xffff:0xffff 则是引用物理地址 0x0ffef。早期的软件依赖硬件来忽略第21位地址位，所以当 Intel 研发出使用超过20位物理地址的处理器时，IBM 就想出了一个技巧来保证兼容性。那就是，如果键盘控制器输出端口（？？？为什么？）的第2位是低位，则物理地址的第21位被清零；否则，第21位可以正常使用。

`lgdt`指令：把指向 `gdt` 的指针 `gdtdesc`加载到全局描述符表（GDT）寄存器中

将 `%cr0` 中的 `CR0_PE` 位置为1，开启保护模式

* 在保护模式下，段寄存器保存着*段描述符表*的索引。每一个表项都指定了一个基物理地址，最大虚拟地址（称为限制），以及该段的权限位。这些权限位在保护模式下起着保护作用，内核可以根据它们来保证一个程序只使用属于自己的内存
* xv6 几乎没有使用段；取而代之的是第2章讲述的分页。引导加载器将段描述符表 `gdt`（8482-8485）中的每个段的基址都置零，并让所有段都有相同的内存限制（4G字节）。该表中有一个空指针表项，一个可执行代码的表项，一个数据的表项。代码段描述符的标志位中指示了代码只能在32位模式下执行（0660）

允许保护模式并不会马上改变处理器把逻辑地址翻译成物理地址的过程；只有当某个段寄存器加载了一个新的值，然后处理器通过这个值读取 GDT 的一项从而改变了内部的段设置。

没法直接修改 `%cs`，使用一个 `ljmp` 指令，放上code段地址和跳转后的偏移（就是start32修改段寄存器的几行指令）

movl    $start, %esp

* 内存 0xa0000 到 0x100000 属于设备区，而 xv6 内核则是放在 0x100000 处。引导加载器自己是在 0x7c00 到 0x7d00。本质上来讲，内存的其他任何部分都能用来存放栈。引导加载器选择了 0x7c00（在该文件中即 `$start`）作为栈顶；栈从此处向下增长，直到 0x0000，不断远离引导加载器代码。

call    bootmain  

* `bootmain` 的工作就是加载并运行内核。只有在出错时该函数才会返回，这时它会向端口 0x8a00（8470-8476）输出几个字。在真实硬件中，并没有设备连接到该端口，所以这段代码相当于什么也没有做。如果引导加载器是在 PC 模拟器上运行，那么端口 0x8a00 则会连接到模拟器并把控制权交还给模拟器本身。无论是否使用模拟器，这段代码接下来都会执行一个死循环（8477-8478）。而一个真正的引导加载器则应该会尝试输出一些调试信息。

### 代码：C 引导程序

引导加载器的 C 语言部分 `bootmain.c`（8500）目的是在磁盘的第二个扇区开头找到内核程序。

从1扇区载入 ELF 文件的前4096字节，并将其拷贝到内存中 0x10000 处。

通过 ELF 头检查这是否的确是一个 ELF 文件。

加载程序数据，此时没有页表，虚拟地址是直接映射的

call entry，内核第一条指令的执行地址。在 xv6 中是 0x10000c

* 按照惯例，在 `entry.S`（1036）中定义的 `_start` 符号即 ELF 入口。由于 xv6 还没有建立虚拟内存，xv6 的入口即 `entry`（1040）的物理地址

### 现实情况

该附录中谈到的引导加载器编译后大概有470字节的机器码，具体大小取决于编译优化。为了放入比较小的空间中，xv6 引导加载器做了一个简单的假设：内核放在引导磁盘中从扇区1开始的连续空间中。通常内核就放在普通的文件系统中，而且可能不是连续的。也有可能内核是通过网络加载的。这种复杂性就使得引导加载器必须要能够驱动各种磁盘和网络控制器，并能够解读不同的文件系统和网络原型。也就是说，引导加载器本身就已经成为了一个小操作系统。显然这样的引导加载器不可能只有512字节，大多数的 PC 操作系统的引导过程分为2步。首先，一个类似于该附录介绍的简单的引导加载器会从一个已知的磁盘位置上把完整的引导加载器加载进来，通常这步会依靠空间权限更大的 BIOS 来操作磁盘。接下来，这个超过512字节的完整加载器就有足够的能力定位、加载并执行内核了。也许在更现代的设计中，会直接用 BIOS 从磁盘中读取完整的引导加载器（并在保护模式和32位模式下启动之）。

本文假设在开机后，引导加载器运行前，唯一发生的事即 BIOS 加载引导扇区。但实际上 BIOS 会做相当多的初始化工作来确保现代计算机中结构复杂的硬件能像传统标准中的 PC 一样工作。

### 练习

1. 基于扇区大小，文中提到的调用 `readseg` 的作用和 `readseg((uchar*)0x100000, 0xb500, 0x1000)` 的作用是相同的。实际上，这个草率的实现并不会导致错误。这是为什么呢？
2. 一些关于 BIOS 存在时长与安全性的问题。
3. 假设你希望 `bootmain()` 能把内核加载到 0x200000 而非 0x100000，于是你在 `bootmain()` 中把每个 ELF 段的 `va` 都加上了 0x100000。这样做是会导致错误发生的，请说明会发生什么错误。
4. 引导加载器把 ELF 头拷贝到了一个随意的地址 0x10000 上，这样做看起来似乎有些危险。那么为什么不调用 `malloc` 来分配它所需要的空间呢？