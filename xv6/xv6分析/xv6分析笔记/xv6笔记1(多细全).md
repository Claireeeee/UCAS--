- 便于以后复习查阅
- 并不是完整的概念解释，仅保留了我自己的知识盲点
- 很多都是直接复制来的，看完有删减或修改
- 没有注明引用出处

## xv6了解

课程的老师们决定以V6为基础，使用ANSI C写一个新的在Intel x86多处理器计算机上的系统，也就是Xv6，来代替V6。Xv6使用x86 CPU，更适合学生的经验，而且将6.828课程统一到单一的体系结构上。增加多处理器支持需要使用锁和线程来处理并发（单处理器只要简单的开/关中断即可）。重写一个新系统的过程中，也使V6中比较粗糙的一些部分，如调度和文件系统得到了改进。

Xv6是在x86处理器上(x即指x86)用ANSI标准C重新实现的Unix第六版(Unix V6，通常直接被称为V6 [3]  )

Xv6可以在真实的硬件上启动，但通常使用虚拟机来运行它，如Bochs，QEMU和Vmware player，VirtualBox。

## 内核

系统调用：由[操作系统](https://baike.baidu.com/item/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/192)实现提供的所有系统调用所构成的集合即[程序接口](https://baike.baidu.com/item/%E7%A8%8B%E5%BA%8F%E6%8E%A5%E5%8F%A3/150383)或应用编程接口(Application Programming Interface，API)。是[应用程序](https://baike.baidu.com/item/%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F)同系统之间的接口。

内核是操作系统最基本的部分。它是为众多应用[程序](https://baike.baidu.com/item/%E7%A8%8B%E5%BA%8F)提供对计算机[硬件](https://baike.baidu.com/item/%E7%A1%AC%E4%BB%B6)的安全访问的一部分[软件](https://baike.baidu.com/item/%E8%BD%AF%E4%BB%B6/12053)，这种访问是有限的，并且内核决定一个程序在什么时候对某部分硬件操作多长时间。

内核对外表现为		系统调用接口

xv6 内核提供了 Unix 传统系统调用的一部分

| 系统调用                  | 描述                               |
| ------------------------- | ---------------------------------- |
| fork()                    | 创建进程                           |
| exit()                    | 结束当前进程                       |
| wait()                    | 等待子进程结束                     |
| kill(pid)                 | 结束 pid 所指进程                  |
| getpid()                  | 获得当前进程 pid                   |
| sleep(n)                  | 睡眠 n 秒                          |
| exec(filename, *argv)     | 加载并执行一个文件                 |
| sbrk(n)                   | 为进程内存空间增加 n 字节          |
| open(filename, flags)     | 打开文件，flags 指定读/写模式      |
| read(fd, buf, n)          | 从文件中读 n 个字节到 buf          |
| write(fd, buf, n)         | 从 buf 中写 n 个字节到文件         |
| close(fd)                 | 关闭打开的 fd                      |
| dup(fd)                   | 复制 fd                            |
| pipe( p)                  | 创建管道， 并把读和写的 fd 返回到p |
| chdir(dirname)            | 改变当前目录                       |
| mkdir(dirname)            | 创建新的目录                       |
| mknod(name, major, minor) | 创建设备文件                       |
| fstat(fd)                 | 返回文件信息                       |
| link(f1, f2)              | 给 f1 创建一个新名字(f2)           |
| unlink(filename)          | 删除文件                           |



## shell

Shell俗称壳（用来区别于核），是指“提供使用者使用界面”的软件（命令解析器）。它类似于[DOS](https://baike.baidu.com/item/DOS)下的command.com和后来的cmd.exe。它接收用户命令，然后调用相应的应用程序。

Shell 是一个用 C 语言编写的程序，它是用户使用 Linux 的桥梁。Shell 既是一种命令语言，又是一种程序设计语言。

Shell 是指一种应用程序，这个应用程序提供了一个界面，用户通过这个界面访问操作系统内核的服务。

Shell Script [1]  ，Shell脚本与Windows/Dos下的[批处理](https://baike.baidu.com/item/%E6%89%B9%E5%A4%84%E7%90%86/1448600)相似，也就是用各类命令预先放入到一个文件中，方便一次性执行的一个[程序文件](https://baike.baidu.com/item/%E7%A8%8B%E5%BA%8F%E6%96%87%E4%BB%B6/10510952)，主要是方便管理员进行设置或者管理用的。但是它比Windows下的批处理更强大，比用其他编程[程序编辑](https://baike.baidu.com/item/%E7%A8%8B%E5%BA%8F%E7%BC%96%E8%BE%91)的程序效率更高，它使用了Linux/Unix下的命令。

换一种说法也就是，shell script是利用shell的功能所写的一个程序，这个程序是使用[纯文本文件](https://baike.baidu.com/item/%E7%BA%AF%E6%96%87%E6%9C%AC%E6%96%87%E4%BB%B6)，将一些shell的语法与指令写在里面，然后用正规表示法，管道命令以及数据流重导向等功能，以达到我们所想要的处理目的。

更明白地来说，shell script就像早期dos年代的[.bat](https://baike.baidu.com/item/.bat)，最简单的功能就是将许多指令汇整写一起，让使用者很容易地就能够一个操作执行多个命令，而shell script更是提供了[数组](https://baike.baidu.com/item/%E6%95%B0%E7%BB%84)，循环，条件以及[逻辑判断](https://baike.baidu.com/item/%E9%80%BB%E8%BE%91%E5%88%A4%E6%96%AD)等重要功能，让使用者可以直接以shell来写程序，而不必使用类似C程序语言等传统程序编写的语法。

Shell 脚本（shell script），是一种为 shell 编写的脚本程序。

业界所说的 shell 通常都是指 shell 脚本，但读者朋友要知道，shell 和 shell script 是两个不同的概念。

shell和shell脚本有什么区别？确切一点说，Shell就是一个[命令行解释器](https://baike.baidu.com/item/%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%A7%A3%E9%87%8A%E5%99%A8)，它的作用就是遵循一定的语法将输入的命令加以解释并传给系统。它为用户提供了一个向Linux发送请求以便运行程序的接口系统级程序，用户可以用Shell来启动、挂起、停止甚至是编写一些程序。 Shell本身是一个用C语言编写的程序，它是用户使用Linux的桥梁。Shell既是一种命令语言，又是一种[程序设计语言](https://baike.baidu.com/item/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E8%AF%AD%E8%A8%80)(就是你所说的shell脚本)。作为命令语言，它互动式地解释和执行用户输入的命令；作为程序设计语言，它定义了各种变量和参数，并提供了许多在高阶语言中才具有的控制结构，包括循环和分支。它虽然不是 Linux系统[内核](https://baike.baidu.com/item/%E5%86%85%E6%A0%B8)的一部分，但它调用了系统内核的大部分功能来执行程序、创建文档并以并行的方式协调各个程序的运行。

shell 的主要结构很简单

* 主循环通过 `getcmd` 读取命令行的输入
* 调用 `fork` 生成一个 shell 进程的副本
* 父 shell 调用 `wait`，子进程执行用户命令

## 文件描述符

文件描述符是非负整数。打开现存文件或新建文件时，内核会返回一个文件描述符。读写文件也需要使用文件描述符来指定待读写的文件。

文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向[内核](https://baike.baidu.com/item/%E5%86%85%E6%A0%B8)为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于[UNIX](https://baike.baidu.com/item/UNIX)、[Linux](https://baike.baidu.com/item/Linux)这样的操作系统。 

习惯上，标准输入（standard input）的文件描述符是 0，标准输出（standard output）是 1，标准错误（standard error）是 2。尽管这种习惯并非[Unix](https://baike.baidu.com/item/Unix)内核的特性，但是因为一些 shell 和很多应用程序都使用这种习惯，因此，如果内核不遵循这种习惯的话，很多应用程序将不能使用。

## 分页机制

分页机制在段机制之后进行，以完成线性—物理地址的转换过程。段机制把逻辑地址转换为线性地址，分页机制进一步把该线性地址再转换为物理地址。

分页机制由CR0中的PG位启用。如PG=1，启用分页机制，并使用本节要描述的机制，把线性地址转换为物理地址。如PG=0，禁用分页机制，直接把段机制产生的线性地址当作物理地址使用。分页机制管理的对象是固定大小的存储块，称之为页(page)。分页机制把整个线性地址空间及整个物理地址空间都看成由页组成，在线性地址空间中的任何一页，可以映射为物理地址空间中的任何一页（我们把物理空间中的一页叫做一个页面或页框(page frame)）

80386使用4K字节大小的页。每一页都有4K字节长，并在4K字节的边界上对齐，即每一页的起始地址都能被4K整除。因此，80386把4G字节的线性地址空间，划分为1M个页面，每页有4K字节大小。分页机制通过把线性地址空间中的页，重新定位到物理地址空间来进行管理，因为每个页面的整个4K字节作为一个单位进行映射，并且每个页面都对齐4K字节的边界，因此，线性地址的低12位经过分页机制直接地作为物理地址的低12位使用。

页内地址连续，虚拟和物理间的映射要保证映射两端的内存大小相等

32位的线性地址被分为3个域：Directory(目录)，高10位；Table（页表），中间10位；Offset（偏移量），最低12位。

现在的Windows和Linux都是每个进程一个页表，当进程切换时，通过改变CR3来完成页表切换。

线性地址到物理：

现在的Windows和Linux都是每个进程一个页表，当进程切换时，通过改变CR3来完成页表切换。

* CR3包含着页目录（二级页表中的第一级，随着页表变化而变化）的起始地址，用32位线性地址的最高10位A31~A22作为页目录的页目录项的索引，将它乘以4，与CR3中的页目录的起始地址相加，形成相应页表的地址。
* 从指定的地址中取出32位页目录项，它的低12位为0，这32位是页表的起始地址。用32位线性地址中的A21~A12位作为页表中的页面的索引，将它乘以4，与页表的起始地址相加，形成32位页面地址。
* 将A11~A0作为相对于页面地址的偏移量，与32位页面地址相加，形成32位物理地址。

### 为什么使用两级页表

假设每个进程都占用了4G的线性地址空间，页表共含1M个表项，每个表项占4个字节（每个表项存储的是首地址吧，因为页边对齐，后面12位是0，可以记录其他信息），那么每个进程的页表要占据4M的内存空间。为了节省页表占用的空间，我们使用两级页表。每个进程都会被分配一个页目录，但是只有被实际使用页表才会被分配到内存里面。一级页表需要一次分配所有页表空间（存首地址的空间），两级页表则可以在需要的时候再分配页表空间（分配一个页，存首地址）。

### 页面高速缓存

由于在分页情况下，每次存储器访问都要存取两级页表，这就大大降低了访问速度。所以，为了提高速度，在386中设置一个最近存取页面的高速缓存硬件机制，它自动保持32项处理器最近使用的页面地址，因此，可以覆盖128K字节的存储器地址。当进行存储器访问时，先检查要访问的页面是否在高速缓存中，如果在，就不必经过两级访问了，如果不在，再进行两级访问。平均来说，页面高速缓存大约有98%的命中率，也就是说每次访问存储器时，只有2%的情况必须访问两级分页机构。这就大大加快了速度。

依赖硬件

因为Linux的分页机制是建立在硬件基础之上的，不同的平台需要有不同的实现。Linux在软件层面构造的虚拟地址，最终还是要通过MMU转换为物理地址，也就是说，不管Linux的分页机制是怎样实现的，CPU只按照它的分页实现来解读线性地址，所以Linux传给CPU的线性地址必然是满足硬件实现的。例如说：Linux在32位CPU上，它的四级页表结构就会兼容到硬件的两级页表结构。可见，Linux在软件层面上做了一层抽象，用四级页表的方式兼容32位和64位CPU内存寻址的不同硬件实现。

## MUU

如果处理器启用了MMU，CPU执行单元发出的内存地址将被MMU截获，从CPU到MMU的地址称为虚拟地址（Virtual Address，以下简称VA），而MMU将这个地址翻译成另一个地址发到CPU芯片的外部地址引脚上，也就是将VA映射成PA，如下图所示。

如果是32位处理器，则内地址总线是32位的，与CPU执行单元相连（图中只是示意性地画了4条地址线），而经过MMU转换之后的外地址总线则不一定是32位的。也就是说，虚拟地址空间和物理地址空间是独立的，32位处理器的虚拟地址空间是4GB，而物理地址空间既可以大于也可以小于4GB。

MMU将VA映射到PA是以页（Page）为单位的，32位处理器的页尺寸通常是4KB。

例如，MMU可以通过一个映射项将VA的一页0xb7001000--0xb7001fff映射到PA的一页0x2000~0x2fff，如果CPU执行单元要访问虚拟地址0xb7001008，则实际访问到的物理地址是0x2008。

物理内存中的页称为物理页面或者页帧（Page Frame）。虚拟内存的哪个页面映射到物理内存的哪个页帧是通过页表（Page Table）来描述的，页表保存在物理内存中，MMU会查找页表来确定一个VA应该映射到什么PA。

操作系统和MMU是这样配合的：

1. 操作系统在初始化或分配、释放内存时会执行一些指令在填写页表，然后用指令设置MMU，告诉MMU页表在物理内存中的什么位置。
2. 设置好之后，CPU每次执行访问内存的指令都会自动引发MMU做查表和地址转换操作，地址转换操作由硬件自动完成，不需要用指令控制MMU去做。

那为什么要设计这么复杂的内存管理机制呢？多了一层VA到PA的转换到底换来了什么好处？All problems in computer science can be solved by another level of indirection.还记得这句话吗？多了一层间接必然是为了解决什么问题的，等讲完了必要的预备知识之后，将在第 5 节 “虚拟内存管理”讨论虚拟内存管理机制的作用

MMU除了做地址转换之外，还提供内存保护机制。各种体系结构都有用户模式（User Mode）和特权模式（Privileged Mode）之分，操作系统可以在页表中设置每个内存页面的访问权限，有些页面不允许访问，有些页面只有在CPU处于特权模式时才允许访问，有些页面在用户模式和特权模式都可以访问，访问权限又分为可读、可写和可执行三种。这样设定好之后，当CPU要访问一个VA时，MMU会检查CPU当前处于用户模式还是特权模式，访问内存的目的是读数据、写数据还是取指令，如果和操作系统设定的页面权限相符，就允许访问，把它转换成PA，否则不允许访问，产生一个异常（Exception）。异常的处理过程和中断类似，不同的是中断由外部设备产生而异常由CPU内部产生，中断产生的原因和CPU当前执行的指令无关，而异常的产生就是由于CPU当前执行的指令出了问题，例如访问内存的指令被MMU检查出权限错误，除法指令的除数为0等都会产生异常。

通常操作系统把虚拟地址空间划分为用户空间和内核空间，例如x86平台的Linux系统虚拟地址空间是0x00000000--0xffffffff，前3GB（0x00000000--0xbfffffff）是用户空间，后1GB（0xc0000000~0xffffffff）是内核空间。用户程序加载到用户空间，在用户模式下执行，不能访问内核中的数据，也不能跳转到内核代码中执行。这样可以保护内核，如果一个进程访问了非法地址，顶多这一个进程崩溃，而不会影响到内核和整个系统的稳定性。CPU在产生中断或异常时不仅会跳转到中断或异常服务程序，还会自动切换模式，从用户模式切换到特权模式，因此从中断或异常服务程序可以跳转到内核代码中执行。事实上，整个内核就是由各种中断和异常处理程序组成的。总结一下：在正常情况下处理器在用户模式执行用户程序，在中断或异常情况下处理器切换到特权模式执行内核程序，处理完中断或异常之后再返回用户模式继续执行用户程序。

段错误我们已经遇到过很多次了，它是这样产生的：

1. **用户程序要访问的一个VA，经MMU检查无权访问。**
2. **MMU产生一个异常，CPU从用户模式切换到特权模式，跳转到内核代码中执行异常服务程序。**
3. **内核把这个异常解释为段错误，把引发异常的进程终止掉。**

## ide

IDE是英文Integrated Drive Electronics的缩写，翻译成中文叫做“集成驱动器电子装置”， 它的本意是指把控制器与盘体集成在一起的硬盘驱动器。通常我们所说的IDE指的是硬盘等设备的一种接口技术。

把盘体与控制器集成在一起的做法减少了硬盘接口的电缆数目与长度，数据传输的可靠性得到了增强，硬盘制造起来变得更容易，因为厂商不需要再担心自己的硬盘是否与其它厂商生产的控制器兼容，对用户而言，硬盘安装起来也更为方便。

## 中断的一般过程

主程序只是在设备A，B，C数据准备就绪时，才去处理A，B ，C，进行数据交换。在速度较慢的外围设备准备自己的数据时，CPU照常执行自己的主程序。在这个意义上说，CPU和外围设备的一些操作是并行地进行的，因而同串行进行的程序查询方式相比，计算机系统的效率是大大提高了。

轮询(Polling)I／O方式或程序控制I／O方式，是让CPU以一定的周期按次序查询每一个外设，看它是否有数据输入或输出的要求，若有，则进行相应的输入／输出服务；若无，或I／O处理完毕柏，CPU就接着查询下一个外设。

## 原子操作

如果这个操作所处的层(layer)的更高层不能发现其内部实现与结构，那么这个操作是一个原子(atomic)操作。

原子操作可以是一个步骤，也可以是多个操作步骤，但是其顺序不可以被打乱，也不可以被切割而只执行其中的一部分。

将整个操作视作一个整体是原子性的核心特征。

"原子操作(atomic operation)是不需要synchronized"，这是多线程编程的老生常谈了。所谓原子操作是指不会被[线程调度](https://baike.baidu.com/item/%E7%BA%BF%E7%A8%8B%E8%B0%83%E5%BA%A6/10226112)机制打断的操作；这种操作一旦开始，就一直运行到结束，中间不会有任何 context switch （切 [1]  换到另一个线程）。

## 镜像

（Mirroring）是冗余的一种类型，一个磁盘上的数据在另一个磁盘上存在一个完全相同的副本即为镜像。

## 控制台

可以指：控制面板，Windows图形用户界面的一部分；命令行界面（CLI），也有人称之为字符用户界面（CUI）

 控制面板是Windows图形用户界面一部分，可通过开始菜单访问。它允许用户查看并操作基本的系统设置和控制，比如添加硬件、添加/删除软件、控制用户帐户、更改辅助功能选项等等。

## x86架构

于1978年推出的Intel 8086中央处理器中首度出现，它是从Intel 8008处理器中发展而来的，而8008则是发展自Intel 4004的。8086在三年后为IBM PC所选用，之后x86便成为了个人计算机的标准平台，成为了历来最成功的CPU架构。

从CPU发明到现在，有非常多种架构，从我们熟悉的X86，ARM，到不太熟悉的MIPS，IA64，它们之间的差距都非常大。但是如果从最基本的逻辑角度来分类的话，它们可以被分为两大类，即所谓的“复杂指令集”与“精简指令集”系统，也就是经常看到的“CISC”与“RISC”。 Intel和ARM处理器的第一个区别是，前者使用复杂指令集（CISC)，而后者使用精简指令集（RISC）。属于这两种类中的各种架构之间最大的区别，在于它们的设计者考虑问题方式的不同。

Intel和ARM的处理器除了最本质的复杂指令集（CISC)和精简指令集（RISC）的区别之外，下面我们再从以下几个方面对比下ARM和X86架构。

## 半双工

半双工(Half Duplex)		数据传输	包含一个双向线路	可以在信号载体的两个方向		不能同时传输

全双工	Full－Duplex 	发送数据的同时也能够接收数据	同步进行	迟延小，速度快

## 噗……

Multics 的设计的复杂性对 Unix 的设计者们产生了直接的影响，他们因此想把文件系统的设计做的更简单

## 实模式和保护模式

最早设定段机制是因为20位和16位，实模式下也是段机制

在Protected Mode下，对一个段的描述则包括3方面因素：[Base Address, Limit, Access]，它们加在一起被放在一个64-bit长的数据结构中，被称为[段描述符](https://baike.baidu.com/item/%E6%AE%B5%E6%8F%8F%E8%BF%B0%E7%AC%A6)。

解决的方法就是把这些长度为64-bit的段描述符放入一个数组中，而将段寄存器中的值作为下标索引来间接引用（事实上，是将段寄存器中的高13 -bit的内容作为索引）。这个全局的数组就是GDT。事实上，在GDT中存放的不仅仅是[段描述符](https://baike.baidu.com/item/%E6%AE%B5%E6%8F%8F%E8%BF%B0%E7%AC%A6)，还有其它描述符，它们都是64-bit长

不过Intel的实模式的概念实属不得已而为之，现在的意义已经不大了，从实模式启动然后跳转到保护模式纯粹是在绕圈子，没有实质的意义，商业上为了保护以前的投资不得不将技术做的没有意义的复杂...

另外，Protected Mode，顾名思义，就是为段访问提供了保护机制，也就说一个段的描述符需要规定对自身的访问权限（Access）。所以，在Protected Mode下，对一个段的描述则包括3方面因素：[Base Address, Limit, Access]，它们加在一起被放在一个64-bit长的数据结构中，被称为[段描述符](https://baike.baidu.com/item/%E6%AE%B5%E6%8F%8F%E8%BF%B0%E7%AC%A6)。

解决的方法就是把这些长度为64-bit的段描述符放入一个数组中，而将段寄存器中的值作为下标索引来间接引用（事实上，是将段寄存器中的高13 -bit的内容作为索引）。这个全局的数组就是GDT。

除了GDT之外，IA-32还允许程序员构建与GDT类似的数据结构，它们被称作LDT（Local Descriptor Table），但与GDT不同的是，LDT在系统中可以存在多个，并且从LDT的名字可以得知，LDT不是全局可见的，它们只对引用它们的任务可见，每个任务最多可以拥有一个LDT。另外，每一个LDT自身作为一个段存在，它们的[段描述符](https://baike.baidu.com/item/%E6%AE%B5%E6%8F%8F%E8%BF%B0%E7%AC%A6)被放在GDT中。

该表是为整个软硬件系统服务的。在进入保护模式之前，必须要定义全局描述符表

由于在实模式下只能访问1MB的内存，故GDT通常都定义在1MB以下的内存范围中当然，允许在进入保护模式之后换个位置重新定义GDT。

## 字节序

大于一个字节类型的数据在内存中的存放顺序

a) Little-Endian就是低位字节排放在内存的低地址端，高位字节排放在内存的高地址端。

b) Big-Endian就是高位字节排放在内存的低地址端，低位字节排放在内存的高地址端。

c) 网络字节序：TCP/IP各层协议将字节序定义为Big-Endian，因此TCP/IP协议中使用的字节序通常称之为网络字节序。

## 魔数：

很多类型的文件，其起始的几个字节的内容是固定的（或是有意填充，或是本就如此）。根据这几个字节的内容就可以确定文件类型，因此这几个字节的内容被称为魔数 (magic number)。此外在一些程序代码中，程序员常常将在代码中出现但没有解释的数字[常量](https://baike.baidu.com/item/%E5%B8%B8%E9%87%8F/10232375)或字符串称为魔数 (magic number)或魔字符串。

## call

常见的CPU的CALL指令（“调用”指令）的功能，就是以下两点： 

(1)将下一条指令的所在地址（即当时程序计数器PC的内容）入栈

(2)并将子程序的起始地址送入PC（于是CPU的下一条指令就会转去执行子程序）。而子程序结尾处通常都要编写一条RET指令（“返回”指令），RET指令的功能就是一条： 从栈中取出一条数据送入PC。  

## ret

子程序的返回在功能上是子程序调用的逆操作。

子程序的返回也分：远返回和近返回

在近类型的子程序中，返回指令RET是近返回，其功能是把栈顶之值弹出到指令指针寄存器IP中，SP会被加2；       

在远类型的子程序中，返回指令RET是远返回，其功能是：先弹出栈顶之值到IP中，再弹出栈顶之值到CS之中，SP总共会被加4。        

如果返回指令后面带有立即数(其值通常为偶数)，则表示在得到返回地址之后，SP还要增加的偏移量，它不是类似于高级语言中子程序的返回值。

## 汇编

汇编是直接面向硬件的，它可以访问系统的mem空间，也可以直接访问系统的io空间。

汇编中使用in/out来访问系统的io空间。

端口是主机与外设进行数据交换的。（外设接口电路有专用于数据交互的寄存器。为了与[CPU](https://www.baidu.com/s?wd=CPU&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao)中的寄存器相区别，称之为“端口”）。端口有数据端口，状态端口和控制端口3种。  PC机给予每一个端口分配了一个地址（称为端口号），形成一个独立于内存空间的I/O地址空间。在8086/8088中，端口地址的范围是0000至FFFF。  [CPU](https://www.baidu.com/s?wd=CPU&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao)对外设的操作通过专门的端口读写指令来完成。读端口用[IN](https://www.baidu.com/s?wd=IN&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao)指令，写端口用[OUT](https://www.baidu.com/s?wd=OUT&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao)指令。

IN AL,21H；表示从21H端口读取一字节数据到AL 

OUT 21H,AL；将AL的值写入21H端口 

    "__asm__"表示后面的代码为内嵌汇编， "asm"是"__asm__" 的别名。 "__volatile__"表示编译器不要优化代码，后面的指令保留原样， "volatile"是它的别名。括号里面是汇编指令。
    内嵌汇编的模板时：
    __asm__(汇编语句模板: 输出部分: 输入部分: 破坏描述部分)

## elf文件

![image-20180815160237502](/var/folders/xx/ztdttg0x1jl1lhkrxbzhk7rw0000gn/T/abnerworks.Typora/image-20180815160237502.png)

在计算机科学中，是一种用于[二进制文件](https://baike.baidu.com/item/%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%87%E4%BB%B6/996661)、[可执行文件](https://baike.baidu.com/item/%E5%8F%AF%E6%89%A7%E8%A1%8C%E6%96%87%E4%BB%B6)、[目标代码](https://baike.baidu.com/item/%E7%9B%AE%E6%A0%87%E4%BB%A3%E7%A0%81/9407934)、共享库和核心转储格式文件

是UNIX系统实验室（[USL](https://baike.baidu.com/item/USL)）作为应用程序二进制接口（Application Binary Interface，[ABI](https://baike.baidu.com/item/ABI)）而开发和发布的，也是[Linux](https://baike.baidu.com/item/Linux/27050)的主要可执行文件格式。

1999年，被86open项目选为[x86架构](https://baike.baidu.com/item/x86%E6%9E%B6%E6%9E%84/7470217)上的类[Unix操作系统](https://baike.baidu.com/item/Unix%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F)的二进制文件标准格式

ELF文件由4部分组成，分别是ELF头（ELF header）、程序头表（Program header table）、节（Section）和节头表（Section header table）。实际上，一个文件中不一定包含全部内容，而且他们的位置也未必如同所示这样安排，只有ELF头的位置是固定的，其余各部分的位置、大小等信息由ELF头中的各项值来决定。 [1] 

文件头

最开头是16个字节的e_ident, 其中包含用以表示ELF文件的字符，以及其他一些与机器无关的信息。开头的4个字节值固定不变，为0x7f和ELF三个字符。

**e_type** 它标识的是该文件的类型。

**e_machine** 表明运行该程序需要的体系结构。

**e_version** 表示文件的版本。

**e_entry** 程序的入口地址。

**e_phoff** 表示Program header table 在文件中的[偏移量](https://baike.baidu.com/item/%E5%81%8F%E7%A7%BB%E9%87%8F/9180391)（以字节计数）。

**e_shoff** 表示Section header table 在文件中的偏移量（以字节计数）。

**e_flags** 对IA32而言，此项为0。

**e_ehsize** 表示ELF header大小（以字节计数）。

**e_phentsize** 表示Program header table中每一个条目的大小。

**e_phnum** 表示Program header table中有多少个条目。

**e_shentsize** 表示Section header table中的每一个条目的大小。　　

**e_shnum** 表示Section header table中有多少个条目。　　

**e_shstrndx** 包含节名称的字符串是第几个节（从零开始计数）。 

### 程序头表

Program header描述的是一个段在文件中的位置、大小以及它被放进内存后所在的位置和大小

p_type 当前Program header所描述的段的类型。

p_offset 段的第一个字节在文件中的偏移。

p_vaddr 段的一个字节在内存中的[虚拟地址](https://baike.baidu.com/item/%E8%99%9A%E6%8B%9F%E5%9C%B0%E5%9D%80/1329947)

p_paddr 在[物理内存](https://baike.baidu.com/item/%E7%89%A9%E7%90%86%E5%86%85%E5%AD%98/2502263)定位相关的系统中，此项是为物理地址保留。

p_filesz 段在文件中的长度。

p_memsz 段在内存中的长度。

p_flags 与段相关的标志。

p_align 根据此项值来确定段在文件及内存中如何对齐。 

## Makefile



make: *** No rule to make target `.gdbinit.tmpl', needed by `.gdbinit'.

有可能makefile里面文件名后面加了空格，没看出来

也有可能就是文件不在，因为是隐藏文件，所以没复制过来



一个工程中的源文件不计其数，其按类型、功能、模块分别放在若干个目录中，makefile定义了一系列的规则来指定，哪些文件需要先编译，哪些文件需要后编译，哪些文件需要重新编译，甚至于进行更复杂的功能操作，因为 makefile就像一个[Shell脚本](https://baike.baidu.com/item/Shell%E8%84%9A%E6%9C%AC)一样，其中也可以执行操作系统的[命令](https://baike.baidu.com/item/%E5%91%BD%E4%BB%A4/8135974)。

Makefile 文件描述了整个工程的编译、连接等规则。其中包括：工程中的哪些源文件需要编译以及如何编译、需要创建那些库文件以及如何创建这些库文件、如何最后产生我们想要的可执行文件。尽管看起来可能是很复杂的事情，但是为工程编写Makefile 的好处是能够使用一行命令来完成“自动化编译”，一旦提供一个（通常对于一个工程来说会是多个）正确的 Makefile。编译整个工程你所要做的唯一的一件事就是在shell 提示符下输入make命令。整个工程完全自动编译，极大提高了效率。

make是一个命令工具，它解释Makefile 中的指令（应该说是规则）。在Makefile文件中描述了整个工程所有文件的编译顺序、编译规则。Makefile 有自己的书写格式、关键字、函数。像C 语言有自己的格式、关键字和函数一样。而且在Makefile 中可以使用系统shell所提供的任何命令来完成想要的工作。Makefile（在其它的系统上可能是另外的文件名）在绝大多数的IDE 开发环境中都在使用，已经成为一种工程的编译方法。

#### 附录的一个习题

这道题希望我们能够去修改boot loader的链接地址，在Lab 1中，作者引入了两个概念，一个是链接地址，一个是加载地址。链接地址可以理解为通过编译器链接器处理形成的可执行程序中指令的地址，即逻辑地址。加载地址则是可执行文件真正被装入内存后运行的地址，即物理地址。

　　那么在boot loader中，由于在boot loader运行时还没有任何的分段处理机制，或分页处理机制，所以boot loader可执行程序中的链接地址就应该等于加载地址。在Lab中作者说，BIOS默认把boot loader加载到0x7C00内存地址处，所以就要求boot loader的链接地址也要在0x7C00处。boot loader地址的设定是在boot/Makefrag中完成的，所以根据题目的要求，我们需要改动这个文件的值。

　　首先按照题目要求，在lab目录下输入make clean，清除掉之前编译出来的内核可执行文件

BIOS会把boot loader程序默认装入到0x7c00处，结果就会出现错误

## 管道

管道	一个小的内核缓冲区		文件描述符对形式	提供给进程	一个用于写操作，一个用于读操作        一种进程间交互的方式    通信共享文件

管道半双工，数据只能向一个方向流动，需要双方通信时，需要建立起两个管道

只能用于父子进程或者兄弟进程（为什么？具体实现？）之间（具有亲缘关系的进程）

单独构成一种独立的文件系统，只存在与内存中

对于两端的进程而言，管道就是一个文件

缓冲区	两端进程	固定读出写入

不足：不能多个接收者（但是可以多个进程在读，读过就移去）；多个读写进程之间没有区分，同时操作一个管道

```c
代码运行了程序 wc，它的标准输出绑定到了一个管道的读端口。
int p[2];
char *argv[2];
argv[0] = "wc";
argv[1] = 0;
pipe(p);    //创建一个新的管道并且将读写描述符记录在数组 p 中
if(fork() == 0) {
	close(0);
	dup(p[0]);  //将管道的读端口拷贝在描述符0上
	close(p[0]);  //关闭文件的某个接口（还有其他的接口可以访问）
	close(p[1]);
	exec("/bin/wc", argv);//当 wc 从标准输入读取时，它实际上是从管道读取的
} else {
	write(p[1], "hello world\n", 12);
    //父进程向管道的写端口写入然后关闭它的两个文件描述符。
	close(p[0]);
	close(p[1]);
}

这段程序调用 pipe，创建一个新的管道并且将读写描述符记录在数组 p 中。在 fork 之后，父进程和子进程都有了指向管道的文件描述符。子进程将管道的读端口拷贝在描述符0上，关闭 p 中的描述符，然后执行 wc。当 wc 从标准输入读取时，它实际上是从管道读取的。父进程向管道的写端口写入然后关闭它的两个文件描述符。
```

如果数据没有准备好，`read`会一直等待，直到有数据，或者写端口的描述符都已经关闭（返回0）

```
xv6 shell 对管道的实现（比如 fork sh.c | wc -l）和上面的描述是类似的（7950行）

子进程创建一个管道连接管道的左右两端。然后它为管道左右两端都调用 runcmd，然后通过两次 wait 等待左右两端结束。管道右端可能也是一个带有管道的指令，如 a | b | c, 它 fork 两个新的子进程（一个 b 一个 c），因此，shell 可能创建出一颗进程树。树的叶子节点是命令，中间节点是进程，它们会等待左子和右子执行结束。理论上，你可以让中间节点都运行在管道的左端，但做的如此精确会使得实现变得复杂。
（管道实现没看懂）
```

pipe 和临时文件：看上去没有什么两样：命令

`echo hello world | wc`

可以用无管道的方式实现：

`echo hello world > /tmp/xyz; wc < /tmp/xyz`

管道和临时文件起码有三个关键的不同点：

* 自我清扫。如果是 shell 重定向的话，我们必须要在任务完成后删除 `/tmp/xyz`。
* 可以传输任意长度的数据
* 允许同步：两个进程可以使用“一对管道”来进行二者之间的信息传递，每一个读操作都阻塞调用进程（？管道数据充足也阻塞？），直到另一个进程用 `write` 完成数据的发送。（临时文件也可以实现信息传递吧？两个进程用不同的偏移量）

## 文件系统

xv6 文件系统：文件和目录	文件：字节数组，目录：引用（指针）（文件和其他目录）

xv6 把目录实现为一种特殊的文件		

目录是一棵树，根节点是一个特殊的目录 `root` 

`/a/b/c`    	目录 `b` 中的文件 `c`			而b在目录 `a` 中	 `a` 在 `root` 目录下

不从 `/` 开始的目录：相对当前目录的目录

当前目录通过 `chdir` （系统调用）改变

```c
下面的这些代码都打开同一个文件（假设所有涉及到的目录都是存在的）
chdir("/a");
chdir("b");
open("c", O_RDONLY);

open("/a/b/c", O_RDONLY);

第一个代码段将当前目录切换到 /a/b; 第二个代码片段则对当前目录不做任何改变。
```



```
mkdir 创建一个新的目录

open 加上 O_CREATE 标志打开一个新的文件

mknod 创建一个新的设备文件

（设备文件：是DOS管理设备的一种方法：为设备起一个固定的文件名，可以象使用文件一样方便地管理这些设备。）

mkdir("/dir");
fd = open("/dir/file", O_CREATE|O_WRONGLY);
close(fd);
mknod("/console", 1, 1);

mknod 在文件系统中创建一个文件，没有内容，元信息标志它是一个设备文件，并且记录主设备号和辅设备号（mknod 的两个参数），两个设备号唯一确定一个内核设备。后续操作转发到内核设备的实现上，而不是传递给文件系统。
```

（这些系统调用都是怎么实现的呀？）

```
fstat 可以获取一个文件描述符指向的文件的信息，填充一个名为 stat 的结构体，它在 stat.h 中定义为：

#define T_DIR  1
#define T_FILE 2
#define T_DEV  3
// Directory
// File
// Device
     struct stat {
       short type;  // Type of file
       int dev;     // File system’s disk device
       uint ino;    // Inode number
       short nlink; // Number of links to file
       uint size;   // Size of file in bytes
};
```

文件	 `inode` 	inode号		文件名	 `links` 		 `nlink`（link数） 为0时清空文件的磁盘空间

```c
系统调用 link ：创建另一个文件名，指向同一个 inode		下面的代码创建了一个既叫做 a 又叫做 b 的新文件

open("a", O_CREATE|O_WRONGLY);
link("a", "b");

系统调用 unlink ：从文件系统移除一个文件名
```

```c
fd = open("/tmp/xyz", O_CREATE|O_RDWR);
unlink("/tmp/xyz");  //路径也算是一个文件名吗？

是创建一个临时 inode 的最佳方式，这个 inode 会在进程关闭 fd 或者退出的时候被清空。
```

xv6 关于文件系统的操作都被实现为用户程序，诸如 `mkdir`，`ln`，`rm` 

* 这种设计允许任何人都可以通过用户命令拓展 shell
* Unix 时代的其他系统都将这样的命令内置在了 shell 中，而 shell 又是内置在内核中的。
* 有一个例外， `cd`是在 shell 中实现的（8016）
* `cd` 必须改变 shell 自身的当前工作目录，如果 作为一个普通命令执行， shell 就会 `fork` 一个子进程，子进程运行 `cd`，`cd` 只会改变*子进程*的当前工作目录

## 内核空间

Kernel space 可以执行任意命令，调用系统的一切资源；User space 只能执行简单的运算，不能直接调用系统资源，必须通过系统接口（又称 system call），才能向内核发出指令。

上面代码中，第一行和第二行都是简单的赋值运算，在 User space 执行。第三行需要写入文件，就要切换到 Kernel space，因为用户不能直接写文件，必须通过内核安排。第四行又是赋值运算，就切换回 User space。

查看 CPU 时间在 User space 与 Kernel Space 之间的分配情况，可以使用`top`命令。它的第三行输出就是 CPU 时间分配统计。

## 文件描述符

如果要对某个设备进行操作，就不得不打开此设备文件，打开文件就会获得该文件的文件描述符fd( file discriptor), 它就是一个很小的整数，每个进程在PCB（Process Control Block）中保存着一份文件描述符表，文件描述符就是这个表的索引，每个表项都有一个指向已打开文件的指针。

文件描述符的分配规律：从当前未使用的最小的整数开始分配；

已打开的文件在内核中用file结构体表示，文件描述符表中的指针指向file结构体。file结构体才是内核中用来描述文件属性的结构体。

## fork函数

使用fork函数得到的子进程从父进程的继承了整个进程的地址空间，包括：进程上下文（所以子进程在fork之后开始向下执行，而不会从头开始执行）、进程堆栈、内存信息、打开的文件描述符、信号控制设置、进程优先级、进程组号、当前工作目录、根目录、资源限制、控制终端等。

子进程与父进程的区别在于：

1、父进程设置的锁，子进程不继承（因为如果是排它锁，被继承的话，矛盾了）

2、各自的进程ID和父进程ID不同

3、子进程的未决告警被清除；

4、子进程的未决信号集设置为空集。

#### 写时复制

linux系统为了提高系统性能和资源利用率，在fork出一个新进程时，系统并没有真正复制一个副本。

如果多个进程要读取它们自己的那部分资源的副本，那么复制是不必要的。

每个进程只要保存一个指向这个资源的指针就可以了。

如果一个进程要修改自己的那份资源的“副本”，那么就会复制那份资源。这就是写时复制的含义

#### 僵尸进程和孤儿进程

fork系统调用之后，父子进程将交替执行，执行顺序不定。

（注：任何一个进程都必须有父进程）

* 进程0：Linux引导中创建的第一个进程，完成加载系统后，演变为进程调度、交换及存储管理进程 
* 进程1：init 进程，由0进程创建，完成系统的初始化. 是系统中所有其它用户进程的祖先进程

任何一个子进程(init除外)在exit()之后，并非马上就消失掉，而是留下一个称为僵尸进程(Zombie)的数据结构，等待父进程处理。

这种机制就是: 在每个进程退出的时候,内核释放该进程所有的资源,包括打开的文件,占用的内存等。 但是仍然为其保留一定的信息(包括进程号the process ID,退出状态the termination status of the process,运行时间the amount of CPU time taken by the process等)。直到父进程通过wait / waitpid来取时才释放。

如果进程不调用wait / waitpid的话， 那么保留的那段信息就不会释放，其进程号就会一直被占用

每当出现一个孤儿进程的时候，内核就把孤儿进程的父进程设置为init，而init进程会循环地wait()它的已经退出的子进程

## wait

进程一旦调用了wait，就立即阻塞自己，由wait自动分析是否当前进程的某个子进程已经退出，如果让它找到了这样一个已经变成僵尸的子进程，wait就会收集这个子进程的信息，并把它彻底销毁后返回；如果没有找到这样一个子进程，wait就会一直阻塞在这里，直到有一个出现为止。

参数status用来保存被收集进程退出时的一些状态，它是一个指向int类型的指针。但如果我们对这个子进程是如何死掉的毫不在意，只想把这个僵尸进程消灭掉，（事实上绝大多数情况下，我们都会这样想），我们就可以设定这个参数为NULL，就象下面这样：

```
		pid = wait(NULL);
```

#### 返回值：

如果成功，wait会返回被收集的子进程的进程ID，如果调用进程没有子进程，调用就会失败，此时wait返回-1，同时errno被置为ECHILD。

#### waitpid

本质上讲，系统调用waitpid和wait的作用是完全相同的，但waitpid多出了两个可由用户控制的参数pid和options，从而为我们编程提供了另一种更灵活的方式。

（具体介绍见收藏）

## exec

事实上，并没有叫做“exec ”的系统调用，所谓的“ exec ”系统调用实际上是6个以execAB形式命名的系统调用，其搭配方式如下图所示：

这里的A可以为 l 或 v ，这取决于参数是直接在调用（列表）中，还是在数组（向量）中；B要么没有，要么为 p ，p 表示使用 PATH 环境变量查找程序，要么为 e ，e 表示使用特定环境。（用户不能在同一调用中同时获得特征 p 和 e 。因此，这6个调用分别为execl,execv, execlp, execvp, execle, execve。

path参数是一个可由有效用户ID（比如说模式755）执行的程序文件路径名，该可执行程序的内容要正确。

    path 参数后面的一组参数收入到一个字符指针数组，且最后一个参数的值为NULL，用来停止收入和终止数组，这组参数的第一个都是程序文件的名字（并不是整个路径）。
    
    通过重新初始化栈，来自程序的指令覆盖了进程的指令段，并且来自程序的数据也覆盖了进程的用户数据，然后进程从顶端执行该新程序（也就是说，调用了它的 mian 函数）。

注意一下 execlp 和 execvp ，由前面的介绍可知，当B为 p 时，表示使用 PATH 环境变量查找程序，下面让我们来简单的看一下。

    若 execlp 或 execvp 的 file 参数没有斜线，则把PATH环境变量中所列出的字符串一个一个与之比较，直到定位到一个带有结果路径名的普通文件，其中结果路径名具有执行权限。

  情况一：如果这个文件包含一个可执行程序（在它的第一个字处由一个代码号指示的），便执行它；

  情况二：如果不包含一个程序，则将其假定为一个脚本文件，通常为了运行这个脚本文件，都把路径作为shell的第一个参数来执行shell。

    如果 PATH 查找的结果证明没有任何内容是可在执行的，则 exec 失败。
    
    若 file 参数中有“ / ”，就不用进行查找，即认为完成了路径查找，但是它仍然可能是一个脚本文件。

## init进程

Linux中的所有进程都是由init进程创建并运行的。首先Linux内核启动，然后在用户空间中启动init进程，再启动其他系统进程。在系统启动完成后，init将变成为守护进程监视系统其他进程。

## 特权级

1.在x86中的数据和代码是按段来存放的:[section],GTL/LDT里的每个段描述符被设置有不同的特权级DPL.

2.程序是通过选择子/门调用等等来在段之间来回走动的.实现用户级与系统级的调用跳转

	跳转：CPU 工作时，它是不停的读出存储器中的程序代码、执行。  通常是，顺序读出、顺序执行。  但是，编程者，有时，希望从另外一个地址，再执行一些代码。  这就要编写进去一个无[条件转移指令](https://www.baidu.com/s?wd=%E6%9D%A1%E4%BB%B6%E8%BD%AC%E7%A7%BB%E6%8C%87%E4%BB%A4&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao)。更常用的，是[条件转移指令](https://www.baidu.com/s?wd=%E6%9D%A1%E4%BB%B6%E8%BD%AC%E7%A7%BB%E6%8C%87%E4%BB%A4&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao)。  比如，当前运算，出现了进位，则要转移到另一处执行。 如果没有进位，就不转移，继续按照顺序执行。

3.与GDT里的段描述符一样，CPU寄存器内的每一个选择子/门调用选择子是有分等级的:这个是在选择符的结构中:RPL(最后2位)



这节讲述IA32分段机制中的特权级。包括CPL、DPL、RPL的介绍以及代码实现不同特权级之间的转换。

IA32的分段机制有四种特权级别，从高到低分别是0、1、2、3。数字越小表示的特权级越大。

处理器引入特权级的目的是为了保护核心代码和数据。核心的代码和数据会被放在较高的层级中。从而避免低特权级（外层）的任务在不被允许的情况下访问位于高特权级（内层）的段。



为了有效地实现保护，同一个任务在不同的特权级下使用不同的堆栈（就是栈）

比如，每个任务都有用户态栈和内核栈

（不同的堆栈，就是两块区域作为独立的栈存在，独立发挥栈的功能

- 所谓堆栈，就是在存储器中按数据“后进先出（Last In First Out,LIFO）”的原则组织的连续存储空间。每个任务都有自己的堆栈，任务堆栈是任务的重要组成部分，其作用主要表现在以下两点：

  1. 当任务切换或响应中断时，用来保存CPU寄存器中的内容，当任务挂起的时候，将CPU寄存器的内容压入堆栈，恢复的时候再弹出来给CPU寄存器。
  2. 当任务运行时，它用来保存一些局部变量，函数参数

  ）

栈不是用来存储指令的，只是作为cpu的一个临时存储器，一个演算纸，但是上面有权利保存与任务相关的重要的信息（虽然可擦去），任务的活动，尤其是与硬件相关的底层操作，不应该

例如，当从外层特权级3变换到内层特权级0时，任务使用的堆栈也同时从3级变换到0级堆栈；当从内层特权级0变换到外层特权级3时，任务使用的堆栈也同时从0级堆栈变换到3级堆栈。所以，一个任务可能具有四个堆栈，对应四个特权级。四个堆栈需要四个堆栈指针。 　　TSS的内层堆栈指针区域中有三个堆栈指针，它们都是48位的全指针(16位的选择子和32位的偏移)，分别指向0级、1级和2级堆栈的栈顶，依次存放在TSS中偏移为4、12及20开始的位置。当发生向内层转移时，把适当的堆栈指针装入SS及ESP寄存器以变换到内层堆栈，外层堆栈的指针保存在内层堆栈中。没有指向3级堆栈的指针，因为3级是最外层，所以任何一个向内层的转移都不可能转移到3级。 　　但是，当特权级由内层向外层变换时，并不把内层堆栈的指针保存到TSS的内层堆栈指针区域。实际上，处理器从不向该区域进行写入，除非程序设计者认为改变该区域的值。这表明向内层转移时，总是把内层堆栈认为是一个空栈。因此，不允许发生同级内层转移的[递归](https://baike.baidu.com/item/%E9%80%92%E5%BD%92)，一旦发生向某级内层的转移，那么返回到外层的正常途径是相匹配的向外层返回。

## TSS

任务状态段(Task State Segment, TSS)是[x86](https://baike.baidu.com/item/x86)架构电脑上是一个保存[任务](https://baike.baidu.com/item/%E4%BB%BB%E5%8A%A1)信息的数据结构，通过它实现任务的挂起和恢复

tss的作用举例：保存不同特权级别下任务所使用的寄存器，特别重要的是esp，因为比如中断后，涉及特权级切换时(一个任务切换)，首先要切换栈，这个栈显然是内核栈，那么如何找到该栈的地址呢，这需要从tss段中得到，这样后续的执行才有所依托(在x86机器上，c语言的函数调用是通过栈实现的)。

只要涉及地特权环到高特权环的任务切换，都需要找到高特权环对应的栈，因此需要esp2，esp1，esp0起码三个esp，然而linux只使用esp0

tss是一个段，tss段基址在tr寄存器中

intel建议为每一个进程准备一个独立的tss段，进程切换的时候切换tr寄存器使之指向该进程对应的tss段，然后在任务切换时(比如涉及特权级切换的中断)使用该段保留所有的寄存器。

linux没有为每一个进程都准备一个tss段，而是每一个cpu使用一个tss段，tr寄存器保存该段。进程切换时，只更新唯一tss段中的esp0字段到新进程的内核栈。

linux的tss段中只使用esp0和iomap等字段，不用它来保存寄存器，在一个用户进程被中断进入ring0的时候，tss中取出esp0，然后切到esp0，其它的寄存器则保存在esp0指示的内核栈上而不保存在tss中

## 进程的内存空间

**各个地址空间的数据**

内核空间

**栈**：保存局部变量、函数形参、自动变量。

**堆**：保存由malloc、ralloc、calloc分配空间的变量。

**bss段**：保存未初始化或初始化为0的全局变量。

**rodata段**：在全局数据里保存常量。

**data段**（静态数据区）：保存初始化不为0的全局变量或者static修饰的变量。

**代码段**：保存代码。

**各地址空间的特点** 

栈：栈的权限是由系统决定的。数据具有先进后出、后进先出的特点。如果定义了一个未初始化的局部变量，那么它的值是一个随机值。

堆：堆的权限是由用户决定的，用户通过malloc、rolloc、calloc分配地址空间，并使用free()函数能够释放空间。数据具有先进先出、后进后出的特点。

bss段：如果定义了一个未初始化的全局变量，那么它的值为0。

rodata段：保存常量，常量的值是不允许被修改的。

data段：如果定义了一个未初始化的静态全局变量，那么它的值为0。

代码段：保存代码。

 

第一、4G的进程地址空间被人为的分为两个部分——用户空间与内核空间。用户空间从0到3G（0xC0000000），内核空间占据3G到4G。用户进程通常情况下只能访问用户空间的虚拟地址，不能访问内核空间虚拟地址。只有用户进程进行系统调用（代表用户进程在内核态执行）等时刻可以访问到内核空间。

第二、用户空间对应进程，所以每当进程切换，用户空间就会跟着变化；而内核空间是由内核负责映射，它并不会跟着进程改变，是固定的。内核空间地址有自己对应的页表（init_mm.pgd），用户进程各自有不同的页表。

第三、每个进程的用户空间都是完全独立、互不相干的。不信的话，你可以把上面的程序同时运行10次（当然为了同时运行，让它们在返回前一同睡眠100秒吧），你会看到10个进程占用的线性地址一模一样。

 从上面已经看到进程所能直接操作的地址都为虚拟地址。当进程需要内存时，从内核获得的仅仅是虚拟的内存区域，而不是实际的物理地址，进程并没有获得物理内存（物理页面——页的概念请大家参考硬件基础一章），获得的仅仅是对一个新的线性地址区间的使用权。实际的物理内存只有当进程真的去访问新获取的虚拟地址时，才会由“请求页机制”产生“缺页”异常，从而进入分配实际页面的例程。

该异常是虚拟内存机制赖以存在的基本保证——它会告诉内核去真正为进程分配物理页，并建立对应的页表，这之后虚拟地址才实实在在地映射到了系统的物理内存上。（当然，如果页被换出到磁盘，也会产生缺页异常，不过这时不用再建立页表了）

这种请求页机制把页面的分配推迟到不能再推迟为止，并不急于把所有的事情都一次做完（这种思想有点像设计模式中的代理模式（proxy））。之所以能这么做是利用了内存访问的“局部性原理”，请求页带来的好处是节约了空闲内存，提高了系统的吞吐率。要想更清楚地了解请求页机制，可以看看《深入理解linux内核》一书。 

## 子程序

为了[简化](https://baike.baidu.com/item/%E7%AE%80%E5%8C%96)程序，可以把这些重复的[程序段](https://baike.baidu.com/item/%E7%A8%8B%E5%BA%8F%E6%AE%B5)单独列出，并按一定的[格式](https://baike.baidu.com/item/%E6%A0%BC%E5%BC%8F)编写成[子程序](https://baike.baidu.com/item/%E5%AD%90%E7%A8%8B%E5%BA%8F)。主程序在执行过程中如果需要某一子程序，通过调用[指令](https://baike.baidu.com/item/%E6%8C%87%E4%BB%A4)来调用该子程序，子程序执行完后又返回到主程序，继续执行后面的[程序段](https://baike.baidu.com/item/%E7%A8%8B%E5%BA%8F%E6%AE%B5)。

一般会有输入参数并有返回值，提供对过程的封装和细节的隐藏。这些代码通常被集成为[软件库](https://baike.baidu.com/item/%E8%BD%AF%E4%BB%B6%E5%BA%93/5932418)

子程序的[主体](https://baike.baidu.com/item/%E4%B8%BB%E4%BD%93/13020287)（body）是一个代码区块，当它被调用时就会进入运行

## cpu指令周期

计算机之所以能自动地工作，是因为CPU能从存放程序的内存里取出一条指令并执行这条指令；紧接着又是取指令，执行指令，如此周而复始，构成了一个封闭的循环。除非遇到停机指令，否则这个循环将一直继续下去。

时钟周期=振荡周期，名称不同而已，都是等于单片机晶振频率的倒数，如常见的外接12M晶振，那它的时钟周期=1/12M。

机器周期，8051系列单片机的机器周期=12*[时钟周期](https://www.baidu.com/s?wd=%E6%97%B6%E9%92%9F%E5%91%A8%E6%9C%9F&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao)，之所以这样分是因为单个时钟周期根本干不了一件完整的事情（如取指令、写寄存器、读寄存器等），而12个时钟周期就能基本完成一项基本操作了。 

指令周期，CPU从内存取出一条指令并执行这条指令的时间总和

## cpu指令集

可执行文件中保存的是二进制机器码，这串机器码可以直接被CPU读取和执行

一条汇编指令与一段机器码是一一对应的

需要被执行的机器码先要被OS调度到内存之中, 程序执行时, 机器码依次经过了Memory--Cache--CPU fetch, 进入CPU流水线, 接着就要对它进行译码了, 译码工作生成的象是CPU内部数据格式, 微码(或者类似的格式, 这个格式不同的厂商会自己设计).   

软件层: 汇编语言

接口: 汇编语言所对应的机器码

硬件层: CPU使用内部数据结构进行运算  

如果机器码代表的功能是在指令集规范内的, 这条机器码就可以生产微码, 并在CPU内正常流动. 假设机器码是错误的, 是不可以通过CPU的译码阶段的, 控制电路一定会报错. 这种情况反映在Windows里往往都是蓝屏, 因为CPU无法继续执行, 它连下一条指令在哪都不知道

那么指令集在CPU里就代表: 只有CPU指令集范围内的指令可以被成功的译码, 并送往CPU流水线后端去执行.和常规的想法不一样, CPU不需要任何形式的存储介质去存储指令集, 因为"译码"这个步骤就是在对指令集里规范的机器码做解码. 硬件上, 译码这件事需要庞大数目的逻辑门阵列来实现

比如我们设计一套指令集，其中肯定有条加法指令。比如Add R1 R2 。我们可以认为这条指令的意思是计算寄存器R1中的内容和R2的和，然后把结果存到R1寄存器中。

 

那么经过编译后这条指令会变成二进制，比如010100010010 。这条二进制指令一共12位。明显可以分为三大部分。最前面的0101表示这是条加法指令，后面0001说的是第一个操作数是寄存器1，最后0010说的是第二个数就是寄存器2（其实实际没有这么简单的指令，至少应该区分操作数是寄存器还是直接的数据，但为了把这说的更容易理解作了简化）。我们可以通过十二根导线把这条指令输入一个CPU中。导线通电就是1，不通电就是0 。为了叙述方便我们从左到右用A0-A11给这12根导线编上号。

 

然后计算机会分析这条指令。步骤如下：

1. 最开始的两根导线A0和A1，第一根有电第二根没电，就能知道这是一条运算指令（而非存储器操作或者跳转等指令）。那么指令将被送入逻辑运算单元（ALU）去进行计算。其实很简单。只要这两根线控制接下来那部分电路开关即可。
2. 接下来的A2和A3，01表示加法，那么就走加法运算那部分电路，关闭减法等运算电路。
3. A4-A7将被送入寄存器电路，从中读取寄存器保存的值。送到ALU的第一个数据接口电路上。
4. 后面的A8-A11同样被送入寄存器选择电路，接通R2寄存器，然后R2就把值送出来，放到ALU的第二个数据接口上。
5. ALU开始运算，把两个接口电路上的数据加起来，然后输出。
6. 最后结果又被送回R1。

基本上简单的运算计算机就是这么操作的。他其实不知道你那些指令都是什么意思。具体的指令编程机器码后就会变成数字电路的开关信号。其中某几段会作为控制信号，控制其他部分的数据走不同的电路以执行运算。他没有一个地方保存着如何翻译这些机器码的字典，所有机器码的意义都被体现在整个电路的设计中了。

 当然，从汇编到机器码这步是汇编程序翻译的。汇编程序当然知道某条指令要翻译成什么样的机器码。

## cpu和寄存器

数据从内存里拿出来先放到寄存器，然后CPU 再从寄存器里读取数据来处理，处理完后同样把数据通过寄存器存放到内存里，CPU 不直接和内存打交道

8086机中，任意时刻，CPU将CS:IP指向的内容当作指令执行。

修改CS，IP

mov指令不能用于设置CS、IP的值，8086CPU没有提供这样的功能。

8086CPU为CS、IP提供了另外的指令来改变它们的值：转移指令

#### 控制寄存器  cr0，cr1……

控制寄存器（CR0～CR3）用于控制和确定处理器的操作模式以及当前执行任务的特性，如图4-3所示。CR0中含有控制处理器操作模式和状态的系统控制标志；CR1保留不用；CR2含有导致页错误的线性地址；

CR3中含有页目录表物理内存基地址，因此该寄存器也被称为页目录基地址寄存器PDBR（Page-Directory Base address Register）

1、  每个进程有一个属于自己的页目录表，可通过 CR3 寄存器找到 
2、  而内核也有一个独立于其它进程的页目录表，保存在 swapper_pg_dir[] 数组中

3、  当进程切换的时候，只需要将新进程的页目录把地址加载到 CR3 寄存器中即可

4、  创建一个新进程的时候，需要为它分配一个 page，作为页目录表，并将 swapper_pg_dir[] 的高 256 项拷贝过来，低 768 项则清0



**JMP 段地址：偏移地址**

JMP 2AE3:3

功能：用指令中给出的段地址修改CS，偏移地址修改IP。CS = 2AE3H， IP = 0003H。

仅修改IP的内容：

jmp 某一合法寄存器

jmp ax   （类似于 mov IP,ax）

功能：用寄存器中的值修改IP。



### 8.21

从一个操作系统的角度来说，xv6 的代码量并不大，总共不到一万行，分散在众多的源文件中。一上来可能觉得很迷茫，这么多文件，该从哪个开始看起？Makefile 则是这些文件的“目录”，通过它可以很容易找到头绪。

makefile

xv6 编译成功后会生成两个文件：xv6.img 和 fs.img

从 Makefile 中可以看到 xv6.img 的生成条件需要bootlock

bootblock 的生成只需要两个文件，一个汇编一个 C 源码

### 分段机制

#### 程序 = 数据 + 指令

无论是操作系统还是运行在操作系统上的软件，对于计算机来说他们都是程序。而程序的组成我们可以简单的理解为：数据加上指令就是程序。当一个程序被从硬盘加载到内存后，CPU 从内存读取程序中的指令执行，执行过程中需要从内存中读取程序的数据，配合指令计算出结果之后还需要放回到内存中。这就是简化后的程序执行过程。

#### 如何从内存读取指令和数据

x86 使用“段基址 + 偏移量”的方式来读写内存。这就好比问路，当你向一个人问路时，一般人们回这么回答你：“从前面那个路口开始，往前再走三个路口就到了”。x86 CPU 对内存的寻址也是这个思路，“前面那个路口”就指的是“段基址”，“往前再走三个路口”指的就是“偏移量”，有了这两个线索，CPU 也可以顺利到达内存中的目的地写入或取走数据或指令。

#### 为什么有个“段”字

有人可能会问“段基址”里面的“段”代表什么呢？前面说了，程序是由数据和指令组成的，一个程序要运行就先要加载到内存中。而程序中的数据和指令是两个相互独立的部分，CPU 从内存读取他们的时候也是将他们看作是不同的“段”。这里还要插一句，程序中的数据还要分很多种类型，所以 CPU 针对一个程序的不同部分准备了 4 个寄存器来分别存储他们的“段基址”。这 4 个寄存器分别是用于程序指令的 CS 代码段寄存器、用于程序数据的 DS 数据段寄存器、用于程序堆栈（也是数据的一种）的 SS 堆栈段寄存器和 ES 附加段寄存器（也是数据的一种）。

有了这 4 个寄存器存储“基地址”（数据的存放起始点），再配合“偏移量” CPU 就可以从内存读写数据和指令了。例如 CPU 在从内存中读取一个程序的指令准备执行的时候就可以说：“从 CS 指向的地方开始向后读取 2 个位置”，内存收到 CPU 给的“指路信息”后就会把相应位置的指令发给 CPU，CPU 拿到指令就可以开始执行了。

### A20 gate



8086 的年代已经远去。现在的 x86 已早经是 32 位的了（目前 32 位基本已经没有了，64 位是主流了）。但无论位数如何增加，寻址能力如何增大，x86 一直保持着向下兼容的良好传统。即便是当初为 8086 这种 16 位机器写的软件或操作系统（如 DOS）仍能够在现在的 x86 计算机上顺利运行。

那么这种良好的向下兼容性是如何实现的呢？答案是：“开关”。现代的 x86 计算机，无论你是 32 位的还是 64 位的，在开机的那一刻 CPU 都是以模拟 16 位模式运行的，地址卷绕机制也是有效的，所以无论你的电脑内存有多大，开机的时候 CPU 的寻址能力只有 1MB，就好像回到 8086 时代一样。

那么什么时候才结束 CPU 的 16 位模式运行呢？这由你（操作系统）说了算，现代的计算机都有个“开关”叫 A20 gate，开机的时候 A20 gate 是关闭的，CPU 以 16 位模式运行，当 A20 gate 打开的时候“卷绕”机制失效，内存寻址突破 1MB 限制，我们就可以切换到正常的模式下运行了。

### 虚拟地址访问



我们编写程序代码，编译器将我们的程序代码变成 CPU 可以理解的指令（二进制可执行程序）。在运行我们写的程序前，需要将程序先加载进内存，而我们的程序应该加载到内存的什么位置这应该是由操作系统来负责的，我们程序本身是不能决定这一切的。

这里就产生了一个“矛盾”。在程序真正运行前我们是不知道我们会被放在内存的什么地方，但是我们的程序本身还有数据，代码也含有对数据的访问（例如我们的代码中使用的各种变量），我们不知道我们的数据会被操作系统放在哪，但我们还要在代码里写访问这些数据的逻辑，这是一个矛盾，要怎么办？

解决上述矛盾的办法就是使用相对地址访问。我们的程序在运行前不知道会被操作系统放在内存的什么地方，所以我们在编写程序的时候会做个假设，假设我们的程序会被放在从内存地址 N 开始向后的地方。这个时候我们的程序在访问我们的变量时都继续这个假设，加入我们想要读取我们的变量 a 时，我们就编写指令说我们要访问 N + X 的内存地址，那里存放着我们的变量 a，当然这些假设和生成每个数据的相对访问地址的工作都由编译器代劳了，对于我们程序的编写来说不用为这些事情而烦恼。

所以我们的每一个程序都会基于一个统一的假设：“我们会被从内存地址 N 开始放置”，至于到真正运行时这个 N 对应的内存地址具体是多少无所谓，因为我们对我们程序数据的访问都是相对于 N 的偏移。这就好比说：“我在距离你左边 20 米的地方”，无论你在哪，在火星上也罢，向左走 20 米，你总能找到我。

 当程序真正运行的时候，操作系统将程序加载到内存时就不能对程序的这个“假设”听之任之了。当操作系统把程序放置到真正的内存位置后，程序运行起来，程序基于假设 N + X 计算出的内存地址就需要操作系统“翻译”成真正的内存地址后才能真的从内存中读取到想要的数据，而这个“翻译”的过程就需要操作系统和 CPU 来配合实现了。

程序基于“假设”计算出的地址叫做“虚拟地址”也叫做“逻辑地址”（他们是一样的，只是叫法不同），与之对应的内存的真实地址叫做“物理地址”，从“虚拟地址”到“物理地址”的转换是通过一个叫做 MMU（内存管理单元）的硬件实现的，当然这里还少不了操作系统的配合。

从“虚拟地址”到“物理地址”，计算机硬件与操作系统的配合为在操作系统上运行的各种程序提供了“智能”、“安全”、“高效”的运行环境，好处多多。比如程序通过这种假设，统一了虚拟的内存布局，从程序开发层面屏蔽了内存规划的复杂性，运行环境的差异性等，程序只需要关系自己的逻辑，内存布局的事情交给操作系统来负责。另一方面，每个程序运行在各自的内存空间上，彼此处于相互隔离的状态，程序之间无法操作自己内存空间以外的内存，这也增加了程序运行的安全性。



## 实模式与保护模式

（在“实模式”下没有“虚拟地址”到“物理地址”的转换，“虚拟地址”就相当于是“物理地址”，而想要这些特性就需要对应的把计算机的运行环境切换到“保护模式”下。）

就像之前我们讲到的 A20 gate 从 1MB 的内存寻址模式切换到更大的寻址能力一样。x86 架构的计算机为了向下兼容，开机的时候不仅运行在 1MB 内存寻址环境下，这时候也是运行在“实模式”环境下的。同样有一个开关控制着从“实模式”到“保护模式”的切换，这个开关叫“控制寄存器”。



## GDT

全局描述符表GDT（Global Descriptor Table）

一个处理器对应一个GDT，GDT可以被放在内存的任何位置，Intel的设计者门提供了一个寄存器GDTR用来存放GDT的入口地址，程序员将GDT设定在内存中某个位置之后，可以通过LGDT指令将GDT的入口地址装入此寄存器，从此以后，CPU就根据此寄存器中的内容作为GDT的入口来访问GDT了。GDTR中存放的是GDT在内存中的基地址和其表长界限

基地址指定GDT表中字节0在线性地址空间中的地址，表长度指明GDT表的字节长度值。指令LGDT和SGDT分别用于加载和保存GDTR寄存器的内容。在机器刚加电或处理器复位后，基地址被默认地设置为0，而长度值被设置成0xFFFF。在保护模式初始化过程中必须给GDTR加载一个新值

#### LDT

LDT和GDT从本质上说是相同的，只是LDT嵌套在GDT之中。LDTR记录局部描述符表的起始位置，与GDTR不同，LDTR的内容是一个段选择子。由于LDT本身同样是一段内存，也是一个段，所以它也有个描述符描述它，这个描述符就存储在GDT中，对应这个表述符也会有一个选择子，LDTR装载的就是这样一个选择子。LDTR可以在程序中随时改变，通过使用lldt指令。

由于每个进程都有自己的一套程序段、数据段、堆栈段，有了局部描述符表则可以将每个进程的程序段、数据段、堆栈段封装在一起，只要改变LDTR就可以实现对不同进程的段进行访问。

当进行任务切换时，处理器会把新任务LDT的段选择符和段描述符自动地加载进LDTR中。在机器加电或处理器复位后，段选择符和基地址被默认地设置为0，而段长度被设置成0xFFFF。

### 访问过程



保护模式下的段寄存器 由 16位的选择器 与 64位的段描述符寄存器 构成
段描述符寄存器： 存储段描述符
选择器：存储段选择子

段选择子包括三部分：描述符索引（index）、TI、请求特权级（RPL）

- index（描述符索引）部分表示所需要的段的描述符在描述符表的位置，由这个位置再根据在GDTR中存储的描述符表基址就可以找到相应的描述符。
- 然后用描述符表中的段基址加上逻辑地址（SEL:OFFSET）的OFFSET就可以转换成线性地址，段选择子中的TI值只有一位0或1，0代表选择子是在GDT选择，1代表选择子是在LDT选择。请求特权级（RPL）则代表选择子的特权级，共有4个特权级（0级、1级、2级、3级）

当TI=0时表示段描述符在GDT中

①先从GDTR寄存器中获得GDT基址。

②然后再GDT中以段选择器高13位位置索引值得到段描述符。

③段描述符符包含段的基址、限长、优先级等各种属性，这就得到了段的起始地址（基址），再以基址加上偏移地址才得到最后的线性地址。



当TI=1时表示段描述符在LDT中

①还是先从GDTR寄存器中获得GDT基址。

②从LDTR寄存器中获取LDT所在段的位置索引(LDTR高13位)。

③以这个位置索引在GDT中得到LDT段描述符从而得到LDT段基址。

④用段选择器高13位位置索引值从LDT段中得到段描述符。

⑤段描述符符包含段的基址、限长、优先级等各种属性，这就得到了段的起始地址（基址），再以基址加上偏移地址才得到最后的线性地址。



## xv6 准备 GDT

见收藏“【学习xv6】从实模式到保护模式 - leenjewel Blog”

## 两个16位表示20位，为什么不用4+16

16+16法：物理地址（physicaladdress）=段值（segment） * 16 + 偏移（offset）

只能表示20位的（因为中间重合的12位，在不同段会有相同的地址出现），而且要求段基质最后4位是0

为什么不直接用映射比较简洁的4+16法？为什么用16+16，很容易有冲突，冲突怎么解决？两个不同的段中找到同一个物理地址，这就不能自如的使用段的虚拟空间，难道最初这个不是为了方便的使用虚拟空间的？只是为了表示20位？而16+16法比16+4法更高效？为什么？

### 相关资料：

真实的段地址有以下的缺点：

1、一个段地址只能指向64K内存(16位偏移的上限)。如果一个程序拥有大于64K的代码那又怎么办呢？在CS里的一个单一的值不能满足整个程序执行的需要。程序必须分成小于64K的段(segment)。当执行从一段移到另一段时，CS里的值必须改变。同样的问题发生在大量的数据和DS 寄存器之间。这样使用是非常不方便的！

2、每个字节在内存里并不只有唯一的段地址。物理地址04808可以表示为：047C:0048，047D:0038，047E:0028 或047B:0058。这将使段地址的比较变得复杂。

## 相对地址访问

“我们编写程序代码，编译器将我们的程序代码变成 CPU 可以理解的指令（二进制可执行程序）。在运行我们写的程序前，需要将程序先加载进内存，而我们的程序应该加载到内存的什么位置这应该是由操作系统来负责的，我们程序本身是不能决定这一切的。

这里就产生了一个“矛盾”。在程序真正运行前我们是不知道我们会被放在内存的什么地方，但是我们的程序本身还有数据，代码也含有对数据的访问（例如我们的代码中使用的各种变量），我们不知道我们的数据会被操作系统放在哪，但我们还要在代码里写访问这些数据的逻辑，这是一个矛盾，要怎么办？”

在c中我们只需要用标示符就好了，是汇编要知道每次引用的数据的地址吧？

## 键盘控制器端口

“如果键盘控制器输出端口（？？？为什么这个方法？这个位置没有其他用处吗？）的第2位是低位，则物理地址的第21位被清零；否则，第21位可以正常使用”

控制 A20 gate 的方法有 3 种：

* 804x 键盘控制器法
* Fast A20 法
* BIOS 中断法

xv6 用了第一种 804x 键盘控制器法，这也是最古老且效率最慢的一种。当然因为硬件的不同，这三种方法可能不会被硬件都支持，正确的做法应该是这三种都尝试一下，每尝试一个就验证一下 A20 gate 是否被正确打开以保证兼容各种硬件。但是 xv6 作为一款教学用的操作系统就没必要做的这么复杂里。只用了一种最古老的方法（保证兼容大多数硬件）而且没有对打开成功与否做验证。像诸如 Linux 这样的操作系统就把三种方法的实现都做好里，并且加上了验证机制。



## 跳转  jmp

jmp指令是无条件的跳转指令，jmp分为3种跳转模式，一种是短转移，一种是近转移，最后是远转移。首先介绍一下前两种跳转：

jmp short 标号               ；短转移，程序跳转到标号标注的地方，和C语言中的goto指令类似

jmp near prt 标号           ；近转移，和短转移类似

这两种跳转指令是不需要加上跳转的地址的，他们属于段内跳转指令，也就是说，他们只能改变IP的指向，而不改变CS的指向，他们跳转依靠的是位移，这是编译器在编译的时候处理的事情，编译器会将你的标号地址减去jmp指令的地址，计算出之间位移，然后在翻译成机器语言。位移为正就是向下跳转，位移为负就是向上跳转。short和near prt之间的差别就是short跳转的极限是2^7（8位位移）， near的极限是2^15（16位位移）。

jmp far prt 标号             ；远转移，和短转移意思类似，只是细节不同

远转移，又称为段间转移，它不仅仅会修改IP指向，还会修改CS的指向，这个跳转的依据就是实实在在的地址。



## 物理地址

物理内存是指 DRAM 中的储存单元。每个字节的物理内存都有一个地址，称为物理地址

物理地址出现在CPU外部地址总线上，被发送给 DRAM 硬件以读写存储器

## 内存

我们平常所提到的计算机的内存指的是动态内存（即DRAM）

* 动态内存中所谓的"动态"，指的是当我们将数据写入DRAM后，经过一段时间，数据会丢失，因此需要一个额外设电路进行内存刷新操作
* 具体的工作过程是这样的：一个DRAM的存储单元存储的是0还是1取决于电容是否有电荷，有电荷代表1，无电荷代表0。但时间一长，代表1的电容会放电，代表0的电容会吸收电荷，这就是数据丢失的原因；刷新操作定期对电容进行检查，若电量大于满电量的1／2，则认为其代表1，并把电容充满电；若电量小于1／2，则认为其代表0，并把电容放电，藉此来保持数据的连续性 。

ROM和RAM指的都是半导体存储器，ROM是Read Only Memory的缩写，RAM是Random Access Memory的缩写。

ROM在系统停止供电的时候仍然可以保持数据，而RAM通常都是在掉电之后就丢失数据，典型的RAM就是计算机的内存。 

RAM主要分为两类，一类是静态RAM（static RAM/SRAM）。另一类是动态RAM（Dynamic RAM/DRAM）。

**RAM**

SRAM速度非常快，是目前读写最快的存储设备了，但是它也非常昂贵，所以只在要求很苛刻的地方使用，譬如CPU的一级缓冲，二级缓冲。利用寄存器来存储信息，所以一旦掉电，资料就会全部丢失，只要供电，它的资料就会一直存在，不需要动态刷新，所以叫静态随机存储器。

DRAM保留数据的时间很短，速度也比SRAM慢，不过它还是比任何的ROM都要快，但从价格上来说DRAM相比SRAM要便宜很多，计算机内存就是DRAM的。 

### 内存大小

那么现在总结下 IA-32e 模式下，Ubuntu 64 位系统，从小到大的各种内存限制：

1. 应该是主板/内存的限制，我的主板是华硕B85M-V5 PLUS，只有两个DDR3的插槽，目前插了一个8GB，好像16GB的 DDR3 很稀有，16GB 的 DDR4 倒是进入了可接受的价位，但主板不支持，所以最多再加一条 8GB，组成16GB内存。
2. 假设主板/内存限制突破了，下一个坎是 CPU的物理地址宽度限制，512GB，如果到时候这个 CPU 还能在当时的硬件上用的话(估计是不可能的)。
3. 按照文档的描述，现在难道存在物理地址是52位的 CPU？如果有这样的 CPU，而且以上的限制都不需要考虑的话，下一个坎就到了 Linux 内核这里了，最大可操作64TB的物理内存(可以想象一下：一个特制的主板上插着8192条8GB内存是一幅多么震撼的画面)。
4. 再往上就得改内核了，而实际上64TB内存都有了的时候，CPU应该也支持真64位了，而且真的需要这么大的内存么，说不定到时候计算机的结构都不一样了。

因此 x86_64 平台上的 Linux 理论上最多支持64TB的物理内存，单个进程理论上可用128TB的地址空间(高128TB被操作系统占用了)，但受限于物理内存的限制，其中最多有64TB是物理内存，其余的理论上还可以用虚拟内存(swap分区)顶包。

## PC主板

![31162602-cb2675a3eb5b4b199adc9c3eefcaa167](/Users/caowanlu/Desktop/md%E7%AC%94%E8%AE%B0%E4%B8%AD%E7%94%A8%E5%88%B0%E7%9A%84%E5%9B%BE/31162602-cb2675a3eb5b4b199adc9c3eefcaa167.jpg)

事实上这也是现代主板所采用的结构，当然可能部分地方有略微不同（大体结构是差不多的），仔细观察过主板构成的朋友可能对上面一幅图很熟悉。在主板上主要有两大主要部分：北桥（North Bridge也称Host Bridge）和南桥（South Bridge）。北桥主要负责CPU和内存、显卡这些部件的数据传送，而南桥主要负责I/O设备、外部存储设备以及BIOS之间的通信。现在有些主板已经没有北桥了，因为芯片厂商已经把北桥所负责的功能直接集成到CPU中了（不过暂且我们以上副图的模型来讨论）。

　　在上副图中，我没有画出 数据总线和地址总线等，因为在某些总线标准中它们被集成到一起了，比如在PCI总线中，地址总线和数据总线总是分时复用的（也就是说假如PCI总线有32位数据总线，这32位总线在某个时刻可以充当数据总线的作用，在下一时刻可以充当地址总线的作用）。有的总线同时提供了数据总线和地址总线。

　　下面来说一下几个主要总线和南北桥的作用：

　　FSB总线：即前端总线（Front Side Bus），CPU和北桥之间的桥梁，CPU和北桥传递的所有数据必须经过FSB总线，可以这么说FSB总线的频率直接影响到CPU访问内存的速度。

　　北桥：北桥是CPU和内存、显卡等部件进行数据交换的唯一桥梁，也就是说CPU想和其他任何部分通信必须经过北桥。北桥芯片中通常集成的还有内存控制器等，用来控制与内存的通信。现在的主板上已经看不到北桥了，它的功能已经被集成到CPU当中了。

　　PCI总线：PCI总线是一种高性能局部总线，其不受CPU限制，构成了CPU和外设之间的高速通道。比如现在的显卡一般都是用的PCI插槽，PCI总线传输速度快，能够很好地让显卡和CPU进行数据交换。

　　南桥：主要负责I/O设备之间的通信，CPU要想访问外设必须经过南桥芯片。

　　在了解了这些基础东西之后，下面来讲解一下为何32位系统最大只支持4GB内存。

## 变量类型转换

强制转换，其实也分不同的类型。一个强制转换语句，在不同的场合下，有不同的含义。大体上来看，在 C 语言里面，强制类型转换有两种含义：

1，将一个数据，复制为另外一个数据类型，并且使用新类型的数据，此时可以认为新类型的数据，在内存结构方面与原有数据没有任何关系，这是实实在在的发生了转换，而且执行这个转换相关的代码是编译器生成的。例如：



> char c = 'c';
> int i = (int) c;

在上面的例子中，字符 c 被转化成了 int，两者是完全不同的数据，其内存占用的长度也完全不同。
2，将一个数据，当作另外一个数据类型使用，让编译器认为这就是另外一个数据类型，此时数据没有发生任何转换，在计算机的角度没有任何事情发生，也不为这个转换生成任何代码，只是编译器认为这个语句合法了。例如：



> long l = 0x10000000L;
> volatile void *p = (void *) l;

在这句话中，变量 l 被『直接视为』指针 p，从内存数据的角度没有发生任何转换，这个转换只是让编译器认为这个赋值合法而已。
在 C++ 里面，通常第二种类型被称为 reinterpret_cast，而第一种类型就直接使用显式类型转换实现。

#### 精度丢失

比如，由于char只占据一个字节，导致int的多余三个无法扣上去，这就会被内存管理器给舍弃，这就是精度丢失的原因。所以吧int型转换成char型，编译器会警告精度丢失。





## 虚拟机磁盘扩容

sudo su

su

　#df -h（查看分区情况及数据盘名称）

开始分区 
输入命令`$sudo fdisk /dev/sda` 

n  p

格式化刚才划好的分区/dev/sda4

> sudo mkfs -t ext4 /dev/sda4



## jdk

JDK是 [Java](https://baike.baidu.com/item/Java/85979) 语言的[软件开发工具包](https://baike.baidu.com/item/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%E5%8C%85/10418833)，主要用于移动设备、嵌入式设备上的java应用程序。JDK是整个java开发的核心，它包含了JAVA的运行环境（JVM+Java系统类库）和JAVA工具。



## eclipse初识

Eclipse 是一个开放[源代码](https://baike.baidu.com/item/%E6%BA%90%E4%BB%A3%E7%A0%81/3969)的、基于[Java](https://baike.baidu.com/item/Java/85979)的可扩展开发平台。就其本身而言，它只是一个框架和一组服务，用于通过插件组件构建开发环境

现在又直接的eclipse for c／c++了，可以直接下载安装（需要jdk提供java环境）

### 使用

有不同的模式

build和run，直接不行就开run 配置

有debug模式，里面可以用gdb命令

就是一个ide



## Homebrew／port 使用错误

1、Error: Running Homebrew as root is extremely dangerous and no longer supported.
 As Homebrew does not drop privileges on installation you would be giving all
 build scripts full access to your system.

把/use/local的owner換成自己，就有write權限了

sudo chown -R jack /usr/local

 brew uninstall wget（不要再加sudo了）

2.

提示 

 command not found: port

其实是未设置环境变量

执行下面的语句

```bash
export PATH=/opt/local/bin:/opt/local/sbin:$PATH
```

 sudo port install i386-elf-gcc



## makefile





## mac安装gdb

安装已完毕，用的prot

建一个证书

授证书  sudo codesign /opt/local/bin/ggdb -s gdb-cert（签名证书）

启动 gdb 之后执行`set startup-with-shell off` 

（当然，如果这样，你每次在调试的时候都要键入，这时候你就可以添加到.gdbinit，每次运行gdb的时候都执行一次。 
关于.gdbinit，就是在你每次用gdb调整程序的时候，如果程序文件中有.gdbinit，就会自动读取里面的命令，全部执行一遍。 
于是乎，我们只要用vim在程序文件里面创建一个.gdbinit，并在里面写上`set startyup-with-shell off` 
就解决了。 ）



## mac上运行xv6



make终于成功：

在make file里开了cross编译，他们后面自己转

然后下载了i138-elf-gcc，把工具，i138-jos-elf换成了i138-elf-（它后面自己加gcc）

不行的话就每次都要重装一下i138

然后就可以啦



2018/9/3   gdb+xv6初次失败

make qemu-gdb之后，用gdb加载kernel总是报错：有指令不能识别；kernel不是可执行文件，符号加载不进来。但是可以连接端口

感觉可能是编译器生成的文件gdb解读不了（可能是make文件中使用的gcc和gdb版本不合（看makefile好像在mac上一直用的交叉编译i386-elf-gcc什么的，可能是生成的文件，或者mac中间使用了自己的编译什么的，gdb识别不了））

在linux上直接用的gcc，没有这个问题

我不想再对着版本在mac上装gcc，gdb什么了：之前用安装包装了一次没有成功，现在是用port和homebrew装好的，也不清楚怎么调版本







## mac终端

终端切换用户到root  sudo -i

切换成普通用户  su - caowanlu

反汇编（待续）

## 机器指令，二进制文件，不同操作系统

机器指令是针对 CPU 而言，显然都是 x86，那么机器码是一样的。

* 现代编译器绝大多数后端（生成机器代码的部分）有能力生成多种体系结构的代码。gcc本身支持多种体系结构包括ARM，只是默认情况编译为本机架构。**编译器生成代码可以与本机架构无关。**你可以在x86上把代码编译成ARM的二进制，也可以在ARM上把代码编译成x86的二进制。

二进制文件是否相同：

* 应用程序有组织的格式，具体到windows就是PE，linux 一般是 ELF，这是组织的方式不一样（Windows的.exe和Linux的.out）
* 应用程序与操作系统的接口是系统调用，linux 和 windows 的系统调用是不一样的，导致了程序的主体内容也不一样。其他应用程序中纯逻辑部分编译出来可能有部分机器码片段是相同的。
* #include <stdlib.h>
  那么stdlib是什么，为什么只有stdlib.h而没有stdlib.c呢
  因为stdlib是一个平台特定的链接库，linux下常用glibc，而windows下这玩意叫做vc runtime，所以不同的操作系统下编译出的二进制有区别

自己用objdump反汇编看看呗

## extra

操作系统中涉及到I/O指令的必须用汇编完成。汇编文件数量较大（虽然仍不占5%），编写复杂，因此在ARM市场没有Windows XP的需求时微软不会为其重写这5%

另：（1）Windows很早就有ARM版。参见Windows CE、Windows RT、Windows Mobile和Windows Phone

（2）XP之所以没有ARM版是因为存在Windows CE且大部分当时的PDA和手机（当时仅有运行Windows的ARM版的设备）内存尚不够运行XP。

## smp

SMP的全称是"[对称多处理](https://baike.baidu.com/item/%E5%AF%B9%E7%A7%B0%E5%A4%9A%E5%A4%84%E7%90%86/6274908)"（Symmetrical Multi-Processing）技术，是指在一个计算机上汇集了一组处理器(多CPU),各CPU之间[共享内存](https://baike.baidu.com/item/%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98/2182364)子系统以及[总线结构](https://baike.baidu.com/item/%E6%80%BB%E7%BA%BF%E7%BB%93%E6%9E%84/10183496)。

mp

在20行将调用函数mp_init将获取所有cpu的信息，其中bcpu将指定BOOTSTRAP CPU的编号（即第一个启动的CPU的编号）。

## apic

apic:Advanced Programmable Interrupt Controller高级可编程中断控制器。

APIC 是装置的扩充组合用来驱动 Interrupt 控制器。在目前的建置中，系统的每一个部份都是经由 APIC Bus 连接的。"本机 APIC" 为系统的一部份，负责传递 Interrupt 至指定的处理器；举例来说，当一台机器上有三个处理器则它必须相对的要有三个本机 APIC。自 1994 年的 Pentium P54c 开始Intel 已经将本机 APIC 建置在它们的处理器中。实际建置了 Intel 处理器的电脑就已经包含了 APIC 系统的部份。

CPU内部必须内置APIC单元。Intel多处理规范的核心就是APIC的使用。CPU通过彼此发送中断来完成它们之间的通信。通过给中断附加动作(actions)，不同的CPU可以在某种程度上彼此进行控制。每个CPU有自己的APIC(成为那个CPU的本地APIC)，并且还有一个I/O APIC来处理由I/O设备引起的中断，这个I/O APIC是安装在主板上的，但每个CPU上的APIC则不可或缺，否则将无法处理多CPU之间的中断协调。

一条APIC总线把"前端"I/O APIC连接到本地APIC。来自外部设备的IRQ线连接到I/O APIC，因此，相对于本地APIC，I/O APIC起到路由器的作用。

## 同步通信和异步通信



同步是指：发送方发出数据后，等接收方发回响应以后才发下一个数据包的通讯方式。
异步是指：发送方发出数据后，不等接收方发回响应，接着发送下个数据包的通讯方式。

同步是阻塞模式，异步是非阻塞模式。

其中SPI IIC为同步通信  UART为异步通信

在串行通信中，由于是一位一位地进行数据传送。为了把每个字节区别开来，需要收发双方在传送数据的串行信息流中，加入一些标记信号位。

在数据中根据所添加的标记信号位的不同方式，分成同步通信和异步通信两种。 

“异步通信”是一种很常用的通信方式（效率较低，因为开始位和停止位的开销所占比例较大）异步通信在发送字符时，发送端可以在任意时刻开始发送字符，因此必须在每一个字符的开始和结束的地方加上标志，即加上开始位和停止位，以便使接收端能够正确地将每一个字符接收下来。所传送的数据以字节为单位。每个字节前加上一位起始位，每个字节的后面加上停止位。好处：异步通信的好处是通信设备简单、便宜，但传输效率较低。 

“同步通信”的通信双方必须先建立同步，即双方的时钟要调整到同一个频率。收发双方不停地发送和接收连续的同步比特流。一种是使用全网同步，用一个非常精确的主时钟对全网所有结点上的时钟进行同步。一种是使用准同步，各结点的时钟之间允许有微小的误差，然后采用其他措施实现同步传输。同步通信是把所传送的数据以多个字节（100字节以上）为单位，在其前后添加标志。

同步通信可用于点对多点；异步通信只适用于点对点。

## 串行通信和并行通信

**1  串行通讯**

       一条信息的各位数据被逐位按顺序传送的通讯方式称为串行通讯。串行通讯的特点是：数据位传送，传按位顺序进行，最少只需一根传输线即可完成，成本低但送速度慢。串行通讯的距离可以从几米到几千米。 根据信息的传送方向，串行通讯可以进一步分为单工、半双工和全双工三种。信息只能单向传送为单工；信息能双向传送但不能同时双向传送称为半双工；信息能够同时双向传送则称为全双工。 串行通讯又分为异步通讯和同步通讯两种方式。在单片机中，主要使用异步通讯方式。

串行通讯中，两个设备之间通过一对信号线进行通讯，其中一根为信号线，另外一根为信号地线，信号电流通过信号线到达目标设备，再经过信号地线返回，构成一个信号回路。

初级读者会产生疑问：为何不让信号电流从[电源](http://www.8080.net/key_search.asp?key=74)地线返回？答案：公共地线上存在各种杂乱的电流，可以轻而易举地把信号淹没。因此所有的信号线都使用信号地线而不是电源地线，以避免干扰。

这一对信号线每次只传送1bit（比特）的信号，比如1Byte（字节）的信号需要8次才能发完。传输的信号可以是数据、指令或者控制信号，这取决于采用的是何种通讯协议以及传输状态。串行信号本身也可以带有时钟信息，并且可以通过算法校正时钟。因此不需要额外的时钟信号进行控制。

**2  并行通讯**

        并行通讯中，基本原理与串行通讯没有区别。只不过使用了成倍的信号线路，从而一次可以传送更多bit的信号。

并行通讯通常可以一次传送8bit、16bit、32bit甚至更高的位数，相应地就需要8根、16根、32根信号线，同时需要加入更多的信号地线。比如传统的PATA线路有40根线，其中有16根信号线和7根信号地线，其他为各种控制线，一次可以传送2Byte的数据。并行通讯中，数据信号中无法携带时钟信息，为了保证各对信号线上的信号时序一致，并行设备需要严格同步时钟信号，或者采用额外的时钟信号线。

通过串行通讯与并行通讯的对比，可以看出：串行通讯很简单，但是相对速度低；并行通讯比较复杂，但是相对速度高。更重要的是，串行线路仅使用一对信号线，线路成本低并且抗干扰能力强，因此可以用在长距离通讯上；而并行线路使用多对信号线（还不包括额外的控制线路），线路成本高并且抗干扰能力差，因此对通讯距离有非常严格的限制。

串行通信中数据**按位传输**，即一次传输一位；

并行传输中数据**按字节传输**，即一次传输8位。

**优缺点：**并行速度快，但造价高，内部的多根线缆同步较困 难，相互之间易产生干扰，在远距离通信中多用串 行通信，计算机内部大多使用并行通信。

使用串行通信的接口是串行接口，使用并行通信的接口是并行接口。

## uart

通用异步收发传输器（Universal Asynchronous Receiver/Transmitter)，通常称作UART，是一种异步收发传输器，是电脑硬件的一部分。它将要传输的资料在串行通信与并行通信之间加以转换。作为把并行输入信号转成串行输出信号的芯片，UART通常被集成于其他通讯接口的连结上。

具体实物表现为独立的模块化芯片，或作为集成于微处理器中的周边设备。一般是RS-232C规格的，与类似Maxim的MAX232之类的标准信号幅度变换芯片进行搭配，作为连接外部设备的接口。

UART是一种通用串行数据总线，用于异步通信。该总线双向通信，可以实现全双工传输和接收。在嵌入式设计中，UART用于主机与辅助设备通信，如汽车音响与外接AP之间的通信，与PC机通信包括与监控调试器和其它器件，如EEPROM通信

计算机内部采用并行数据，不能直接把数据发到Modem，必须经过UART整理才能进行异步传输，其过程为：CPU先把准备写入串行设备的数据放到UART的寄存器（临时内存块）中，再通过FIFO（First Input First Output，先入先出队列）传送到串行设备，若是没有FIFO，信息将变得杂乱无章，不可能传送到Modem。

## 缓冲区 缓存

1、Buffer（缓冲区）是系统两端处理速度平衡（从长时间尺度上看）时使用的。它的引入是为了减小短期内突发I/O的影响，起到流量整形的作用。比如生产者——消费者问题，他们产生和消耗资源的速度大体接近，加一个buffer可以抵消掉资源刚产生/消耗时的突然变化。
2，CPU缓存（Cache Memory）是位于CPU与内存之间的临时存储器，它的容量比内存小的多但是交换速度却比内存要快得多。

按照数据读取顺序和与CPU结合的紧密程度，CPU缓存可以分为一级缓存，二级缓存，部分高端CPU还具有三级缓存，每一级缓存中所储存的全部数据都是下一级缓存的一部分，这三种缓存的技术难度和制造成本是相对递减的，所以其容量也是相对递增的。当CPU要读取一个数据时，首先从一级缓存中查找，如果没有找到再从二级缓存中查找，如果还是没有就从三级缓存或内存中查找。一般来说，每级缓存的命中率大概都在80%左右，也就是说全部数据量的80%都可以在一级缓存中找到，只剩下20%的总数据量才需要从二级缓存、三级缓存或内存中读取，由此可见一级缓存是整个CPU缓存架构中最为重要的部分。

缓存系统两端处理速度不匹配时的一种折衷策略。因为CPU和memory之间的速度差异越来越大，所以人们充分利用数据的局部性（locality）特征，通过使用存储系统分级（memory hierarchy）的策略来减小这种差异带来的影响。

3、假定以后存储器访问变得跟CPU做计算一样快，cache就可以消失，但是buffer依然存在。比如从网络上下载东西，瞬时速率可能会有较大变化，但从长期来看却是稳定的，这样就能通过引入一个buffer使得OS接收数据的速率更稳定，进一步减少对磁盘的伤害。
4、TLB（Translation Lookaside Buffer，翻译后备缓冲器）名字起错了，其实它是一个cache.



## 9.3 mac启动小总结

cli

段寄存器置零（ds，es，ss）（cpu刚启动的时候是实模式），开两个端口开16位以外的位

加载全局描述符表，调cr0开保护模式

ljmp，用一个段选择子更新段寄存器（实际取过来的段基址还是0）

然后用其他的段选择子（定义好的常量）更新其他段寄存器

 0x7c00放入esp，call bootmain

从0x10000（磁盘的第一个扇区）处读内核文件。读完后entry进入

调cr4，分一个4M页表（汇编entrypgdir负责映射），放入cr3，调cr0，开分页

开内核栈（分配空间，调esp），mov main eax，jmp eax

建一个初步freelist（用kfree；4M）

换新的页表

mp，lapic，gdt的段描述符（每个cpu启动都要设一次），pic，ioapic，console（锁，结构），uart，ptable（锁），trap vector，buffer，cache，filetable（文件锁），disk，startothers（一个cpu一个cpu的加载entryothers，分配栈，启动）

把剩下的内存也加进freelist

构建第一个进程：

ptable中找一个空结构，设置状态，分配空间，然后构建内核栈框架（分地址给结构表相应位置）（tf，trapret，context，其中eip是forkret），然后返回pid

构建内核部分页表目录，地址放结构里

加载initcode到内存（页表中分一页，映射到虚拟地址0，然后把initcode放进去）

设置tf：cs，ds，es，ss，（用户态中要用到的值，在trapret中的指令下弹出，再加上eip，就可以把cpu切换到新进程的上下文了，吧）esp，eip（刚好放了initcode）

进程状态修改为runnable

（所以抽到进程后，会怎样？）

加载idt

将boot cpu的started设为1，然后进入scheduler函数

sti（允许中断），找一个runnable p

cpu当前进程设置为p，转换当前进程的tss和页表，p设置为running

swtch：切换scheduler和p的上下文，（先在栈中分好地址，旧的一会进来，新的现在就在（p结构的context），一会弹出去，然后就接着新的进行了）

p的con初始化放的0啊，就eip放的forkret

然后进forkret，



## int iret



IRET(interrupt return)中断返回，中断服务程序的最后一条指令。IRET指令将推入堆栈的段地址和偏移地址弹出，使程序返回到原来发生中断的地方。其作用是从中断中恢复中断前的状态，具体作用有如下三点：

1.恢复[IP](https://baike.baidu.com/item/IP)(instruction pointer)：（IP）←（（SP）+1:（SP）），（SP）←（SP）+2

2.恢复[CS](https://baike.baidu.com/item/CS)(code segment)：（CS）←（（SP）+1:（SP）），（SP）←（SP）+2

3.恢复中断前的[PSW](https://baike.baidu.com/item/PSW)(program status word),即恢复中断前的[标志寄存器](https://baike.baidu.com/item/%E6%A0%87%E5%BF%97%E5%AF%84%E5%AD%98%E5%99%A8)的状态。

（FR）←（（SP）+1:（SP）），（SP）←（SP）+2

4.恢复ESP（返回权限发生变化）

5.恢复SS（返回权限发生变化）

以上操作按顺序进行。

### int

中断信息可以来自CPU的内部和外部，当CPU的内部有需要处理的事情发生的时候，将产生需要马上处理的中断信息，引发中断过程。

int指令的最终功能和call指令相似，都是调用一段程序。

 一般情况下，系统将一些具有一定功能的子程序，以中断处理程序的方式提供给应用程序调用。我们在编程的时候，可以用int指令调用这些子程序。当然，也可以自己编写一些中断处理程序供别人使用。

中断处理程序可简称为中断例程。

格式：int n

n为中断类型码，它的功能是引发中断过程。

 CPU执行int n指令，相当于引发一个n号中断的中断过程，执行过程如下：

1）取中断类型码n；

2）标志寄存器入栈，IF=0，TF=0；

3）CS、IP入栈

4）CPU从中断向量表中，找到第n号表项（是个门），跳转

    (IP)=(n*4)，(CS)=(n*4+2)

从此处转去执行n号中断的中断处理程序。

（对8086PC，中断向量表指定放在内存地址0处（地址固定），共1024个字节。每个表项占两个字，低字存放偏移地址，高字存放段地址。）

可以在程序中使用int指令调用任何一个中断的中断处理程序。



## 函数指针数组

概念： 
数组元素是指针函数的数组叫做指针函数数组，通常我们也叫做转移表 
定义个初始化： 
返回类型说明符 (*函数指针数组名[])(参数列表) = {函数指针/函数名，…}; 
如下： int (*fun_array[])(int,int) = {add,del,mul,div};

函数指针数组的使用如下： 
函数指针数组名[下标](https://blog.csdn.net/qq_29924041/article/details/%E5%AE%9E%E5%8F%82%E5%88%97%E8%A1%A8)或者(*函数指针数组名)[下标](https://blog.csdn.net/qq_29924041/article/details/%E5%AE%9E%E5%8F%82%E5%88%97%E8%A1%A8)； 
下面是给出的示例代码：

~~~
int add(int a,int b); 
int sub(int a,int b); 
int mul(int a,int b); 
int div(int a,int b); 
void make_menu();


int main(int argc,char* argv[]){
    int (*fun_array [])(int,int) = {add,sub,mul,div};
	......
	result = fun_array[cmd-1](num1,num2);
~~~

## 数组初始化可以把索引放进内部

```
  int a[]={[0] 2,[1] 4,[2] 6};
```



## push pushl

当要压栈的对象已经确定（也就是说已经知道是字节、字或者双字），那么使用push就不会产生歧义，也就是说汇编器可以自己判断自己要操作的是什么长度的操作对象；但是当汇编器不能自己判断操作对象长度时，就需要使用pushl之类的指令来指明操作对象长度。

## argc argv

main(int argc, char *argv[ ], char **env)才是[UNIX](https://baike.baidu.com/item/UNIX)和[Linux](https://baike.baidu.com/item/Linux)中的标准写法。

argc: 整数,用来统计你运行程序时送给[main函数](https://baike.baidu.com/item/main%E5%87%BD%E6%95%B0)的[命令行参数](https://baike.baidu.com/item/%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0)的个数

argv[ ]: 指针[数组](https://baike.baidu.com/item/%E6%95%B0%E7%BB%84)，用来存放指向你的字符串参数的指针，每一个元素指向一个参数

argv[0] 指向程序运行的全路径名

argv[1] 指向在DOS命令行中执行程序名后的第一个字符串

argv[2] 指向执行程序名后的第二个字符串

...

argv[argc]为NULL。

## ebp esp

有一个同名网页书签，在笔记文件夹里，介绍的很好

## 9.6

好多地方保存的不是实体，只是一个地址

cpu的proc，scheduler每次调用进程时换，保存的是地址，可以随时修改当前进程的proc，主要是tf，控制返回用户空间后的环境）

子进程fork之后就runnable了，被scheduler调用（内核栈，proc，页表）之后，才会进到分支里exec

if(pid == 0){   //子进程中的分支，进到这个分支，说明scheduler转换了子进程，这时候cpu的proc就是子进程的proc地址了，而后才开始掉用exec

每个进程都重新分配内核栈，地址不同

遇到fork，特权级不够，中断，启动父进程内核栈（到内核态），tf，执行fork，返回

遇到exec的时候也是这样

PC 实际上就是 CS:IP 组合的逻辑表示。PC 不是一个实体，真正用来表示 PC 值的是 CS:IP，在 x86 体系里是这样。x86 系统中自增的是 IP，用 CS:IP 组合表示正在执行的指令地址，此时 PC 只是一个概念上的说法。在 ARM 体系中 R15 就是 PC，



## 内存部分的思考

PTE结构

kernbase只是为了保护内核，只有k之上的va映射到内核，而用户空间不能访问k之上的地址。

有必要分2G那么大的空间吗？linux的4G是3:1分的，32位的只有4G这么大的寻址空间

是不是不只有内核？xv6的内核内存空间结构？：先是内核文件，最后是高地址（FE,000,000到4G），本来物理空间的高地址就是设备文件，虚拟和物理的最后部分直接映射一下就好了

中间呢，还有其他用处吗？kernel.ld里定义的data是什么意思啊，没看懂

也可以把内核封在低地址，让用户空间从零开始生长是为了程序中使用方便吧

通过多个cpu访问同一个freelist，一般情况下内存访问不会出现冲突

系统调用之类的内核程序，在kernbase之上有自己的内存空间，通过内核页表可以访问需要的数据和代码

可以多个cpu同时执行内核程序吗？可以的吧，只是去那里调几个数据，堆栈用的都是自己的内核栈，应该可以



## 关于内核和操作系统的关系（linux大小）

"Linux内核编译出来后bzImage大小只有1.5M左右,但怎么一般的Linux系统那么庞大呢?"
自己编译的驱动和模块自己定制，而发布版包括所有你要的和不要的

“平时用的Linux除了内核以外还有什么东西才造成LINUX系统这么大呢?”
基本的大家伙：xwindow(xorg/XFree86)100M左右。
　　　　　　　gtk:100M左右
　　　　　　　gnome:300M左右
​              kde:200M左右
　　　　　　　openoffce:500Ｍ左右
　　　　　　　glib+gcc+system v基本工具:差不多100Ｍ多
　　　　　　　应用有关的开发工具都比较大
以上基于源码编译不是很准确的数字

“是不是如果我只要一个简单的LINUX的话,我就编译一个内核就可以了?是不是理论上安装一个LINUX系统只要几兆的磁盘空间就够了呢?”
不可以,因为内核没有应用，最少是kernel+glibc+bash(嵌入式:busybox).实际就可能，网上有做２M的，本人做过3-5M的系统。


上面"glib+gcc+system v基本工具:差不多100Ｍ多"
更正:glibc



## xv6有哪些可以优化或添加的功能？

当年操作系统大作业就是改进xv6
很多组加了图形界面，鼠标驱动，声卡之类的。
还有内存管理优化，进程调度优化（换一种进程调度方式提高效率），文件系统优化（复制粘贴剪切文件）等等。



虚拟内存、多线程、图形界面、文件系统优化、等等。
这就是你的玩具，很多地方只要你愿意花时间，总能做出自己想要的样子

## 磁盘驱动补充

用一个buf结构代表一个扇区（512）

`iderw` 根据标志位更新一个锁住的缓冲区

`iderw` 睡眠，等待中断处理程序在操作完成时更新缓冲区的标志位

其实硬盘和硬盘I/O接口(硬盘控制器)整合在一起了，也就是以前所说的IDE接口

硬中断是由硬件产生的，比如，像磁盘，网卡，键盘，时钟等。每个设备或设备集都有它自己的IRQ（中断请求）。基于IRQ，CPU可以将相应的请求分发到对应的硬件驱动上（注：硬件驱动通常是内核中的一个子程序，而不是一个独立的进程）。

一般来说操作系统会通过 memory mapped IO 技术把键盘、磁盘等硬件上的寄存器连接到 IO 总线，再通过 IO 控制器连接到内存总线，这样硬件上的寄存器也被映射到了一段内存地址上，CPU 可以直接通过读写内存的指令来读写硬件寄存器中的数据

磁盘的 IO 操作完成后，磁盘会触发一个中断(interrupt)，CPU 会暂时中止当前线程的执行，保存相关的寄存器信息后，调用对应的中断处理器(interrupt handler)，把读取到的内容从内核地址空间拷贝到进程 P 的地址空间里面，然后将进程 P的状态设置为 runnable, 进程 P 排队等待自己的 CPU 时间片，被调度器调度以后可以继续执行。

major、 first_minor和minors共同表征了磁盘的主、次设备号，同一个磁盘的各个分区共享1个主设备号，而次设备号则不同。fops为 block_device_operations，即上节描述的块设备操作集合。queue是内核用来管理这个设备的 I/O请求队列的指针。

### 设备号

    Linux的设备管理是和文件系统紧密结合的，各种设备都以文件的形式存放在/dev目录下，称为设备文件。应用程序可以打开、关闭和读写这些设备文件，完成对设备的操作，就像操作普通的数据文件一样。为了管理这些设备，系统为设备编了号，每个设备号又分为主设备号和次设备号。主设备号用来区分不同种类的设备，而次设备号用来区分同一类型的多个设备。对于常用设备，Linux有约定俗成的编号，如硬盘的主设备号是3。
    
     一个字符设备或者块设备都有一个主设备号和次设备号。主设备号和次设备号统称为设备号。主设备号用来表示一个特定的驱动程序。次设备号用来表示使用该驱动程序的各设备。例如一个嵌入式系统，有两个LED指示灯，LED灯需要独立的打开或者关闭。那么，可以写一个LED灯的字符设备驱动程序，可以将其主设备号注册成5号设备，次设备号分别为1和2。这里，次设备号就分别表示两个LED灯。

**区分块设备的分区**

块设备具有被称为**分区**的分配领域。例如，硬盘在物理上是一个设备，从内核的角度，硬盘被分为多个分区，而以这些分区为对象则形成了文件系统，此时，**次设备号既表示设备，也表示分区**。

## 磁盘中断中的问题

磁盘的 IO 操作完成后，磁盘会触发一个中断(interrupt)   ：Idestart通过buffer的flags分发读或写给设备和扇区。如果是写操作，idestart必须提供数据，然后中断会通知数据已经写完了。如果是读，会有中断通知数据已经准备好了

（这些中断是磁盘自己完成的是吗？不需要其他的指示，它会自己去读到buf上，然后发一个中断？）

目前这句话只理解到这里，再细节层次的就查不到了

为什么要用init check disk1呀，xv6有几个disk？通过哪里的标志切换？



## 位图

见书签夹“笔记”中“数据结构 位图”



## 文件系统



超级块，i节点块（i节点只是一个索引），空闲块，数据块，日志块

区块间大小是固定的吗？

最外面都是包装成file的，file层层往下调，最终由某更深的一层执行

### 块缓冲层  bio.c，buf.h，ide.c

以buf块为单位，封在bcache中，链表连接

bread，bwrite更新，bget查询，brelse修链表

中间要用到锁



buf结构保存的是一个buf块的信息

每一个块，同一时间只有一份拷贝放在内存中并且只有一个内核线程使用这份拷贝

buffer cache主要有两项功能1）提供磁盘block的同步访问；2）作为磁盘block在内存中的缓存

块代表的磁盘地址，flag，锁，引用count，在buf链表和d queue中的位置

最后还有一个data，是buf数据在内存的虚拟地址

#### 补充

可以看到几个名词：heads/sectors/cylinders，分别就是磁头/扇区/柱面，每个扇区512byte（现在新的硬盘每个扇区有4K）了
硬盘的最小存储单位就是扇区了，而且硬盘本身并没有block的概念。
文件系统不是一个扇区一个扇区的来读数据，太慢了，所以有了block（块）的概念，它是一个块一个块的读取的，block才是文件存取的最小单位。

xv6的bsz也是512

Logical Block Size一般是Sector Size的倍数 

要明确的是，块是文件系统的抽象，不是磁盘本身的属性。 
扇区大小则是磁盘的物理属性，它是磁盘设备寻址的最小单元。 

buffer_head中有两个字段表示块的磁盘地址:

```
               b_bdev 表示包含该块的块设备（通常是磁盘或者分区）

               b_blocknr存放逻辑块号，也就是块在磁盘或分区中的编号

                b_data 字段表示块缓冲区数据在缓冲区页中的位置。

               b_state存放缓冲区的状态

         只要内核必须单独访问一个块，就要涉及存放块缓冲区中的缓冲区页，并检查相应的buffer_head
```

在内存页中，有一种叫专门用途的页面叫“缓冲区页”。专门用来放块缓冲区。而每个块缓存区由两部分组成，缓冲区首部（用数据结构buffer_head表示）及真正的缓冲区内容（即所存储的数据，这些数据就放在刚刚说到的缓冲区页中）。在缓冲区首部中，有一个指向数据的指针和一个缓冲区长度的字段。当一个块被调入到内存中，它要被存储在一个缓冲区中。**每个缓冲区与一个块对应，它相当于磁盘块在内存中的表示**。而文件在内存中由file结构体表示，而磁盘块在内存中是由缓冲区来进行表示的。由于内核处理块时需要一些信息，如块属于哪个设备与块对应于哪个缓冲区。所以每个缓冲区都有一个缓冲区描述符，称为buffer_head. 它包含了内核操作缓冲区所需要的全部信息。 

### 日志层  log.c

xv6fs采用固定大小的log区，这就导致文件系统要检查每个写操作的大小会不会超过log区大小，并且需要将大文件的写入切分为小块，分批次commit（问题：如果批次commit过程中宕机了怎么办）。

日志层只负责写

read dst   dst也要读出来？，不是往里写就行了吗？感觉bget开个空buf就行了啊

#### 补充

begin_op()会等待正在commit的操作完成，以及log区剩余空间满足所有正准备写入的数据量。之后将log的outstanding加一，表示有系统调用正在写日志。

log_write()可以看作是对bwrite()的封装。但是只是将block加入logheader，修改block标志位为 `B_DIRTY` 从而保证不会别的进程占用，并不写数据。在此过程中，log如果发现当前修改的block之前有重复，则会合并两次修改，减少磁盘请求。

end_op()，将log的outstanding操作数减1，如果减为0则进行commit操作，否则唤醒其它等待begin_op()的进程，因为此次end_op()减少了outstanding数，也就是可能减少了占用的log空间。

提交阶段分四步，1）write_log()将内存log中的buf blocks写入磁盘log区的数据区；2）write_head()将内存log的block数组写入log区中，这一步是真正的commit操作；3）用lh中的信息，将log区数据区拷贝到真正的block（磁盘数据区）；4）将内存log清空，并将磁盘log区lh清空。

实际lh就代表整个log的状态了，没有lh的索引log的data区逻辑上就不存在

log区从start开始的block负责在commit时暂时存储转移的buf的数据，然后在trans的时候转移到buf对应的block

内存log到磁盘log，磁盘log到数据区

对于小文件写负载，日志式文件系统将传统文件系统中大量的小的同步随机写操作转换为大块的异步连续传输，这样能够利用磁盘接近100%的原始带宽。 

虽然日志式文件系统的基本思想是简单的，为了可以实现日志方式的优势，有两个关键问题需要解决。第一个是如何从日志中获取信息；这是3.1节的主题. 第二个问题是如何管理磁盘中的空闲空间，使得在向磁盘中写入数据时总是有大块的空闲空间可以使用。这是一个十分困难的问题。



### inode

struct inode *ip

ip->type = dip->type;
​    ip->major = dip->major;
​    ip->minor = dip->mino

ip不是地址吗？可以这样直接用吗？



begin_op中的log检查，决定了如果log在写，任何FS syscall都得等待吧

不能分区操作吗？一边log一些block，一边处理其他的block？

（这个就是log的短板）

#### 补充

目录（一个i节点，一个名字组成的结构条目组成的i节点）（i节点中有标志位）

路径名之前的节点都是目录，最后的一项才是文件

有了底层的这些函数，大多数的系统调用的实现都是很简单的

每个层面有各自面对的对象，与处理这些对象的函数，往往是一层层封装，一层层调用形成的

chan是一个void指针，里面记录的是什么？

intena是干嘛的？





## 锁

### 补充

竞争问题在于它们的结果由 CPU 执行时间以及其内存操作的先后决定的，并且这个问题难以重现。

参数，返回地址（函数后的下一条指令），上一个ebp，（然后ebp换为esp）

### 自旋锁

何谓**自旋锁**？它是为实现保护[共享资源](https://baike.baidu.com/item/%E5%85%B1%E4%BA%AB%E8%B5%84%E6%BA%90)而提出一种锁机制。其实，自旋锁与[互斥锁](https://baike.baidu.com/item/%E4%BA%92%E6%96%A5%E9%94%81)比较类似，它们都是为了解决对某项资源的互斥使用。无论是[**互斥锁**](https://baike.baidu.com/item/%E4%BA%92%E6%96%A5%E9%94%81)，还是**自旋锁**，在任何时刻，最多只能有一个保持者，也就说，在任何时刻最多只能有一个执行单元获得锁。但是两者在调度机制上略有不同。对于互斥锁，如果资源已经被占用，资源申请者只能进入睡眠状态。但是自旋锁不会引起调用者睡眠，如果自旋锁已经被别的执行单元保持，调用者就一直循环在那里看是否该自旋锁的保持者已经释放了锁，"自旋"一词就是因此而得名。

跟[互斥锁](https://baike.baidu.com/item/%E4%BA%92%E6%96%A5%E9%94%81)一样，一个执行单元要想访问被自旋锁保护的**共享资源**，必须先得到锁，在访问完共享资源后，必须释放锁。如果在获取自旋锁时，没有任何执行单元保持该锁，那么将立即得到锁；如果在获取自旋锁时锁已经有保持者，那么获取锁操作将自旋在那里，直到该自旋锁的保持者释放了锁。由此我们可以看出，自旋锁是一种比较低级的保护数据结构或代码片段的原始方式，这种锁可能存在两个问题：

[死锁](https://baike.baidu.com/item/%E6%AD%BB%E9%94%81)。试图递归地获得自旋锁必然会引起死锁：递归程序的持有实例在第二个实例循环，以试图获得相同自旋锁时，不会释放此自旋锁。在递归程序中使用自旋锁应遵守下列策略：递归程序决不能在持有自旋锁时调用它自己，也决不能在[递归调用](https://baike.baidu.com/item/%E9%80%92%E5%BD%92%E8%B0%83%E7%94%A8)时试图获得相同的自旋锁。此外如果一个进程已经将资源锁定，那么，即使其它申请这个资源的进程不停地疯狂“自旋”,也无法获得资源，从而进入[死循环](https://baike.baidu.com/item/%E6%AD%BB%E5%BE%AA%E7%8E%AF)。

过多占用[cpu](https://baike.baidu.com/item/cpu)资源。如果不加限制，由于申请者一直在循环等待，因此自旋锁在锁定的时候,如果不成功,不会睡眠,会持续的尝试,单cpu的时候自旋锁会让其它process动不了. 因此，一般自旋锁实现会有一个参数限定最多持续尝试次数. 超出后, 自旋锁放弃当前time slice. 等下一次机会。

由此可见，自旋锁比较适用于锁使用者保持锁时间比较短的情况。正是由于自旋锁使用者一般保持锁时间非常短，因此选择自旋而不是睡眠是非常必要的，自旋锁的效率远高于[互斥锁](https://baike.baidu.com/item/%E4%BA%92%E6%96%A5%E9%94%81)。[信号量](https://baike.baidu.com/item/%E4%BF%A1%E5%8F%B7%E9%87%8F)和读写信号量适合于保持时间较长的情况，它们会导致调用者睡眠，因此只能在[进程上下文](https://baike.baidu.com/item/%E8%BF%9B%E7%A8%8B%E4%B8%8A%E4%B8%8B%E6%96%87)使用，而自旋锁适合于保持时间非常短的情况，它可以在任何上下文使用。如果被保护的[共享资源](https://baike.baidu.com/item/%E5%85%B1%E4%BA%AB%E8%B5%84%E6%BA%90)只在进程上下文访问，使用信号量保护该共享资源非常合适，如果对共享资源的访问时间非常短，自旋锁也可以。但是如果被保护的共享资源需要在中断上下文访问（包括底半部即[中断处理](https://baike.baidu.com/item/%E4%B8%AD%E6%96%AD%E5%A4%84%E7%90%86)句柄和顶半部即[软中断](https://baike.baidu.com/item/%E8%BD%AF%E4%B8%AD%E6%96%AD)），就必须使用自旋锁。自旋锁保持期间是抢占失效的，而信号量和读写信号量保持期间是可以被抢占的。自旋锁只有在[内核](https://baike.baidu.com/item/%E5%86%85%E6%A0%B8)可抢占或[SMP](https://baike.baidu.com/item/SMP)（多处理器）的情况下才真正需要，在单CPU且不可抢占的内核下，自旋锁的所有操作都是空操作。

事实上，自旋锁的初衷就是：在短期间内进行轻量级的锁定。一个被争用的自旋锁使得请求它的线程在等待锁重新可用的期间进行自旋(特别浪费处理器时间)，所以自旋锁不应该被持有时间过长。如果需要长时间锁定的话, 最好使用[信号量](https://baike.baidu.com/item/%E4%BF%A1%E5%8F%B7%E9%87%8F)。

1自旋锁实际上是忙等锁

当锁不可用时，CPU一直循环执行“测试并设置”该锁直到可用而取得该锁，CPU在等待自旋锁时不做任何有用的工作，仅仅是等待。因此，只有在占用锁的时间极短的情况下，使用自旋锁才是合理的。当[临界区](https://baike.baidu.com/item/%E4%B8%B4%E7%95%8C%E5%8C%BA)很大或有共享设备的时候，需要较长时间占用锁，使用自旋锁会降低系统的性能。

自旋锁可能导致系统死锁

引发这个问题最常见的情况是[递归](https://baike.baidu.com/item/%E9%80%92%E5%BD%92)使用一个自旋锁，即如果一个已经拥有某个自旋锁的CPU 想第二次获得这个自旋锁，则该CPU 将[死锁](https://baike.baidu.com/item/%E6%AD%BB%E9%94%81)。此外，如果进程获得自旋锁之后再阻塞，也有可能导致死锁的发生。copy_from_user()、copy_to_user()和[kmalloc](https://baike.baidu.com/item/kmalloc)()等函数都有可能引起阻塞，因此在自旋锁的占用期间不能调用这些函数。代码清单7.2 给出了自旋锁的使用实例，它被用于实现使得设备只能被最多一个进程打开。



![image-20180911104149559](/var/folders/xx/ztdttg0x1jl1lhkrxbzhk7rw0000gn/T/abnerworks.Typora/image-20180911104149559.png)



xv6用的全是自旋锁



### 模块化

现在还没有一种透明方案可以让调用者和被调者互相隐藏所使用的锁

一个常见、透明但不甚理想的解决方案是*递归锁（recursive locks）*，它使得被调者能够再次获得调用者已经持有的锁

但是递归锁不能用来保护不变量

由于没有理想、透明的解决方法，我们不得不在函数的使用规范中加入锁。编程者必须保证一个函数不会在持有锁时调用另一个需要获得该锁的函数 `f`。就这样，锁也成为了我们的抽象中的一员

比如acquire中写明，当 CPU 持有锁时，它不能再调用另一个试图获得该锁的函数 `f`



### 死锁，顺序锁

为了避免死锁，当中断处理程序会使用某个锁时，处理器就不能在允许中断发生时持有锁。

xv6 做得更决绝：允许中断时不能持有任何锁。（acquire时要先pushcli）

比方说如果 `lk->locked=0` 在乱序后被放到了 `popcli` 之后，可能在锁被释放之前，另一个线程中就允许中断了

打开的乱序FB什么时候关呢？

用户级程序也需要锁，但 xv6 的程序只有一个运行线程，进程间也不会共享内存，所以就不需要锁了。





## 操作系统和内核

包括操作系统内核、shell、驱动、运行库、引导程序等各种东西都是操作系统的一部分。（内核不包括驱动？）

对于Linux来说，/lib，/boot下的大多数东西都属于操作系统

内核：操作系统最核心的部分，一般来说，内核特指某一个或者几个文件。

**而操作系统除了内核程序外，还有包括其他一些基本组件**，如文本编辑器、编译器、用来与用户进行交互的程序等

总的说来，**一个操作系统包含了内核**（是一个提供硬件抽象层、磁盘及文件系统控制、多任务等功能的系统软件）**以及其他计算机系统所必须的组件**（如函数库、编译器、调式工具、文本编辑器、网站服务器，以及一个Unix的使用者接口（Unix shell）等，这些都是操作系统的一部分，而且每一个模块如编译器都是一个单独的进程，运行在操作系统中）。**所以一个内核不是一套完整的操作系统，拿Linux来说，****Linux这个词本身只表示Linux内核，但现在大家已经默认的把Linux理解成整个Linux系统，这是由于历史原因造成的（具体可以看本文前言中提到的那篇文章），**也就是说人们已经习惯了用Linux来形容整个基于Linux内核，并且使用GNU 工程各种工具和应用程序的操作系统（也被称为GNU/Linux），而基于这些组件的Linux软件被称为Linux发行版。一般来讲，一个Linux发行版本出来包括Linux内核之外，还包含大量的软件（套件），比如软件开发工具，数据库，Web服务器（例如Apache），X Window，桌面环境（比如GNOME和KDE），办公套件（比如OpenOffice、org）等等。

操作系统位于底层硬件与用户之间，是两者沟通的桥梁。用户可以通过操作系统的用户界面，输入命令。操作系统则对命令进行解释，驱动硬件设备，实现用户要求。以现代标准而言，一个标准PC的操作系统应该提供以下的功能：

* 进程管理（Processing management）
* 内存管理（Memory management）
* 文件系统（File system）
* 网络通信（Networking）
* 安全机制（Security）
* 用户界面（User interface）
* 驱动程序（Device drivers）

![image-20180912105132534](/var/folders/xx/ztdttg0x1jl1lhkrxbzhk7rw0000gn/T/abnerworks.Typora/image-20180912105132534.png)

### linux内核

Linux试图在硬件无关的源代码与硬件相关的源代码之间保持清晰的界限。为了做到这一点，在arch和include目录下包含了23个子目录，以对应Linux所支持的不同硬件平台。这些平台的标准名字如下：

Linux 内核可以进一步划分成 3 层。最上面是系统调用接口，它实现了一些基本的功能，例如 `read` 和 `write`。系统调用接口之下是内核代码，可以更精确地定义为独立于体系结构的内核代码。这些代码是 Linux 所支持的所有处理器体系结构所通用的。在这些代码之下是依赖于体系结构的代码，构成了通常称为 BSP（Board Support Package）的部分。这些代码用作给定体系结构的处理器和特定于平台的代码。

Linux 内核的主要组件：

系统调用接口，进程管理，内存管理，虚拟文件系统，网络堆栈，设备驱动，依赖体系结构的代码（每个体系结构子目录都包含了很多其他子目录，每个子目录都关注内核中的一个特定方面，例如引导、内核、内存管理等）

Linus 领导的开源社区只负责开发内核，不开发其它的东西（比如：运行库、图形界面、应用软件、等）。

这就引出一个问题——光有一个赤裸裸的内核，用户是没法用的（就好比你光拿到一个汽车引擎，你是没法开车的）。为此，就有一大帮开源社区或商业公司，在这个裸露的内核外面，再包上一些东西（比如：运行库、应用软件）。经过这样包装之后，就成为"发行版"。

在这种模式下，会有各种各样的发行版，正好可以覆盖千奇百怪的需求；其次，同质化的发行版之间会产生竞争，最终只有优秀的发行版会存活，差劲的发行版会逐渐消亡——这就是开源生态圈的"达尔文主义"。

如果用一个词来形容 Linux 的优点，那就是——多元化。内核的开发是多元化的（任何人都可以参与），发行版是多元化的（任何人都可以搞发行版）。





## 中断处理机制  特权级转换  内核态

### 整体感觉

中断：主板上的设备可以产生中断，xv6 必须配置硬件来处理这些中断。

我们得对设备编程来产生一个中断，然后令 CPU 接受它们的中断。

对设备编程

在初始化的过程中，xv6 将第 0 号中断映射到 IRQ 0，以此类推，然后把它们都屏蔽掉。

不同的设备自己开启自己的中断，并且同时指定哪一个处理器接受这个中断。



我们希望分时硬件大约以每秒 100 次的速度产生一个中断，这样内核就可以对进程进行时钟分片

100 次每秒的速度足以提供良好的交互性能并且同时不会使处理器进入不断的中断处理中

像 x86 处理器一样，PC 主板也在进步，并且提供中断的方式也在进步。

* 早期的主板有一个简单的可编程中断控制器（被称作 PIC），你可以在 picirq.c 中找到管理它的代码。
* 随着多核处理器主板的出现，需要一种新的处理中断的方式，因为每一颗 CPU 都需要一个中断控制器来处理发送给它的中断，而且也得有一个方法来分发中断。
* 这一方式包括两个部分：第一个部分是在 I/O 系统中的（IO APIC，ioapic.c），另一部分是关联在每一个处理器上的（局部 APIC，lapic.c）。
* xv6 是为搭载多核处理器的主板设计的，每一个处理器都需要编程接受中断。

为了在单核处理器上也能够正常运行

* xv6 也为 PIC 编程（6932）。每一个 PIC 可以处理最多 8 个中断（设备）并且将他们接到处理器的中断引脚上。
* 为了支持多于八个硬件，PIC 可以进行级联，典型的主板至少有两集级联。使用 `inb` 和 `outb` 指令，xv6 配置主 PIC 产生 IRQ 0 到 7，从 PIC 产生 IRQ 8 到 16。
* 最初 xv6 配置 PIC 屏蔽所有中断。timer.c 中的代码设置时钟 1 并且使能 PIC 上相应的中断（7574）。
* 这样的说法忽略了编写 PIC 的一些细节。这些 PIC（也包括 IOAPIC 和 LAPIC）的细节对本书来说并不重要，但是感兴趣的读者可以参考 xv6 源码引用的各设备的手册。

在多核处理器上

* xv6 必须编写 IOAPIC 和每一个处理器的 LAPIC。
* IO APIC 维护了一张表，处理器可以“通过内存映射 I/O 写这个表的表项”，而非使用 `inb` 和 `outb` 指令。
* 在初始化的过程中，xv6 将第 0 号中断映射到 IRQ 0，以此类推，然后把它们都屏蔽掉。
* 不同的设备自己开启自己的中断，并且同时指定哪一个处理器接受这个中断。举例来说，xv6 将键盘中断分发到处理器 0（7516）。将磁盘中断分发到编号最大的处理器，你们将在下面看到。

时钟芯片是在 LAPIC 中的，每一个处理器可以独立地接收时钟中断。

* xv6 在 `lapicinit`（6651）中设置时钟中断
* 关键的一行代码`timer`（6664）告诉 LAPIC 周期性地在 IRQ_TIMER（也就是 IRQ 0) 产生中断。第 6693 行打开 CPU 的 LAPIC 的中断，这使得 LAPIC 能够将中断传递给本地处理器

处理器可以通过设置 `eflags` 寄存器中的 `IF` 位来控制自己是否想要收到中断。

* 指令 `cli` 通过清除 `IF` 位来屏蔽中断，而 `sti` 又打开一个中断。
* xv6 在启动主 cpu（8412）和其他 cpu（1126）时屏蔽中断。每个处理器的调度器打开中断（2464）。
* 为了控制一些特殊的代码片段不被中断，xv6 在进入这些代码片段之前关中断（例如 `switchuvm`（1773））。

xv6 在 `idtinit`（1265）中设置时钟中断触发中断向量 32（xv6 使用它来处理 IRQ 0）。

* 中断向量 32 和中断向量 64（用于实现系统调用）的唯一区别就是 32 是一个中断门，而 64 是一个陷阱门。
* 中断门会清除 IF，所以被中断的处理器在处理当前中断的时候不会接受其他中断。从这儿开始直到 `trap` 为止，中断执行和系统调用或异常处理有相同的代码——建立中断帧。

当因时钟中断而调用 `trap` 时，`trap` 只完成两个任务：递增时钟变量的值（3064），并且调用 `wakeup`。后者可能会使得中断返回到一个不同的进程（章5）

### 中断分类

**中断：**

*     可屏蔽中断    ——I/O设备发出的所有中断请求(lRQ)都产生可屏蔽中断
*     不可屏蔽中断——只有几个危急事件(如硬件故障)才引起非屏融中断。非屏蔽中断总是由CPU辨认

**异常：**

*     处理器异常——当CPU执行指令时探测到的一个反常条件所产生的异常.（故障、陷阱、异常中止）（这些是怎么传的？）
*     编程异常    ——在编程者发出请求时发生.是由int或int3指令触发

中断和异常要通过硬件设施来产生中断请求，这些都是硬中断，与其相对应的不必由硬件产生中断源而能引发的一种中断称为软中断。软中断分为：信号和软件中断。

### 中断产生

#### 硬件

每个能够发出中断请求的硬件设备控制器都有一条名为IRQ(lnterrupt ReQuest) 的输出线。所有现有的IRQ线都与一个名为可编程中断控制器（PIC）的硬件电路的输入引脚相连。

    可编程中断控制器的功能：
    
    1）监视IRQ线，检查产生的信号(raised signal).如果有条或两条以上的IRQ线上产生信号，就选择引脚编号较小的IRQ线.（现在的高级可编程中断控制器APIC，通过重定向和中断向量优先级决定中断的优先级。
    
    2） 如果某个IRQ线上产生了引发信号：
    
        a.  把接收到的引发信号转换成对应的向量.
    
        b. 把这个向量存放在中断控制器的一个I/O端口，从而允许CPU通过数据总线读此向量。
    
        c.  把引发信号发送到处理器的INTR引脚，则产生一个中断
    
        d.  等待，直到CPU通过把这个中断信号写进可编程中断控制器的一个I/O端口来确认它，当这种情况发生肘，清INTR线.    
    
    3）返回到第1 步。
    
    IRQ线从0开始编号（IRQ0），与中断向量一一对应。其中IRQn 对应的是中断向量n+32，（即IRQ0对应的是中断向量32）。可通过可编程中断控制器修改映射关系。

#### int

压栈，查表，跳转

### 中断处理（有的部分有硬件电路实现）

#### 宏观

不管来源是哪里，CPU检测到了中断请求信号，就会进行处理。

中断处理的过程是这样的： 首先，CPU会关闭中断响应，也就是不再接受后面其他的外部中断请求了。注意，只是不接收外部中断请求。然后将发生中断处的这个指令的地址 保存到栈中，也就是内存当中的一个区域，这个信息必须要保存好，以便于处理完中断后可以正确地返回当前的程序继续执行。那究竟是保存发生中断的这条指令的地址， 还是发生中断的这条指令之后的一条指令的地址，和这个中断具体的类型会有关系。第三步则由CPU识别中断的来源， 确定中断类型号，（查表？？？）从而能够找到相应的中断服务程序的入口地址。

这三步一般都是由硬件自动完成的。硬件可以直接分析门，转换特权级和堆栈（函数代码位于不同的段中，每个段有自己的特权级，不同特权级之间的调用要用到call和门调用），直接转移到门中指定的的入口。

第四步是保存现场，然后第五步，就是执行这个中断服务程序的主体内容。而且在中断服务程序中，可以在适当的时候重新开放中断， 以便响应其他高优先级的外部中断。以免中断服务程序本身要执行比较长的时间，造成有高优先级的外部中断长时间无法得到响应，

那如何在中断服务程序当中，重新开放中断呢？

那这就涉及到对标志寄存器的操作。在标志寄存器当中有一些标志位是状态标志。状态标志一般情况下，是由硬件设置，然后由软件读取。例如进位标志，就是在运算器产生进位时，由硬件自动设置，然后由软件的指令读取出来，可能会进行相关的累加操作。而另一类标志称为控制标志，这些标志通常是由软件进行设置，然后硬件电路根据这些标志设置的不同，而执行不同的功能。那么标志寄存器的第九位就是中断标志。这个标志控制对可屏蔽中断的响应。那外部 中断分为两大类，一类是可屏蔽中断，一类是非屏蔽中断。IF标志只对外部中断当中的可屏蔽中断起作用。如果IF等于1，那就允许CPU响应可屏蔽中断请求； 如果IF等于0，那就不允许CPU响应可屏蔽中断请求。那怎么设置IF标志位的值呢？有两条 指令。STI指令就是把IF位置为1，而CLI指令就是把IF标志位清零。这两条指令都是没有操作数的指令。当然，IF指令对非屏蔽中断和内部中断都是不起作用的。

然后我们来看中断处理过程的最后一步，那就是恢复现场并且返回主程序继续运行。中断返回指令放在中断服务程序的末尾，可以不带操作数，就是从当前的栈顶弹出三个字， 分别送到IP、CS和FLAGS寄存器当中去。在中断调用时，CPU中的硬件电路会将这三个寄存器的值 压入栈中，在执行中断返回指令时，也会由硬件从栈顶弹出这三个字，再放回到这三个寄存器当中。那当CS和IP寄存器改变之后， 下一条指令就会回到程序发生中断的地方继续执行了。

#### 细节

假定内核已被初始化，CPU在保护模式下运行

当执行了一条指令后，CS和eip这对寄存器包含下一条将要执行的指令的逻辑地址.在处理那条指令之前，控制单元会检查在运行前一条指令时是否已经发生了一个中断或异常。如果发生了一个中断或异常，那么控制单元执行下列操作（这些都是硬件电路实现的吧？像分页硬件一样？）:

    1）确定与中断或异常关联的向量i (0 ≤ i ≤ 255)
    
    2）读由idtr寄存器指向的IDT表中的第i项(在下面的描述中，我们假定IDT表项中包含的是一个中断门或一个陷阱门)。

* 中断描述符表见下

    3）从gdtr寄存器获得GDT的基地址，并在GDT中查找，以读取IDT表项中的选择符所标识的段描述符.这个描述符指定中断或异常处理程序所在段的基地址.

    4）确信中断是由授权的(中断)发生源发出的。首先将当前特权组CPL(存放在cs寄存器的低两位)与段描述符(存放在GDT中)的描述符特权级DPL比较，如果CPL小于DPL. 就产生一个"Generalprotection" 异常.因为中断处理程序的特权不能低于引起中断的程序的特权.对于编程异常，则做进一步的安全检查:比较CPL与处于IDT中的门描述符的DPL，如果DPL小于CPL，就产生一个"General protection"异常。这最后一个检查可以避免用户应用程序访问特殊的陷阱门或中断门.

    5）检查是否发生了特权级的变化，也就是说，CPL是否不同于所选择的段描述符的DPL. 如果是，控制单元必须开始使用与新的特权级相关的栈。通过执行以下步骤来做到这点:

        a.  读tr寄存器，以访问运行进程的TSS段.
        
        b.  用与新特权级相关的钱段和战指针的正确值装载ss和esp寄存器。这些值可以在TSS中找到
        
        c.  在新的栈中保存ss和esp以前的值. 这些值定义了与旧特权级相关的梭的逻辑地址

    6）如果故障已发生，用引起异常的指令地址装载cs和elp客存器，从而使得这条指令能再次被执行。    

    7）在战中保存eflags、 cs及eip的内容.

    8）如果异常产生了一个硬件出错码，则将它保存在战中.

    9）装载cs和eip寄存器，其值分别是IDT表中第i项门描述符的段选择符和偏移量字段.这些值给出了中断或者异常处理程序的第一条指令的逻辑地址。

控制单元所执行的最后一步就是跳转到中断或者异常处理程序.换句话说.处理完中断信号后.控制单元所执行的指令就是被选中处理程序的第一条指令.

中断或异常被处理完后，相应的处理程序必须产生一条iret指令，把控制权转交给被中断的进程，这将迫使控制单元:

    l） 用保存在校中的值装载cs、 eip或eflags寄存器。如果一个硬件出错码曾被压入校中，并且在eip内容的上面，那么，执行iret指令前必须先弹出这个硬件出错码.
    
    2）检查处理程序的CPL是否等于cs中最低两位的值(这意味着被中断的进程与处理程序运行在同一特权级). 如果是，iret终止执行， 否则，转入下一步.
    
    3）从战中装载ss和esp寄存器，因此，返回到与旧特权级相关的钱。
    
    4） 检查ds、 es、 fs，&gs段寄存器的内容，如果其中一个寄存器包含的选择符是一个段描述符，并且其DPL值小子CPL，那么，清相应的段寄存器.控制单元这么做是为了禁止用户态的程序(CPL=3)利用内核以前所用的段客存器(DPL=O).如果不清这些寄存器，怀有恶意的用户态程序就可能利用它们采访问内核地址空间.

#### 具体的门

调用门：

call 和 jmp 指令后面接 **调用门的选择子**作为参数， 这里的**偏移量无视**。 call 指令可以实现向高特权级代码转移， jmp 只能同级转移

当处理器访问调用门时，它会使用调用门中的段选择子来定位目标代码段的段描述符。然后CPU会把代码段描述符中的基地址和调用门中的偏移值进行组合，形成代码段中指定程序入口点的线性地址。

如果通过调用门把控制转移到了更高特权级的非一致代码段中，那么CPL就会被设置为目标代码段的DPL值，并且会引起堆栈切换。

根据调用门中描述符中的选择子对应的目标代码段的DPL，处理器会自动在 TSS 中找到合适的栈段选择子 SS 和 栈指针 ESP，它们作为转移后的新栈，记录为 SS_new 和 ESP_new。

调用门描述符中记录的是目标程序所在代码段的选择子及偏移地址。这意味着代码段寄存器 CS 要用选择子重新加载。为了在之后恢复到用户进程，我们同样也需要备份 CS_old 和 EIP_old。

一切就绪，最后只需要把代码段选择子装载到代码段寄存器CS中，把偏移量赋值给EIP。

任务门：

以任务状态段 TSS 为单位，用来实现任务切换，它可以借助中断或指令发起。当中断发生后，对应的中断向量号（哪里有中断向量表？）是任务门，则会发起任务切换（这段代码又在哪里？）。也可以像调用门那样，用 call 或 jmp 指令后接任务门的选择子或任务TSS 的选择子。

call 和 jmp 指令后接调用门选择子作为参数，以调用函数实现从低特权向高特权级转移，可用来实现系统调用。但主流OS都是用中断门实现系统调用。

除了任务门外，其他三种门都是对应一段例程，即对应一段函数，而不是像段描述符对应的是一片内存区域。

中断门：int 指令主动发中断的方式，从低特权级向高特权级转移。（意思应该是int根据参数查表后会进入某个中断门指定的地址）linux 系统调用就是用中断门实现。

陷阱门：int3 指令主动发中断形式从低特权级向高特权级转移， 一般是在编译器调试时使用

### 中断描述符表

![image-20180912220746517](/var/folders/xx/ztdttg0x1jl1lhkrxbzhk7rw0000gn/T/abnerworks.Typora/image-20180912220746517.png)

表示一个系统表，它与中断或异常向量相联系。每一个中断或异常向量在这个系统表中有对应的中断或异常处理程序入口地址。 中断描述符 的每一项对应一个中断或异常向量，每个向量由8个字节组成。因此，最多需要256*8=2048 字节来存放IDT。IDT包含三种类型的中断描述符：任务门、中断门、陷阱门。描述符的第40~43位用于区分不同的描述符（每个描述符有8个字节/64bit）。 

在运行中断之前，必须初始化IDT（中断描述符表）。 

	80x86微机支持256个中断，对应每个中断需要安排一个中断服务程序。在80x86实模式运行方式下，每个中断向量由4字节组成。这4字节指明了一个中断服务程序的段值和段内偏移值。因此整个向量表的长度为1KB。当80x86微机启动时，ROM BIOS中的程序会在物理内存开始地址0x0000:0x0000处初始化并设置中断向量表，而各中断的默认中断服务程序则在BIOS中给出。由于中断向量表中的向量是按中断号顺序排列，因此给定一个中断号N，那么它对应的中断向量在内存中的位置就是0x0000:N*4，即对应的中断服务程序入口地址保存在物理内存0x0000:N*4位置处。
	
	在BIOS执行初始化操作时，它设置了两个8259A芯片支持的16个硬件中断向量和BIOS提供的中断号为0x10～0x1f的中断调用功能向量等。对于实际没有使用的向量则填入临时的哑中断服务程序的地址。以后在系统引导加载操作系统时会根据实际需要修改某些中断向量的值。例如，对于DOS操作系统，它会重新设置中断0x20～0x2f的中断向量值。而对于Linux系统，除了在刚开始加载内核时需要用到BIOS提供的显示和磁盘读操作中断功能，在内核正常运行之前则会在setup.s程序中重新初始化8259A芯片并且在head.s程序中重新设置一张中断向量表（中断描述符表）。完全抛弃了BIOS所提供的中断服务功能。

**任务门(task gate)** 

     当中断信号发生时，必须取当前进程的那个进程的TSS选择符存放在任务门中。

**中断门(interruptgate)** 

     包含段选择符和中断或异常处理程序的段内偏移量。当控制权转移到一个适当的段 时，处理器 清IF标志，从而关闭将来会发生的可屏蔽中断.

**陷阱门(Trap gate)**

     与中断门相似，只是控制权传递到一个适当的段时处理器不修改IF标志.

以上为Intel对中断描述符的分类。Linux采用了更细的分类方法 

**中断门(interrupt gate)**

    用户态的进程不能访问的一个lntel中断门(门的DPL字段为0。)（这是给硬件中断用的）。所有的Linux中断处理程序都通过中断门激活，并全部限制在内核态。

**系统门(syslem gate)**

    用户态的进程可以访问的一个Intel陷阱门。通过系统门来激 活三个Linux异常处理程序，它们的向量是4，5及128，因此，在用户态下，可以 发布into、 bound及int $Ox80三条汇编语言指令。

**系统中断门(system interrupt gate)**

    能够被用户态进程访问的Intel中断门(门的DPL字段为3). 与向量3相关的异常 处理程序是由系统中断门激活的，因此，在用户态可以使用汇编语言指令int3.

**陷阱门(Irapgate)**

    用户态的进程不能访问的一个Inte)陷阱门(f]的DPL字段为0). 大部分Linux异 常处理程序都通过陷阱门来激活.

**任务门(task gate)**

    不能被用户态进程访问的Intel任务门(门的DPL字段为0).Linux对"Doublefault" 异常的处理程序是由任务门激活的.

### 系统调用

call _fork（汇编看出来的

如果fork里面是参数压栈和int：	

			movel　　$3,%eax

　　　　　　movel　　fd,%ebx

　　　　　　movel　　buffer,%ecx

　　　　　　movel　　nbutes,%edx　　　　　　

 　　　　　int　　$0x80

int中断可以查表通过中断门找到入口，进入syscall，然后查系统调用表，进入fork

* 对系统调用进行封装，便于用户调用（就是把调用号，参数，int都包装到一个函数里），比如说fork()，getpid()，这些函数都是libc库中的函数，对系统调用进行了封装，遵循POSIX标准

* POSIX是一套操作系统标准，它是随着UNIX标准化而诞生的。linux是遵循POSIX标准的操作系统，如果linux遵循POSIX标准的话，UNIX上的应用程序就可以顺利移植到linux上来了。形象点说也就是linux和UNIX
  很多函数名字及其参数都定义的是一样的
* 现在对系统调用的封装都是用的syscall函数，该函数是不定参数，很好用，可以调用所有的系统调用

但是我在xv6的fork函数里没有看到包装的int中断，sleep和exec也是，都像是c直接实现的函数。sys_fork等里面包含fork函数很正常，但是如果是程序中直接调用fork这些函数呢？那这里call的参数直接就是fork对应的调用门吗？通过调用门直接栈转换切到fork？

还有user.h中声明的类似直接fork的系统调用函数，只有这里的声明，没有找到定义怎么办？

我找到一个系统调用int包装：用预处理包装SYSCALL(name)，指令会被编译为int指令调用，name传递了int需要的系调参数

但是SYSCALL(name)这个串在哪里出现过？我没见到

```hui
#include "syscall.h"
#include "traps.h"
#define SYSCALL(name) \
  .globl name; \
  name: \
    movl $SYS_ ## name, %eax; \
    int $T_SYSCALL; \
    ret
SYSCALL(fork)
SYSCALL(exit)
SYSCALL(wait)
SYSCALL(pipe)
SYSCALL(read)
SYSCALL(write)
SYSCALL(close)
SYSCALL(kill)
SYSCALL(exec)
SYSCALL(open)
SYSCALL(mknod)
SYSCALL(unlink)
SYSCALL(fstat)
SYSCALL(link)
SYSCALL(mkdir)
SYSCALL(chdir)
SYSCALL(dup)
SYSCALL(getpid)
SYSCALL(sbrk)
SYSCALL(sleep)
SYSCALL(uptime)
```



## xv6  进程调度

0、中断可以保证单CPU中代码段的原子性，自旋则实现多CPU之间的互斥。     

1、系统先自构造一个init进程的数据保存起来待运行，构造init进程需要调用userinit()函数，构造其他进程则需当前进程调用sys_fork()(实际为fork()函数)，这两个函数都需要使用allocproc()函数来分配进程号进程内核栈等操作。不同之处在于userinit()是手工填入了进程的其他信息（段，寄存器，装入运行程序代码）；fork()函数则是直接复制了父进程的大部分信息（没有实现写时复制），并设置返回值。

2、每个CPU初始化完毕后就载入scheduler函数开始进程调度，scheduler函数是个死循环，不停的通过内循环遍历所有进程状态，由于遍历的过程中可能会改变进程状态，所以，一次遍历的内循环外需要加锁保护（多个CPU之间保护）。     

3、一旦scheduler函数发现有待运行的进程就调用swithuvm来载入这个进程段数据，并且把这个进程设置为运行中，调用swtch()来切换进程的内核态上下文，swtch()返回后就到了forkret()(新进程)或者sched()(yield,sleep,exit这种会导致进程切换的函数中调用的sched)，具体哪个函数要看swtch切换的进程上下文状态而定。如果没有发现待执行进程则到第6步。     

4、进程从swtch()返回后，要把scheduler()加的锁释放掉，然后返回到系统调用或者中断返回处，进入用户态运行。     

5、如果运行中遇到时间片切换(则立刻调用yield()放弃CPU)或者上述导致进程切换的中断系统调用，进程则会从用户态进入内核态的对应函数

* 调用sleep的时候会通过门提升特权级转到内核态，然后和scheduler切换上下文，进入调度器，这时候的进程，要么是之前挂起的，要么是新进程，这两种情况都是在内核态，和scheduler共用其他的段没问题，context和和iret出的eip足够切换回原来的上下文了（只是没有记录通用寄存器，这个有没有影响？）

获得进程列表锁，然后根据当前函数来设置运行状态，再调用sched()函数检查进程状态，检查ok后，sched会调用swtch()把当前内核态切换到scheduler()中，这时候scheduler的状态就像从第3步的swtch()函数返回出来一样（swtch()所造成的静态代码连续，执行代码交错，超级无敌的函数间切换，天衣无缝），继续执行第3步的操作。     

6、scheduler()内循环执行了一遍。然后释放进程列表的锁，这是考虑到没有进程待执行，但另外的CPU需要进行进程切换。如果不释放进程列表所，另外的CPU就没有进行进程切换，导致CPU空转锁死。然后开始新一次的外循环，外循环开始的时候会开起中断，是为了防止没有进程待执行，但有进程在IO阻塞的时候，关闭中断使得进程永远都不会唤醒，从而导致CPU中断锁死。     

7、使用sleep()和wake()作为进程通信原语，但没有实现信号量，对临界资源的访问需要用自旋锁协助。     

8、使用exit()/kill()和wait()实现进程销毁，因为子进程无法回收自己的内核栈以及页目录等，所以交给父进程的wait()来处理。



## 驱动程序

驱动程序是操作系统中用于管理某个设备的代码

它提供设备相关的中断处理程序，操纵设备完成操作，操纵设备产生中断，等等

驱动程序可能会非常难写，因为它和它管理的设备同时在并发地运行着；必须要理解设备的接口（例如，哪一个 I/O 端口是做什么的）。设备的接口有可能非常复杂并且文档稀缺。



## 汇编符号

#是立即数的标志，#250是立即数250，它代表的是内容；如果不带#，它代表的是单元地址，是存储内容的。

 \$  在数据段定义里，表示当前的偏移地址

leng equ \$ -str      就是当前地址减去str1的初始地址

在字符串中是结束标志，就是字符串遇到$才会结束，类似于c语言中字符串中结尾的'0'

## 数据寻址方式：

查找操作数或操作数存放位置的方法。主要有3大类：

操作数包含在指令中、包含在处理器某个内部寄存器中、包含在存储器中。

**立即数寻址方式 ：**操作数包含在指令中。在取指令的同时，操作数也随着取出，这种操作数被称为立即数，这种寻址方式也就称为**立即数寻址方式**。

**优点：**指令的执行速度快，因为立即寻址方式在取出指令的同时也取出了操作数。

**缺点：**由于操作数是指令的一部分，不便于修改，降低了程序的通用性和灵活性。

**适合的场景：**只适合于操作数固定的场合，通常用于为主存单元和寄存器提供常数。







## 锁（看了代码之后）

### 总体

锁的保护机制就体现在每次acquire的时候，要保护某个数据，就在要修改那个数据的时候acquire它的锁，如果别人在修改，acquire就不会成功，因此修改不了，这就算保护

只是有时候仅仅是保护一个小域，却因为一个结构只有这一个锁而导致其他acquire也只能acquire这个锁，就算是其他域也修改不了

什么时候使用锁，这就需要程序员对哪里可能出现干扰有很深入完备的分析。记得锁的两个要素：实现cpu的原子操作，保护共享数据

（而在没事实现锁的时候，acquire里面的原子操作是xchg实现的。“spinlock 用于CPU同步, 它的实现是基于CPU锁定数据总线的指令”）

锁的使用时机，当且仅当：修改共享数据的时候（防止覆盖），访问共享数据的时候（可能其他cpu进入修改），交互信息修改（比如状态）需要保证原子操作的时候

cpu共享数据：ptable，log，磁盘（通过每个buf锁保护），bcache，buf，icache



### 具体

#### ptable lock

为什么需要ptable lock

ptable中是共享数据，其中一些又要在某些时候保持不变

锁就是为了一段时间保护共享数据，和实现原子操作

ptable lock只在访问ptable和修改proc状态的时候使用了

调度的时候，都是在swtch之前获得，之后释放

#### buf相关的锁

kernel会一直运行

bcache结构编译之后就在kernel的全局变量区，这个空间之后就没再释放过，然后在init的时候连好链表

每个buf也有锁（不同于bcache锁，bcache和buf是两个变量），get之后就锁上，relse的时候释放，保证同一时间只有一个cpu可以使用buf。buf在使用的时候也需要保护，而且时间较长，所以用了sleeplock

#### inode相关的锁

inode结构也有锁。cpu操作的都是内存上的inode，inode锁要保护的是对inode结构的修改。（icache保护的是其中inode的ref（icache就是用ref参考分配的））也是一个sleeplock。

ialloc，iget，idup的时候，有buf和icache里的锁保护，没有用inode锁，

iput（itrunc）的时候，修改link的时候（不能同时修改，会覆盖）使用了inode锁。可能还有修改其他参数的时候

每次fileread中调用iread之前也会用ilock锁上（保证每次），因为iwrite的时候必须用锁，如果iread不用，则另一个cpu就可能进入write，影响read



### 补充

spin lock相对于信号量这样的锁机制的好处就是，节约了2次context switch的开销，所以如果线程等待锁的时间小于2次context switch的时间，系统性能从spin lock获得的提升就越多。

可以看到对于SMP系统，它的实现核心是lock指令，而对于单CPU系统来说，则退化为空操作，因为对于单CPU来说，在某程序执行期间，不可能有其它CPU来中断它的执行，因此，实际上，非SMP系统中的原子操作是没有必要存在的。下面讨论SMP系统。讨论前，先了解x86中的lock指令。lock指令是一种前缀，它可与其他指令联合，用来维持总线的锁存信号直到与其联合的指令执行完为止。当CPU与其他处理机协同工作时，该指令可避免破坏有用信息。它对中断没有任何影响，因为中断只能在指令之间产生。lock前缀的真正作用是保持对系统总线的控制，直到整条指令执行完毕。

了解完lock指令的作用后，对于原子操作采用lock指令的原因就已经很明显。但需注意：lock指令只是针对自身CPU进行处理。lock指令在执行中占用CPU资源，从硬件上考虑，多核之间要负责相互通信，要让某个核的修改被其他核发现，因此lock指令的过多使用必然降低系统的性能。



## 9.14

静态变量虽在程序的整个执 行过程中始终存在，但是在它作用域之外不能使用。

死循环实现监控

tab是为了对齐

匈牙利命名法

kill的参数是pid，pid是每个cpu记录一个index的，cpu能kill的只有自己启动的proc

一个进程只能用自己的虚拟地址，而页表上也只映射了自己的pa，所以触不到其他地方

`Tvinit` (3067) 在 `main` 中被调用，它设置了 `idt` 表中的 256 个表项。

地址是以字节为单位的？一个bit没有地址吗？只是c语言不提供吧，机器语言是可以找到的。但是c语言也提供一部分位操作

测试编译器中定义的类型大小

效仿书中的代码



辩证逻辑：量变到质变

形式逻辑：两态

三态逻辑，三态逻辑计算机



解码器

指令是开关序列，数字就是指令

数据在内存，指令也在内存（冯诺伊曼）



编译器控制可以绕过  编译原理



while  非0为真







## 外部设备
由于外部设备种类繁多，有的设备兼有多种功能，到目前为止，很难对外部设备作出准确的分类。按照功能的不同，大致可以分为输入设备、显示设备、打印设备、外部存储器和网络设备五大类。

输入设备
输入设备（Input Device ）是人或外部与计算机进行交互的一种装置，用于把原始数据和处理这些数据的程序输入到计算机中。现在的计算机能够接收各种各样的数据，既可以是数值型的数据，也可以是各种非数值型的数据，如图形、图像、声音等都可以通过不同类型的输入设备输入到计算机中，进行存储、处理和输出。

显示设备
在计算机输出设备中显示设备相当于我们的眼睛，我们要了解操作是否正确，结果是什么，通常都通过显示设备来观察。目前计算机显示设备主要有CRT显示器、LCD显示、等离子显示器和投影机。而用于微型计算机中的主要是CRT显示器和LCD显示器。

打印设备
打印机（printer）是计算机的输出设备之一，将计算机的运算结果或中间结果以人所能识别的数字、字母、符号和图形等，依照规定的格式印在纸上的设备。打印机的种类很多，按打印元件对纸是否有击打动作，分击打式打印机与非击打式打印机；按打印字符结构，分全形字打印机和点阵字符打印机；按一行字在纸上形成的方式，分串式打印机与行式打印机；按所采用的技术，分柱形、球形、喷墨式、热敏式、激光式、静电式、磁式、发光二极管式等打印机。衡量打印机好坏的指标有三项：打印分辨率，打印速度和噪声。

外部存储器
外部存储器是用来存储计算机中不直接与运算器发生联系的那些指令和数据等信息的设备。它最初主要用来扩充计算机内存的容量，但随着计算机体系结构的变化，后来成为联机定时、分时系统的随机存储体系中不可缺少的部分，软件和数据等信息大都存放在高速外存中。
外存能长期保存信息，并且不依赖于电来保存信息。但是由机械部件带动，其速度与内存相比就显得慢很多。外存储器不直接与运算器和控制器交换信息，而是在处理机控制下，通过外部控制部件把所需的数据和程序随时送到内存储器，并把运算过程中的结果存储起来。这样就解决了速度和容量、造价之间的矛盾。

网络设备
为了高速、准确地进行信息传送，达到资源共享，提高计算机的利用率，往往把许多计算机系统通过专门的设备和通信线路连成计算机网络。随着计算机技术的飞速发展，计算机网络已经渗透到社会的各个领域，与我们的生活密切相关。网络控制着社会经济的发展，也使人们的工作和生活方式发生了巨大的变化。计算机与计算机、工作站与服务器进行连接时，除了使用连接介质外，还需要网络传输介质互联设备、网络物理层互联设备、数据链路层互联设备、网络层互联设备和应用层互联设备等五类中介设备。



## 端口 io空间

端口（port）是接口电路中能被CPU直接访问的寄存器的地址。几乎每一种外设都是通过读写设备上的寄存器来进行的。CPU通过这些地址即端口向接口电路中的寄存器发送命令，读取状态和传送数据。外设寄存器也称为“I/O端口”，通常包括：控制寄存器、状态寄存器和数据寄存器三大类，而且一个外设的寄存器通常被连续地编址。

独立编址

也称为“I/O端口”方式，外设寄存器位于“I/O（地址）空间”。

一些体系结构的CPU（典型地如X86）为外设专门实现了一个单独地地址空间，称为“I/O地址空间”或者“I/O端口空间”。这是一个与CPU的RAM物理地址空间不同的地址空间，所有外设的I/O端口均在这一空间中进行编址。CPU通过设立专门的I/O指令（如X86的IN和OUT指令）来访问这一空间中的地址单元（也即I/O端口）。与RAM物理地址空间相比，I/O地址空间通常都比较小，如x86 CPU的I/O空间就只有64KB（0－0xffff）。这是“I/O映射方式”的一个主要6缺点。

```
inb 从I/O端口读取一个字节(BYTE, HALF-WORD) ;
outb 向I/O端口写入一个字节（BYTE, HALF-WORD） ;
inw 从I/O端口读取一个字（WORD，即两个字节） ;
outw 向I/O端口写入一个字（WORD，即两个字节） ;
```

彩色图形适配器（Color Graphics Adapter），是IBM公司的第一个彩色图形卡，也是第一个IBM PC上的计算机显示标准。 标准IBM CGA图形卡具有16千字节显示内存。CGA卡提供多种图形和文字显示模式，以及可达640×200的显示分辨率，最高16色的显示能力（通常不能显示在最大分辨率下）



## 设备号  驱动

Linux系的/dev目录下面的的设备文件是用来表示外设的，如/dev/sda1表示第一块硬盘的第一个分区。但是这个/dev/sda1仅仅是方便用户观察，linux内核中表示不同的设备是通过major 和minor number实现的，通过major和minor Number来加载相应的驱动程序。

major number：表示不同的设备类型

minor number：表示同一个设备的的不同分区

驱动程序就是向下控制硬件，向上提供接口，这里的向上提供的接口最终对应到应用层有三种方式：**设备文件，/proc，/sys**，其中最常用的就是使用设备文件，而Linux设备中用的最多的就是字符设备，本文就以字符设备为例来分析创建并打开一个字符设备的文件内部机制。

Linux中一切皆文件。当我们使用mknod(或其他方法)创建一个设备文件时，也会在文件系统中创建一个inode，这个inode和其他的inode一样，用来存储关于这个文件的静态信息(不变的信息)，包括这个设备文件对应的设备号，文件的路径以及对应的驱动对象etc

**驱动对象**和一个(一组)**设备号**联系到一起。而创建设备文件，其实是把**设备文件**和**设备号**联系到一起



## console（还有问题）

conslock在每次使用console两个缓冲区的时候用

console就是管理input和output的一些结构与接口

console并不是一个进程，只有中间的函数会被调用

cpu把console当成一个文件，在想要io的时候，调用write，write解析fd进入filewrite，filewrite解析f通过ip的major调用console的读写函数

现在还有：

为什么consoleread的时候要先unlock ip？

1. console的inode的ilock在什么时候会用到？
   * 要io，进readi的时候：如果get不到，会在ip上sleep；如果get到，会进入iread，进入consoleread，然后unlock，acquire conslock，在这里反复get。两者都可以接受。为什么选第二种？
   * 还有哪里用吗？
2. console的inode是不是除了用type引导进入consoleread之外，没有其他用处？只是为了保证设备处理的统一？
3. 其他设备的inode文件呢？有什么用处？都有什么外设？外设的io空间，管理接口

缓冲区和keyboard和screed中的交接：驱动程序在哪？

### 补充知识

从键盘输入的任何信息都属于字符或者字符串类型。当键盘上敲1，计算机所接受到的是1的字符编码，它可能实际上是一个65

函数名值就是函数地址

## sh（代码分析）

cmd的参数数组从缓冲区中取按顺序出来的

调用exec时，exec中的路径，直接用了cmd的第一个参数，而xv6构建的cmd第一个参数只是命令名，只有在命令名在根目录下，当前目录也是根目录的时候，命令名才能直接当做路径

而xv6的shell指令的时候，也就是只是简单的把几个简单的指令都放到了根目录下，指令只能在根目录使用

xv实现的是一个很基础的shell

### 补充知识

每个进程有一个工作目录，在proc的cwd里面。所以cd只能在自己的进程里做，不能开子进程runcmd

\> 是定向输出到文件，如果文件不存在，就创建文件；如果文件存在，就将其清空；一般我们备份清理日志文件的时候，就是这种方法：先备份日志，再用`>`，将日志文件清空（文件大小变成0字节）；

\>>
这个是将输出内容追加到目标文件中。如果文件不存在，就创建文件；如果文件存在，则将新的内容追加到那个文件的末尾，该文件中的原有内容不受影响。

"."--代表目前所在的目录
".."--代表上一层目录
"/"--代表根目录

  ‘/’在linux中表示根目录。在Linux系统中，除根目录(root)以外，所有文件和目录都包含在相应的目录文件中。Linux文件系统采用带链接的树形目录结构，即只有一个根目录（通常用“/”表示），其中含有下级子目录或文件的信息；子目录中又可含有更下级的子目录或者文件的信息。这样一层一层地延伸下去，构成一棵倒置的树。    

   ‘/’在windows中也表示根目录，但此跟目录非彼根目录。windows对磁盘分区后会有多个磁盘，通常系统会装在C盘。windows有多个磁盘所以就会有‘多个根目录’，在dos命令模式下，在D盘的某文件夹中输入‘cd/’命令回车后会直接回到D盘的根目录。其它磁盘下也会回到该磁盘的目录。在哪个磁盘下使用‘/’，它就表示哪个磁盘的根目录。

## 系统时钟：实时时钟和系统定时器

实时时钟 

一个是由纽扣电池供电的“Real Time Clock”也叫做RTC（实时时钟）或者叫CMOS时钟，硬件时钟。当操作系统关机的时候，用这个来记录时间，但是对于运行的系统是不用这个时间的。当系统启动时，内核通过读取RTC来初始化墙上时间（存放在xtime变量中），系统关机的时候将系统时间写回RTC中进行同步。

系统定时器SysTick是属于CM3内核中的一个外设，相关寄存器内嵌在NVIC中

系统定时器完全由操作系统管理。当系统启动时，内核通过RTC初始化系统定时器，系统定时器接着由操作系统共掌管，进行固定频率的定时。系统定时器以某种频率自行触发时钟中断，该频率可以通过编程预定，称节拍率。

系统时间：内核知道连续两次时钟中断的间隔时间。这个间隔时间称为节拍（tick），全局变量jiffies用来记录自系统启动以来产生的节拍的总数。每发生一次timer interrupt，Jiffies变数会被加一。内核就是这样来计算墙上时间和系统运行时间

内核时钟在系统关机的情况下是不存在的

系统定时器是内核定时机制中最为重要的角色。尽管不同体系结构中的定时器实现不尽相同，但是系统定时器的根本思想没有区别——提供一种周期性触发中断机制。



## 多进程系统

> 在Linux系统中，对于用户创建的进程(线程)来说，CPU分配时间片的单位是线程还是进程?

是线程。线程是实际工作的单元[1]，进程只是一个容器，用来管理一个或多个线程。

> 1.这是不是就意味着尽量使用多线程并发，这样可以抢到更多的时间片。

理论上是的，多线程的一种用途就是能同时做好几件事情，以提高效率。但实际问题是，CPU的数量（核心数，下同）是有限的，而且并不多。如果你的CPU有8个CPU，并且整个系统中有8个线程的话，不考虑中断等因素，每个线程理论上能一直执行下去。然而多于8个线程以后，操作系统就必须进行**调度**，也就是分配时间片。具体的分配方案，或者说调度算法有很多种，详情参见[Scheduling (computing)](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Scheduling_%28computing%29)。如果一个进程创建了很多线程的话，最多也只有8个能够处于执行的状态[2]，其余的线程必须等待调度。线程被调度的时候需要进行上下文切换，这个操作是一种额外的开销。线程数量过多的时候，上下文切换产生的额外开销会对系统的效率造成负面影响。

> 2.操作系统对于拥有多线程的进程，是否会减少其每个线程的时间片，或做其他处理来保证公平性？

这就是调度算法需要考虑和优化的问题。比如线程和进程有优先级，在抢占式的调度中，优先级高的线程可以从优先级低的线程那里抢占CPU。另外，在多CPU平台上，调度算法还要考虑缓存的关联性等。

> 在一个进程中的多个线程要注意在可能的情况下将本线程阻塞，将剩余的时间片让给兄弟线程。

在主流的操作系统实现里，一般进程是不能直接控制自己的线程的执行顺序的。也就是说，把一个线程挂起并不能保证另一个线程一定能够被执行。

注1：Linux内核其实不区分进程和线程，内核把执行单元叫做任务(task)。操作系统实际上调度的是进程，进程通过fork()来创建同样的另一个进程。每个进程有一个PID，同一组进程中最先启动的那个还有一个TGID。严格来说前者应该叫线程ID，后者应该叫进程ID。Linux里的线程实际上是共享一些资源的一系列进程而已。不过这里面要展开讲的话就又能写一篇回答了，就先讲这么多吧。

注2：这里说的是物理线程，有别于逻辑线程。

## 目录  /  /usr

/：存放系统程序，也就是At&t开发的Unix程序
/usr：存放Unix系统商（比如IBM和HP）开发的程序
/usr/local：存放用户自己安装的程序
/opt：在某些系统，用于存放第三方厂商开发的程序，所以取名为option，意为"选装"

* 系统级的组件放在/bin、/lib；
* 根用户才能访问的放在/sbin；
* 系统repository提供的应用程序放在/usr/bin、/usr/lib；
* 用户自己编译的放在/usr/local/XXX

## 补充

CPU有一个引脚就是触发中断的。为了接入不同通常会有中断控制器，不同硬件连接到中断控制器上，每个硬件的中断有一个中断编号IRQ

比特图，bitmap，bmp

预处理：编译之前。

c语言不支持同名函数。static，特定文件内可见就好

take responsibility for their behavior

## 线程

自己总结：线程就是和父进程共享很多资源的多个进程，切换的时候很多公用的资源不用换，比如页表，换个上下文就行了，线程可以独立不同的任务，每个线程有自己的运行上下文，多路复用下就像同时进行一样。

### 线程的实现

进程并不执行什么, 只是维护应用程序所需的各种资源. 而线程则是真正的执行实体. 

为了让进程完成一定的工作, 进程必须至少包含一个线程

进程所维护的是程序所包含的资源(静态资源), 如: 地址空间, 打开的文件句柄集, 文件系统状态, 信号处理handler, 等;

线程所维护的运行相关的资源(动态资源), 如: 运行栈, 调度相关的控制信息, 待处理的信号集, 等;

然而, 一直以来, linux内核并没有线程的概念. 每一个执行实体都是一个task_struct结构, 通常称之为进程
进程是一个执行单元, 维护着执行相关的动态资源. 同时, 它又引用着程序所需的静态资源
通过系统调用clone创建子进程时, 可以有选择性地让子进程共享父进程所引用的资源. 这样的子进程通常称为轻量级进程。同一个进程中的线程的 task_struct 里边某些值相同，某些指针指向相同的位置。
linux上的线程就是基于轻量级进程, 由用户态的pthread库实现的.

使用pthread以后, 在用户看来, 每一个task_struct就对应一个线程, 而一组线程以及它们所共同引用的一组资源就是一个进程

但是, 一组线程并不仅仅是引用同一组资源就够了, 它们还必须被视为一个整体.
对此, POSIX标准提出了如下要求:
1, 查看进程列表的时候, 相关的一组task_struct应当被展现为列表中的一个节点;
2, 发送给这个"进程"的信号(对应kill系统调用), 将被对应的这一组task_struct所共享, 并且被其中的任意一个"线程"处理;
3, 发送给某个"线程"的信号(对应pthread_kill), 将只被对应的一个task_struct接收, 并且由它自己来处理;
4, 当"进程"被停止或继续时(对应SIGSTOP/SIGCONT信号), 对应的这一组task_struct状态将改变;
5, 当"进程"收到一个致命信号(比如由于段错误收到SIGSEGV信号), 对应的这一组task_struct将全部退出;
6, 等等(以上可能不够全);

### 线程调度

在Linux中，线程是由进程来实现，线程就是轻量级进程（ lightweight process ），因此在Linux中，线程的调度是按照进程的调度方式来进行调度的

在Linux中，调度器是基于线程的调度策略（scheduling policy）和静态调度优先级（static scheduling priority）来决定那个线程来运行。

### 线程间共享

这代表一个线程使用某些共享内存时，其他线程必须等它结束，才能使用这一块内存。

某些内存区域，只能供给固定数目的线程使用

这时的解决方法，就是在门口挂n把钥匙。进去的人就取一把钥匙，出来时再把钥匙挂回原处。后到的人发现钥匙架空了，就知道必须在门口排队等着了。这种做法叫做["信号量"](http://en.wikipedia.org/wiki/Semaphore_(programming))（Semaphore），用来保证多个线程不会互相冲突。

不难看出，mutex是semaphore的一种特殊情况（n=1时）。也就是说，完全可以用后者替代前者。但是，因为mutex较为简单，且效率高，所以在必须保证资源独占的情况下，还是采用这种设计。

操作系统的设计，因此可以归结为三点：

（1）以多进程形式，允许多个任务同时运行；

（2）以多线程形式，允许单个任务分成不同的部分运行；

（3）提供协调机制，一方面防止进程之间和线程之间产生冲突，另一方面允许进程之间和线程之间共享资源。



## 固态硬盘

固态硬盘和机械硬盘分别采用两种当下流行的持久储存方式（区别于内存断电即失去数据的储存）。

机械硬盘使用磁为介质保存数据，利用磁头读写圆盘面微小磁体的磁极来实现数据读写。

固态硬盘采用闪存为储存介质，和U盘的原理几乎一样，闪存外观很像CPU、内存那种芯片，也和内存一样内部没有任何活动的机械部件。

机械硬盘在不同位置读写时，磁头要来回运动，在同一个地方连续写入一段数据时效率更高，随机乱序读写时效率很低。而闪存不用，随机读写和顺序读写几乎没有速度区别。

普通U盘单个闪存芯片的读写速度无法及得上机械硬盘，而固态硬盘实际是闪存整列，利用多个闪存并行读写来达到甚至超过机械硬盘的读写速度，并行的闪存数无上限，读写速度、容量与闪存芯片数成正比，但价格也成正比。

固态硬盘除了速度高，还有功耗低、无噪音、低故障率的优点。理论上闪存有写次数限制，以目前技术来说同一个位置写超过3000次就会失效并故障，但固态硬盘采用不同于机械硬盘的写入策略，将写入操作平均分摊在整个空间内，所以在普通家用中几乎不会有些写坏的那一天，这也是固态硬盘与普通闪存的本质区别。

目前市面上同等容量的固态硬盘的价格远高于机械硬盘，但差距正在缩小，是否有一天能完全替换机械硬盘仍未知。目前固态硬盘+机械硬盘是高端主机、笔记本的配置，固态硬盘可作为系统盘以提高速度，机械硬盘可装电影、文档等使用不频繁的数据，如此可兼顾性能和容量

### 闪存

闪存（Flash Memory）是一种长寿命的非易失性（在断电情况下仍能保持所存储的数据信息）的存储器，数据删除不是以单个的字节为单位而是以固定的区块为单位（注意：NOR Flash 为字节存储。），区块大小一般为256KB到20MB。闪存是电子可擦除只读存储器（EEPROM）的变种，闪存与EEPROM不同的是，EEPROM能在字节水平上进行删除和重写而不是整个芯片擦写，而闪存的大部分芯片需要块擦除。由于其断电时仍能保存数据，闪存通常被用来保存设置信息，如在电脑的BIOS（基本程序）、PDA（个人数字助理）、数码相机中保存资料等

固态硬盘（Solid State Drive）用固态电子存储芯片阵列而制成的硬盘，由控制单元和存储单元（FLASH芯片、DRAM芯片）组成。固态硬盘在接口的规范和定义、功能及使用方法上与普通硬盘的完全相同，在产品外形和尺寸上也完全与普通硬盘一致。

固态硬盘的存储介质分为两种，一种是采用闪存（FLASH芯片）作为存储介质，另外一种是采用DRAM作为存储介质

## cli 

CLI禁止中断发生
STL允许中断发生

这两个指令只能在内核模式下执行，不可以在用户模式下执行；而且在内核模式下执行时，应该尽可能快的恢复中断，因为CLI会禁用硬件中断，若长时间禁止中断会影响其他动作的执行（如移动鼠标等等），系统就会变得不稳定。在标志寄存器中中断标志清零的情况下，可以以“int  ××”的形式调用软中断。

